{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIE1621 Fall 2017\n",
    "# Project \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def pprint(obj):\n",
    "    '''\n",
    "    For debugging, print statements with numpy variable names and shape\n",
    "    '''\n",
    "    def namestr(obj):\n",
    "        namespace = globals()\n",
    "        return [name for name in namespace if namespace[name] is obj]\n",
    "    # Assumes obj is a numpy array, matrix\n",
    "    try:\n",
    "        print(namestr(obj), obj.shape)\n",
    "    except:\n",
    "        try:\n",
    "            print(namestr(obj), \",\", len(obj))\n",
    "        except:\n",
    "            print(namestr(obj))\n",
    "    print(obj)\n",
    "\n",
    "\n",
    "covarianceMatrix = np.array([[0.02778, 0.00387, 0.00021],\n",
    "                             [0.00387, 0.01112, -0.00020],\n",
    "                             [0.00021, -0.00020, 0.00115]])\n",
    "\n",
    "expectedReturn = np.array([0.1073, 0.0737, 0.0627])\n",
    "\n",
    "stepSize = 1.0\n",
    "\n",
    "delta = 4.0\n",
    "\n",
    "# Randomly initialize to anything\n",
    "initialX = np.array([0.25, 0.25, 0.5])\n",
    "initialPi = 1.2\n",
    "\n",
    "def checkShape(x, u, sigma, delta):\n",
    "    if delta <= 0.0:\n",
    "        raise ValueError(\"Risk aversion, delta needs to be > 0\")\n",
    "    if delta <= 3.5 or delta >= 4.5:\n",
    "        raise ValueError(\"Risk aversion, delta needs to be between (3.5, 4.5)\")\n",
    "    if sigma.shape[0] != sigma.shape[1]:\n",
    "        raise ValueError(\"Covariance matrix must be square\")\n",
    "    if sigma.shape[0] != x.shape[0]:\n",
    "        raise ValueError(\"Dimensions dont match between sigma and x\")\n",
    "    if u.shape[0] != x.shape[0]:\n",
    "        raise ValueError(\"Dimensions dont match between u and x\")\n",
    "    '''\n",
    "    Not always met, only met after training is over\n",
    "    if np.sum(x) != 1.0:\n",
    "        raise ValueError(\"X must sum to 1.0\")\n",
    "    '''\n",
    "\n",
    "def computeF(x, u, sigma, delta):\n",
    "    '''\n",
    "    x is the proportion, the parameters we are changing to maximize f\n",
    "    u is the expected return\n",
    "    sigma is the covariance matrix\n",
    "    delta is the parameter that controls risk aversion, delta > 0\n",
    "    '''\n",
    "    checkShape(x, u, sigma, delta)\n",
    "    \n",
    "    firstTerm = 0.0\n",
    "    for (ui, xi) in zip(u, x):\n",
    "        firstTerm += ui * xi\n",
    "    secondTerm = 0.0\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[0]):\n",
    "            secondTerm += sigma[i][j] * x[i] * x[j]\n",
    "    secondTerm *= (float(delta)/float(2.0))\n",
    "            \n",
    "    f = firstTerm - secondTerm\n",
    "    return f\n",
    "\n",
    "def computeGrad(x, pi, u, sigma, delta):\n",
    "    checkShape(x, u, sigma, delta)\n",
    "    grad = np.zeros((x.shape[0] + 1, 1))\n",
    "    dLdx = np.zeros(x.shape)\n",
    "    for i in range(x.shape[0]): \n",
    "        secondIndex = (i + 1) % x.shape[0]\n",
    "        thirdIndex = (i + 2) % x.shape[0]\n",
    "        \n",
    "        dLdx[i] = u[i] + pi - ((delta)*((float(sigma[i][i]*x[i])/2.0) \\\n",
    "                                        + sigma[i][secondIndex]*x[secondIndex]) \\\n",
    "                               + sigma[i][thirdIndex] * x[thirdIndex])\n",
    "        grad[i] = dLdx[i]\n",
    "        \n",
    "    dLdPi = np.sum(x) - 1.0\n",
    "    grad[x.shape[0]] = dLdPi\n",
    "    \n",
    "    return grad; \n",
    "\n",
    "def computeHessian(x, pi, u, sigma, delta):\n",
    "    checkShape(x, u, sigma, delta)\n",
    "    dim = x.shape[0] + 1\n",
    "    hessian = np.zeros((dim, dim))\n",
    "    for i in range(x.shape[0]): \n",
    "        for j in range(x.shape[0]): \n",
    "            hessian[i][j] = (-1.0) * delta * sigma[i][j]\n",
    "            if i == j:\n",
    "                hessian[i][j] /= 2.0\n",
    "    for i in range(x.shape[0]): \n",
    "        hessian[x.shape[0]][i] = 1.0\n",
    "        hessian[i][x.shape[0]] = 1.0\n",
    "    return hessian\n",
    "\n",
    "x = initialX.copy()\n",
    "pi = initialPi \n",
    "f = computeF(x, expectedReturn, covarianceMatrix, delta)\n",
    "\n",
    "pprint(covarianceMatrix)\n",
    "pprint(expectedReturn)\n",
    "pprint(x)\n",
    "pprint(f)\n",
    "\n",
    "grad = computeGrad(x, pi, expectedReturn, covarianceMatrix, delta)\n",
    "pprint(grad)\n",
    "\n",
    "hessian = computeHessian(x, pi, expectedReturn, covarianceMatrix, delta)\n",
    "pprint(hessian)\n",
    "\n",
    "maxIteration = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton Method\n",
    "currX = x.copy()\n",
    "currPi = pi\n",
    "for iteration in range(maxIteration):\n",
    "    pprint(iteration)\n",
    "    \n",
    "    x = currX.copy()\n",
    "    pi = currPi\n",
    "    \n",
    "    f = computeF(x, expectedReturn, covarianceMatrix, delta)\n",
    "    constraint = np.sum(x)\n",
    "    pprint(f)\n",
    "    pprint(constraint)\n",
    "    pprint(x)\n",
    "    pprint(pi)\n",
    "    \n",
    "    grad = computeGrad(x, pi, expectedReturn, covarianceMatrix, delta)\n",
    "    hessian = computeHessian(x, pi, expectedReturn, covarianceMatrix, delta)\n",
    "    \n",
    "    direction = np.dot(np.linalg.inv(hessian), grad)\n",
    "    pprint(direction)\n",
    "    \n",
    "    # Stopping condition\n",
    "    if np.linalg.norm(direction) < np.power(0.1, 10):\n",
    "        break\n",
    "    xAdd = np.reshape(direction[:-1].copy(), currX.shape)\n",
    "    \n",
    "    # You always deduct in Newton Method!\n",
    "    # As newton method accounts for correct direction regardless of max or min.\n",
    "    # Newton's method always searches for critical points. \n",
    "    currX -= stepSize * xAdd\n",
    "    currPi -= stepSize * direction[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steepest Descent Method\n",
    "# Note: NOT really steepest as the question asks to use stepsize of 1.0, so don't calculate stepsize. \n",
    "# Just do the normal gradient descent in the Machine Learning literature\n",
    "\n",
    "currX = x.copy()\n",
    "currPi = pi\n",
    "for iteration in range(maxIteration):\n",
    "    pprint(iteration)\n",
    "    \n",
    "    x = currX.copy()\n",
    "    pi = currPi\n",
    "    \n",
    "    f = computeF(x, expectedReturn, covarianceMatrix, delta)\n",
    "    constraint = np.sum(x)\n",
    "    pprint(f)\n",
    "    pprint(constraint)\n",
    "    pprint(x)\n",
    "    pprint(pi)\n",
    "    \n",
    "    grad = computeGrad(x, pi, expectedReturn, covarianceMatrix, delta)\n",
    "    # No hessian computation needed for gradient descent\n",
    "    \n",
    "    direction = grad\n",
    "    pprint(direction)\n",
    "    \n",
    "    # Stopping condition\n",
    "    if np.linalg.norm(direction) < np.power(0.1, 10):\n",
    "        break\n",
    "    xAdd = np.reshape(direction[:-1].copy(), currX.shape)\n",
    "    \n",
    "    # Gradient Descent diverges with constant step size\n",
    "    # stepSize = 0.00001 , can only converge with small step size but not OPTIMAL\n",
    "    # Need to add cause maximizing the function, NOT minimizing\n",
    "    currX += stepSize * xAdd\n",
    "    currPi += stepSize * direction[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quasi Newton Methods \n",
    "# BFGS\n",
    "\n",
    "H = np.eye(x.shape[0] + 1)\n",
    "\n",
    "def BFGSUpdate(H, y, s): \n",
    "    Hnext = H + (1.0/(np.dot(np.transpose(y), s))) * np.dot(y, np.transpose(y)) - (1.0/(np.dot(np.dot(np.transpose(s), H), s))) * np.dot(np.dot(np.dot(H, s), np.transpose(s)), H)\n",
    "    return Hnext\n",
    "\n",
    "pprint(x)\n",
    "pprint(pi)\n",
    "xnew = np.zeros((x.shape[0]+1,1))\n",
    "for i in range(x.shape[0]):\n",
    "    xnew[i] = x[i]\n",
    "xnew[x.shape[0]] = pi\n",
    "\n",
    "pprint(xnew)\n",
    "\n",
    "for iteration in range(maxIteration):\n",
    "    xold = xnew\n",
    "    gradFk = computeGrad(xold[:-1], xold[-1], expectedReturn, covarianceMatrix, delta)\n",
    "    direction = np.dot(np.linalg.inv(H), gradFk)\n",
    "    # Stopping condition\n",
    "    if np.linalg.norm(direction) < np.power(0.1, 10):\n",
    "        break\n",
    "    #----------------------------------------------------------\n",
    "    xnew = xold - stepSize * direction\n",
    "    gradFk1 = computeGrad(xnew[:-1], xnew[-1], expectedReturn, covarianceMatrix, delta)\n",
    "    s = xnew - xold\n",
    "    y = gradFk1 - gradFk\n",
    "    # BFGS Update\n",
    "    Hnext = BFGSUpdate(H,y,s)\n",
    "    H = Hnext\n",
    "    \n",
    "    f = computeF(xnew[:-1], expectedReturn, covarianceMatrix, delta)\n",
    "    pprint(f)\n",
    "    pprint(iteration)\n",
    "    pprint(xold)\n",
    "    pprint(H)\n",
    "    pprint(gradFk)\n",
    "    pprint(direction)\n",
    "    pprint(xnew)\n",
    "    pprint(gradFk1)\n",
    "    pprint(y)\n",
    "    pprint(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton Method with Backtracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steepest Descent Method with Backtracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quasi Newton Methods with Backtracking\n",
    "# BFGS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
