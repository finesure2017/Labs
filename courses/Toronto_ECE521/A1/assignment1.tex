\documentclass[a4paper,12pt]{article} 
%Packages included by Soon Chee Loong
\usepackage{amssymb}  % For \therefore
\usepackage{fullpage} % Package to use full page
\usepackage{parskip} % Package to tweak paragraph skipping
\usepackage{tikz} % Package for drawing
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{palatino}
\usepackage{minted} % For code highlighting 
\usepackage{amsmath} % To split long equations
% More custom packages (default or by Christopher) 
\usepackage{amssymb}
\usepackage{bm}
\usepackage{multirow} % Used to merge multiple tables cells along a row
\usepackage{mathtools}
\usepackage{physics}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{relsize}
\usepackage{wrapfig}
\usepackage{cite}
%------------------------------------------------------------------
% Adjust line spacing
\renewcommand{\baselinestretch}{1.5}
%------------------------------------------------------------------
% Colour Definitions
\definecolor{blue}{HTML}{1f77b4}
\definecolor{orange}{HTML}{ff7f0e}
\definecolor{green}{HTML}{2ca02c}
\definecolor{bg}{rgb}{0.95, 0.95, 0.95}
%------------------------------------------------------------------
% Collaboration Notes 
% 0. Ensure tex always compiles, fix any errors immediately
%		This doesn't have version control. So don't mess up.  
% 1. Work on 1 main.tex file only 
% 	Easier to back-up to local
% 	Easier to work on local if needed. 
% 2. TODO: Mark anything important to do as TODO
% 	so we can easily search for all TODO: comments before submitting. 
%------------------------------------------------------------------
\title{ECE521 Winter 2017: Assignment 1}
\author{FuYuan Tee, (999295837)
  \thanks{Equal Contribution (50\%), fuyuan.tee@mail.utoronto.ca}
\and Chee Loong Soon, (999295793) \thanks{Equal Contribution (50\%),  cheeloong.soon@mail.utoronto.ca}} 
\date{February 8th, 2017}
\begin{document}
\maketitle
\tableofcontents
\clearpage
%------------------------------------------------------------------
\section{k-Nearest Neighbour}
%-----------------------------------------------------
\subsection{Geometry of k-NN}
%--------------------------------
\subsubsection{Describe 1D Dataset}
An example of a 1-D dataset with two classes in which k-NN produces an accuracy that is periodic with k is illustrated in Figure \ref{figure:periodicK}. The data point comes from the training set itself. The data points are equally distant from each adjacent data point. 

\begin{figure}[!htb]
	\centering
	\includegraphics[width=\textwidth]{Graphics/A1_PeriodicK}
    \caption{\label{figure:periodicK} 1-D Dataset Illustration with classification accuracy that is periodic to $k$.}
\end{figure}

For such a dataset, the classification accuracy of the data point can be summarised in the Table \ref{table:kAccuracy} below. The accuracy follows a periodic function of $50 \% \cdot \sin \left(\frac{\pi}{2} k \right) + 50 \%$.

%--------------------------------
% Begin Table 
\begin{table}[ht]
\centering % used for centering table
\caption{$k$ and prediction accuracy for two periods}
\label{table:kAccuracy}
\begin{tabular}{c c c c c c c c c} % centered columns (4 columns)
\hline % single horizontal line
$k$ & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\ [0.5ex] 
Prediction Accuracy (\%) & 100 & 50 & 0 & 50 & 100 & 50 & 0 & 50 \\ [1ex] % vertical space
\hline
\end{tabular}
\end{table}
\clearpage
% End Table 
%--------------------------------

% \begin{itemize}
% \item Points from two classes (0, 1) alternate and are equally spaced from each other in One-Dimension as illustrated in Table \ref{table:generatedXY}. 

% The formula for generating the dataset would be 0 for every value of x that is even and 1 for every value of x that is odd as shown in Table \ref{table:generatedXY}. 

%--------------------------------
% Begin Table 
% \begin{table}[ht]
% \centering % used for centering table
% \begin{tabular}{c c c c} % centered columns (4 columns)
% \hline % single horizontal line
% X (Input) & Y (Output) \\ [0.5ex] 
% \hline
% 0 & 0 \\ 
% 1 & 1 \\
% 2 & 0 \\
% 3 & 1 \\
% 4 & 0 \\
% 5 & 1 \\
% 6 & 0 \\
% 8 & 1 \\ [1ex] % vertical space
% \hline
% \end{tabular}
% \caption{X and Y generated values}\label{table:generatedXY} 
% \end{table}
% End Table 
%--------------------------------
% \item This results in the prediction accuracy as shown in Table \ref{table:kAccuracy}.
% \end{itemize}
%-----------------------------------------------------
\subsubsection{Curse of Dimensionality}

Proving equation \ref{equation:CurseOfDim},
\begin{equation}
\label{equation:CurseOfDim}
\mathbf{var}(\frac{||x^{(i)} - x^{(j)}||_{2}^{2}}{\mathbf{E}[||x^{(i)} - x^{(j)}||_{2}^{2}] }) = \frac{N + 2}{N} - 1
\end{equation} 

We can utilise equation \ref{equation:ExpectationFormula} from  Probability Theory,
\begin{equation}
\label{equation:ExpectationFormula}
\mathbf{var}[x] = \mathbf{E}[x^{2}] - \mathbf{E}[x]^{2}
\end{equation} 

Below are the given equations,
\begin{equation}
x \in \mathbb{R}^{n}
\end{equation} 
\begin{equation}
\label{equation:GaussianDistribution}
\Pr(X) \sim \prod_{n=1}^{N} \mathcal{N}(x_{n},\; 0 \; \sigma^{2})
\end{equation} 
{\centering where $n$ represents the $n^{th}$ dimension. 

$N$ represents the number of training data. \par}
\begin{equation}
\label{equation:DifferenceGaussian}
d_{n} = x_{n}^{i} - x_{n}^{j}
\end{equation} 

{\centering where $i$ represents the $i^{th}$ training data. 

$j$ represents the $j^{th}$ training data. \par}

\begin{align}
\label{equation:DifferenceDistribution}
\Pr(d_{n}) &\sim \mathcal{N}(d_{n} ; 0 , 2\sigma^{2}) \\
\label{equation:IndependentDifference}
\mathbf{E}[d_{n}^{2}d_{m}^{2}] &= \mathbf{E}[d_{n}^{2}]\mathbf{E}[d_{m}^{2}] \\
\label{equation:FourthMoment}
\mathbf{E}[d_{n}^{4}] &= 3(\sqrt{2}\sigma)^{4} = 12\sigma^{4}
\end{align} 

From equations \ref{equation:GaussianDistribution} and \ref{equation:ExpectationFormula}, it is implied that
\begin{align}
\mathbf{E}[x_{n}] &= 0 \\
\mathbf{var}[x_{n}] &= \sigma^{2} = \mathbf{E}[x_{n}^{2}]
\end{align} 

From equations \ref{equation:DifferenceDistribution} and \ref{equation:ExpectationFormula}, it is implied
\begin{equation}
\mathbf{E}[d_{n}] = 0
\end{equation} 
\begin{equation}
\label{equation:DifferenceVariance}
\mathbf{var}[d_{n}] = 2\sigma^{2} = \mathbf{E}[d_{n}^{2}]
\end{equation} 

From equations \ref{equation:IndependentDifference} and \ref{equation:DifferenceVariance}, 
\begin{equation}
\mathbf{E}[d_{n}^{2}d_{m}^{2}] = \mathbf{E}[d_{n}^{2}]\mathbf{E}[d_{m}^{2}] = (2\sigma^{2})(2\sigma^{2}) = 4\sigma^{4}
\end{equation} 

From equation \ref{equation:CurseOfDim} and \ref{equation:DifferenceGaussian}, 
\begin{equation}
\label{equation:newCurseOfDim}
||x^{(i)} - x^{(j)}||_{2}^{2} = \sum_{n=1}^{N} (x_{n}^{(i)} - x_{n}^{(j)})^{2} = \sum_{n=1}^{N} d_{n}^{2}
\end{equation} 

Substituting equations \ref{equation:CurseOfDim},  \ref{equation:newCurseOfDim} into \ref{equation:ExpectationFormula}, 
\begin{equation}
\label{equation:ExpandedExpectationFormula}
\mathbf{var}\left(\frac{\sum_{n=1}^{N} d_{n}^{2}}{\mathbf{E}[\sum_{n=1}^{N} d_{n}^{2}]}\right) = \mathbf{E}\left[\left(\frac{\sum_{n=1}^{N} d_{n}^{2}}{\mathbf{E}[\sum_{n=1}^{N} d_{n}^{2}]}\right)^{2}\right] - \mathbf{E}\left[\left(\frac{\sum_{n=1}^{N} d_{n}^{2}}{\mathbf{E}[\sum_{n=1}^{N} d_{n}^{2}]}\right)\right]^{2}
\end{equation} 

Looking into the first term of the Right Hand Side (RHS) of equation \ref{equation:ExpandedExpectationFormula},
\begin{equation}
\label{equation:CurseOfDimLHS}
\begin{split}
\mathbf{E}\left[\left(\frac{\sum_{n=1}^{N} d_{n}^{2}}{\mathbf{E}[\sum_{n=1}^{N} d_{n}^{2}]}\right)^{2}\right]
= 
\mathbf{E}\left[\left(\frac{\sum_{n=1}^{N} d_{n}^{2}}{\sum_{n=1}^{N} \mathbf{E}[\ d_{n}^{2}]}\right)^{2}\right]
=
\mathbf{E}\left[\left(\frac{\sum_{n=1}^{N} d_{n}^{2}}{\sum_{n=1}^{N} 2\sigma^{2}}\right)^{2}\right] \\
=
\mathbf{E}\left[\left(\frac{\sum_{n=1}^{N} d_{n}^{2}}{2N\sigma^{2}}\right)^{2}\right]
= 
\mathbf{E}\left[\left(\frac{\sum_{n=1}^{N} d_{n}^{2}}{2N\sigma^{2}}\right)^{2}\right]
=
\mathbf{E}\left[\frac{(\sum_{n=1}^{N} d_{n}^{2})^{2}}{(2N\sigma^{2})^{2}}\right] \\
= 
\mathbf{E}\left[\frac{(\sum_{n=1}^{N} d_{n}^{2})^{2}}{4N^{2}\sigma^{4}}\right]
=
\mathbf{E}\left[\frac{\sum_{n=1}^{N}\sum_{m=1}^{N}d_{n}^{2}d_{m}^{2}}{4N^{2}\sigma^{4}}\right]
=
\mathbf{E}\left[\frac{\sum_{n=1}^{N}\sum_{m=1}^{N}d_{n}^{2}d_{m}^{2}}{4N^{2}\sigma^{4}}\right] \\
=
\frac{\mathbf{E}[\sum_{n=1}^{N}\sum_{m=1}^{N}d_{n}^{2}d_{m}^{2}]}{4N^{2}\sigma^{4}} \\
= 
\frac{\mathbf{E}[\sum_{n=1}^{N}d_{n}^{4} + 2\sum_{n=1}^{N-1}\sum_{m=n+1}^{N}d_{n}^{2}d_{m}^{2}]}{4N^{2}\sigma^{4}} \\
=
\frac{\mathbf{E}[\sum_{n=1}^{N}d_{n}^{4}] + \mathbf{E}[2\sum_{n=1}^{N-1}\sum_{m=n+1}^{N}d_{n}^{2}d_{m}^{2}]}{4N^{2}\sigma^{4}} \\
=
\frac{\sum_{n=1}^{N}\mathbf{E}[d_{n}^{4}] + \mathbf{E}[2\sum_{n=1}^{N-1}\sum_{m=n+1}^{N}d_{n}^{2}d_{m}^{2}]}{4N^{2}\sigma^{4}} \\
= 
\frac{\sum_{n=1}^{N}12\sigma^{4} + 2\sum_{n=1}^{N-1}\sum_{m=n+1}^{N}\mathbf{E}[d_{n}^{2}d_{m}^{2}]}{4N^{2}\sigma^{4}} \\
=
\frac{12N\sigma^{4} + 2\sum_{n=1}^{N-1}\sum_{m=n+1}^{N} 4\sigma^{4}}{4N^{2}\sigma^{4}} \\
=
\frac{12N\sigma^{4} + 2{N \choose 2}4\sigma^{4}}{4N^{2}\sigma^{4}} 
=
\frac{12N\sigma^{4} + 2(\frac{N(N-1)}{2}) 4\sigma^{4}}{4N^{2}\sigma^{4}} 
= \frac{4N^{2}\sigma^{4} + 8N\sigma^{4}}{4N^{2}\sigma^{4}} = \frac{N + 2}{N}
\end{split}
\end{equation} 

Looking into the second term of the RHS of equation \ref{equation:ExpandedExpectationFormula},
\begin{equation}
\label{equation:CurseOfDimRHS}
\begin{split}
\mathbf{E}\left[\left(\frac{\sum_{n=1}^{N} d_{n}^{2}}{\mathbf{E}[\sum_{n=1}^{\infty} d_{n}^{2}]}\right)\right]^{2} 
= 
\mathbf{E}\left[\left(\frac{\sum_{n=1}^{N} d_{n}^{2}}{\sum_{n=1}^{N} \mathbf{E}[d_{n}^{2}]}\right)\right]^{2} 
=
\mathbf{E}\left[\left(\frac{\sum_{n=1}^{N} d_{n}^{2}}{\sum_{n=1}^{N} 2\sigma^{2}}\right)\right]^{2} \\
= 
\mathbf{E}\left[\left(\frac{\sum_{n=1}^{N} d_{n}^{2}}{2N\sigma^{2}}\right)\right]^{2} 
= 
\left(\frac{\mathbf{E}[\sum_{n=1}^{N} d_{n}^{2}]}{2N\sigma^{2}}\right)^{2} 
=
\left(\frac{\sum_{n=1}^{N} \mathbf{E}[ d_{n}^{2}]}{2N\sigma^{2}}\right)^{2} \\
=
\left(\frac{N2\sigma^{2}}{2N\sigma^{2}}\right)^{2} = 1^{2} = 1
\end{split}
\end{equation} 

Therefore, combining both terms from the RHS of equation  \ref{equation:ExpandedExpectationFormula} as calculated in equations \ref{equation:CurseOfDimLHS} and \ref{equation:CurseOfDimRHS}  results in 
\begin{equation}
\label{equation:CurseOfDimensionalityResult}
 \frac{N + 2}{N} - 1
\end{equation} 

which proves equation \ref{equation:CurseOfDim}. 

To show that equation \ref{equation:CurseOfDimensionalityResult} vanishes as $N\to\infty$,
\begin{equation}
\label{equation:CurseOfDimVanish}
\lim_{N\to\infty} \left(\frac{N + 2}{N} - 1\right) = 
\lim_{N\to\infty} \left(\frac{N + 2 - N}{N}\right) = 
\lim_{N\to\infty} \left(\frac{2}{N}\right) = 0
\end{equation} 

This proves that the variance vanishes which means that a test data point will be equally close to all training examples in a high dimensional space. 
%-----------------------------------------------------
\subsection{Euclidean Distance Function}
\subsubsection{Inner Product}

All input vectors have same magnitude in the training set. 
$||x^{(1)}||_{2}^{2} = ... = ||x^{(M)}||_{2}^{2}$

To show that in order to find nearest neighbor of a test point $x^{*}$ among the training set, it is sufficient to just compare and rank the negative inner product between the training and the test data, $x^{(M)^{T}}x^{*}$. 

In a 2-Dimensional case, if all input vectors have the same magnitude, that defines a circle in the training set. Taking the inner product between two vectors, $x^{(M)^{T}}x^{*}$ would simply be calculating the angle between the two vectors. This is similar to performing a cosine similarity calculation. Therefore, ranking based on negative inner product would be looking for the smallest angle between any 2 input vectors. 

Now, extending this intuition to a M-Dimensional case. It is a M-dimensional hypersphere where all the vectors extend to the surface of the hypersphere as they have the same magnitude. Taking the negative inner product, $x^{(M)^{T}}x^{*}$ and ranking them would be finding the minimum angle between any of the 2 points on the surface, which is its nearest neighbor. 

This can be illustrated below in equation \ref{equation:Hypersphere}, where
$R$ is the radius of the hypersphere and all vectors have a magnitude equal to this radius. 
\begin{equation}
\label{equation:Hypersphere}
\begin{split}
||x^{(*)}||_{2}^{2} =& \sum_{n=1}^{N} (x^{(*)}_{n})^{2} = R^{2} \\
||x^{(1)}||_{2}^{2} =& \sum_{n=1}^{N} (x^{(1)}_{n})^{2} = R^{2} \\
&\vdots \\
||x^{(M)}||_{2}^{2} =& \sum_{n=1}^{N} (x^{(M)}_{n})^{2} = R^{2}
\end{split}
\end{equation}

\subsubsection{Pairwise Distances}
Code snippets are included below:
\begin{minted}[linenos=true,bgcolor=bg,numberblanklines=true,showspaces=false,breaklines=true]{python}
{python}
# 1.2 Euclidean Distance Function 
# 1.2.2 Pairwise Distances
# Write a vectorized Tensorflow Python function that implements
# the pairwise squared Euclidean distance function 
# for two input matrices.
# No Loops and makes use of Tensorflow broadcasting.
def PairwiseDistances(X, Z):
    """
    input:
        X is a matrix of size (B x N)
        Z is a matrix of size (C x N)
    output:
        D = matrix of size (B x C) containing 
        the pairwise Euclidean distances
    """
    B = X.get_shape().as_list()[0]
    N = X.get_shape().as_list()[1]
    C = Z.get_shape().as_list()[0]
    # Ensure the N dimensions are consistent 
    assert  N == Z.get_shape().as_list()[1]
    # Reshape to make use of broadcasting in Python
    X = tf.reshape(X, [B, 1, N])
    Z = tf.reshape(Z, [1, C, N])
    # The code below automatically does broadcasting.
    # Calculates the 
    # pairwise squared Euclidean distance function 
    D = tf.reduce_sum(tf.square(tf.sub(X, Z)), 2)
    return D
\end{minted}

%-----------------------------------------------------
\subsection{Making Predictions}
%--------------------------------
\subsubsection{Choosing Nearest Neighbour}
Code snippets are included below:
\begin{minted}[linenos=true,bgcolor=bg,numberblanklines=true,showspaces=false,breaklines=true]{python}
{python}
# 1.3 Making Predictions
# 1.3.1 Choosing nearest neighbours
# Write a vectorized Tensorflow Python function that 
# takes a pairwise distance matrix
# and returns the responsibilities of the 
# training examples to a new test data point. 
# It should not contain loops.
# Use tf.nn.top_k
def ChooseNearestNeighbours(D, K):
    """
    input:
        D is a matrix of size (B x C)
        K is the top K responsibilities for each test input
    output:
        topK are the value of the squared distances
        for the top K values
        indices are the index of the location of 
        these top K squared distances. 
    """
    # Take topK of negative distances since 
    # closer data ranks higher. 
    topK, indices = tf.nn.top_k(tf.neg(D), K)
    return topK, indices
\end{minted}
%--------------------------------
\subsubsection{Prediction}
Code snippets are included below:
\begin{minted}[linenos=true,bgcolor=bg,numberblanklines=true,showspaces=false,breaklines=true]{python}
{python}
# 1.3.2 Prediction
# Compute the k-NN prediction with K = {1, 3, 5, 50}
# For each value of K, compute and report:
    # training MSE loss
    # validation MSE loss
    # test MSE loss
# Choose best k using validation error = 50
def PredictKnn(trainData , testData, trainTarget,  testTarget, K):
    """
    input:
        trainData: Data for training KNN
        testData: Data used in testing
        trainTarget: Targets used to create prediction.
        testTarget: Targets used to calculate loss. 
    output:
        loss: The mean squared loss of the prediction.
    """
    D = PairwiseDistances(testData, trainData)
    topK, indices = ChooseNearestNeighbours(D, K)
    # Select the proper outputs to be averaged
    # from the target values and average them
    trainTargetSelectedAveraged = tf.reduce_mean( \
    	tf.gather(trainTarget, indices), 1)
    # Calculate the loss from the actual values
    loss = tf.reduce_mean(tf.square(tf.sub( \
    	trainTargetSelectedAveraged, testTarget)))
    return loss

# Plot the prediction function for x = [0, 11]
def PredictedValues(x, trainData, trainTarget, K):
    """
    Plot the predicted values
    input:
        x = test target to plot and predict
    """
    D = PairwiseDistances(x, trainData)
    topK, indices = ChooseNearestNeighbours(D, K)
    predictedValues = tf.reduce_mean( \
    	tf.gather(trainTarget, indices), 1)
    return predictedValues
\end{minted}
\begin{table}[ht]
\centering % used for centering table
\caption{KNN and Loss} % title of Table
\label{table:KLoss} % is used to refer this table in the text
\begin{tabular}{c c c c} % centered columns (4 columns)
\hline % single horizontal line
$k$ & Training MSE Loss & Validation MSE Loss & Test MSE Loss \\ [0.5ex] 
\hline
1 & 0.000  & 0.272 & 0.311 \\
3 & 0.105  & 0.326 & 0.145 \\
5 & 0.119  & 0.310 & 0.178 \\
50 & 1.248 & 1.229 & 0.707 \\ [1ex] % [1ex] adds vertical space
\hline % single line
\end{tabular}
\end{table}

The best value of $k$ is based on one that gives the lowest validation MSE loss. In this case, the best $k$ is found to be $k = 1$. \\

By inspecting the plot, a $k$ value of 3 or 5 would be picked, instead of $k = 1$. The reason for this is simply because $k = 1$ is overfitting the training data, and would have modeled noises in the data. This is confirmed when comparing between the Test MSE losses for different values of $k$.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=\textwidth]{Graphics/A1Q1}
    \caption{\label{figure:kNN} k-NN regression on data1D for various values of k}
\end{figure}

\clearpage

%-----------------------------------------------------
\subsection{Soft kNN and Gaussian Processes}
%--------------------------------

As shown in Table \ref{table:KLossTest} , the Soft Decision performs better than the Gaussian Process Regression Model as it has a lower Test Mean Squared Error Loss. The algorithm was run on the test set.

\begin{table}[ht]
\centering % used for centering table
\caption{Loss on Test Set} % title of Table
\label{table:KLossTest} % 
\begin{tabular}{c c} % centered columns (4 columns)
\hline % single horizontal line
$Algorithm$ & Test MSE Loss \\ [0.5ex] 
\hline
Soft Decision & 0.159  \\
Gaussian Process Regression & 0.380 \\ [1ex] % [1ex] adds vertical space
\hline % single line
\end{tabular}
\end{table}

\begin{figure}[!htb]
	\centering
	\includegraphics[width=\textwidth]{Graphics/A1Q1_4a}
    \caption{\label{figure:abc} Soft Decision kNN on Test Set}
\end{figure}

\begin{figure}[!htb]
	\centering
	\includegraphics[width=\textwidth]{Graphics/A1Q1_4}
    \caption{\label{figure:abc} Gaussian Process Regression on Test Set}
\end{figure}

\begin{minted}[linenos=true,bgcolor=bg,numberblanklines=true,showspaces=false,breaklines=true]{python}
{python}
def SortData(inputVal, outputVal):
    """
    This sorts a given test set by the dataValue before plotting it.
    """
    # Sort across the data values on both pairs of sets
    # Sort in numpy itself to not lose precision. 
    p = np.argsort(inputVal, axis=0)
    inputVal = np.array(inputVal)[p]
    outputVal = np.array(outputVal)[p]
    # Get rid of extra dimensions from np.argsort
    inputVal = inputVal[:, :,0]
    outputVal = outputVal[:, :,0]
    return inputVal, outputVal
\end{minted}

\subsubsection{Soft Decisions and Gaussian Process Regression}
\begin{minted}[linenos=true,bgcolor=bg,numberblanklines=true,showspaces=false,breaklines=true]{python}
{python}
# Predict values using soft decision
# 1.4.1.1 Soft Knn Decision

def PredictedValuesSoftDecision(x, trainData, trainTarget):
    # use hyper parameter of 100 as given by Jimmy. 
    hyperParam = 100
    # Compute pairwise differences. 
    D1 = PairwiseDistances(x, trainData)
    K1 =  tf.exp(-hyperParam*D1)
    # Get the sum term used for normalization
    sum1 = tf.reduce_sum(tf.transpose(K1), axis=0)
    # Reshape to enable broadcasting during division. 
    N = sum1.get_shape().as_list()[0]
    sum1 = tf.reshape(sum1, [N,1])
    # Normalize the data using broadcast
    # to calculate the final responsibility values
    rStar = tf.div(K1, sum1)
    # Calculate the predicted value 
    # using the new responsibilities, rStar
    predictedValues = tf.matmul(rStar,trainTarget)
    return predictedValues
\end{minted}

\begin{minted}[linenos=true,bgcolor=bg,numberblanklines=true,showspaces=false,breaklines=true]{python}
{python}
# Predict values using Gaussian
# 1.4.1.1 Gaussian Processes
def PredictedValuesGaussianProcesses(x, trainData, trainTarget):
    # use hyper parameter of 100 as given by Jimmy. 
    hyperParam = 100
    D1 = PairwiseDistances(x, trainData)
    K1 =  tf.exp(-hyperParam*D1)
    D2 = PairwiseDistances(trainData, trainData)
    K2 =  tf.matrix_inverse(tf.exp(-hyperParam*D2))
    # Calculate the responsibilites, rStar
    # by normalizing using the inverse ofK2. 
    rStar = tf.matmul(K1, K2)
    # Calculate the predicted value 
    # using the new responsibilities, rStar
    predictedValues = tf.matmul(rStar,trainTarget)
    return predictedValue
\end{minted}

%--------------------------------
\subsubsection{Conditional Distribution of a Gaussian}

% Defining custom commands for frequently encountered mathematical expressions
\newcommand{\yTrain}{\bm{y}_{train}}
	% EXTRA: Please comment if you know of a way to do the following better:
	\newcommand{\oneOne}{y^* y^*}
	\newcommand{\oneTwo}{y^* \yTrain}
	\newcommand{\twoOne}{\yTrain y^*}
	\newcommand{\twoTwo}{\yTrain \yTrain}
\newcommand{\sigmaMat}{\begin{bmatrix}
		\Sigma_{y^* y^*} & \Sigma_{y^* \yTrain} \\
    	\Sigma_{\yTrain y^*} & \Sigma_{\yTrain \yTrain}
	\end{bmatrix}
}
\newcommand{\lambdaMat}{\begin{bmatrix}
		\Lambda_{y^* y^*} & \Lambda_{y^* \yTrain} \\
    	\Lambda_{\yTrain y^*} & \Lambda_{\yTrain \yTrain}
	\end{bmatrix}
}
\newcommand{\Amat}{\Sigma_{\oneOne}}
\newcommand{\Bmat}{\Sigma_{\oneTwo}}
\newcommand{\Cmat}{\Sigma_{\twoOne}}
\newcommand{\Dmat}{\Sigma_{\twoTwo}}
%--------------------------------------------------------------------

Given an $M$ + 1 Gaussian random vector:
\begin{align}
	\bm{y} = \begin{bmatrix} 
    		y^* \\ y^{(1)} \\ \vdots \\ y^{(m)}
        \end{bmatrix}
        \sim \mathcal{N} \left( \bm{0}, \Sigma \right) 
\end{align}
Splitting the vector into two parts as follows and describing the resulting distribution 
using stacked block notation:
\begin{alignat}{3}
 	\bm{y} = \begin{bmatrix}
        	y^* \\ \yTrain
        \end{bmatrix}
        & \sim \mathcal{N} \left( \begin{bmatrix}
        		0 \\ \bm{0}
             \end{bmatrix}, \sigmaMat \right) \\
        & \sim \mathcal{N} \left( \begin{bmatrix}
        		0 \\ \bm{0}
             \end{bmatrix}, \Sigma = \Lambda^{-1} = \lambdaMat^{-1} \right)
\end{alignat}

Given that $\yTrain$ is observed, find $P(y^* | \yTrain) \sim \mathcal{N} 
\left( y^*; \mu^*, \Sigma^* \right)$. \vspace{1em}

By completing the squares on the quadratic term of the exponent for the multivariate Gaussian equation,
\begin{align}
\begin{split}
	& - \frac{1}{2} ( \bm{y} - \bm{\mu} )^{T} \bm{\Sigma}^{-1} ( \bm{y} - \bm{\mu} ) \\
    & = - \frac{1}{2} \bm{y}^{T} \bm{\Sigma}^{-1} \bm{y}^{T} \\
    & = - \frac{1}{2} \begin{bmatrix} y^{*T} & \yTrain^{T} \end{bmatrix}
    	\lambdaMat \begin{bmatrix} y^* \\ \yTrain \end{bmatrix} \\
    & = - \frac{1}{2} \begin{bmatrix} 
    		y^{*T} \Lambda_{\oneOne} + \yTrain \Lambda_{\twoOne} 
            & y^{*T} \Lambda_{\oneTwo} + \yTrain \Lambda_{\twoTwo} 
    	\end{bmatrix} \begin{bmatrix} y^* \\ \yTrain \end{bmatrix} \\
    & \label{equation:Bonus_BeforeTranspose}= - \frac{1}{2} \left( 
    	y^{*T} \Lambda_{\oneOne} y^* 
        + \yTrain^{T} \Lambda_{\twoOne} y^*
        + y^{*T} \Lambda_{\oneTwo} \yTrain
        + \yTrain^{T} \Lambda_{\twoTwo} \yTrain
		\right)
\end{split}
\end{align}

Since $ \yTrain^{T} \Lambda_{\twoOne} y^* $ is a scalar and 
$\Lambda_{\twoOne}^{T} = \Lambda_{\oneTwo}$,
\begin{align}
	\left( \yTrain^{T} \Lambda_{\twoOne} y^* \right)^{T} = 
    y^{*T} \Lambda_{\oneTwo} \yTrain
\end{align}

Hence, equation \ref{equation:Bonus_BeforeTranspose} simplifies to:
\begin{align}
\begin{split}
%	& - \frac{1}{2} ( \bm{y} - \bm{\mu} )^{T} \bm{\Sigma}^{-1} ( \bm{y} - \bm{\mu} ) \\
	& - \frac{1}{2} \left(
    	y^{*T} \Lambda_{\oneOne} y^* 
    	+ 2 y^{*T} \Lambda_{\oneTwo} \yTrain
    	+ \yTrain^{T} \Lambda_{\twoTwo} \yTrain
    	\right) \\
    & \label{equation:Bonus_AfterTranspose} = - \frac{1}{2} y^{*T} \Lambda_{\oneOne} y^* 
    	- y^{*T} \Lambda_{\oneTwo} \yTrain
    	- \frac{1}{2} \yTrain^{T} \Lambda_{\twoTwo} \yTrain
\end{split}
\end{align}

Completing the square for a multivariate Gaussian quadratic term with \\
$\bm{x} \sim \mathcal{N} \left( \bm{\mu}, \bm{\Sigma} \right)$ yields:
\begin{align}
\begin{split}
	& - \frac{1}{2} ( \bm{x} - \bm{\mu} )^{T} \bm{\Sigma}^{-1} ( \bm{x} - \bm{\mu} ) \\
    & \label{equation:Bonus_GeneralGaussianSquareCompletion} = - \frac{1}{2} \bm{x}^{T} \Sigma^{-1} \bm{x} + \bm{x}^{T} \Sigma^{-1} \bm{\mu}
    - \frac{1}{2} \bm{\mu}^{T} \Sigma^{-1} \bm{\mu}
\end{split}
\end{align}

Given that $\yTrain$ and $\Sigma_{\twoTwo}$ are known, the conditional Gaussian Distribution $P(y^* | \yTrain)$ can be inferred by performing a term-by-term comparison between the terms of equation \ref{equation:Bonus_AfterTranspose} and those of equation \ref{equation:Bonus_GeneralGaussianSquareCompletion}.

Comparing terms that are of second order with respect to $y^*$,
\begin{align}
	\label{equation:Bonus_ConditionalSigma} \Sigma^* = \Lambda_{\oneTwo}^{-1}
\end{align}

Comparing terms that are of first order with respect to $y^*$,
\begin{align}
\begin{split}
	\Sigma^{*-1} \mu^{*} =& \ \Lambda_{\oneTwo} \yTrain \\
    \mu* =& \ \Sigma^* \Lambda_{\oneTwo} \yTrain \\
    \label{equation:Bonus_ConditionalMu} =& \Lambda_{\oneOne}^{-1} \Lambda_{\oneTwo} \yTrain
\end{split}
\end{align}

Using the matrix-inverse identity provided in Tutorial 3 (pg. 41), the terms $\Lambda_{\oneOne}$ and $\Lambda_{\oneTwo}$ can be expressed using terms in $\Sigma = \sigmaMat$,
\begin{align}
\begin{split}
	\Lambda_{\oneOne} =& \left( \Amat - \Bmat \Dmat^{-1} \Cmat \right)^{-1} \\
    \Lambda_{\oneTwo} =& - \left( \Dmat - \Cmat \Amat^{-1} \Bmat \right)^{-1} 
    	\Bmat \Dmat^{-1} \\
\end{split}
\end{align}

The results above allow equations \ref{equation:Bonus_ConditionalSigma} and \ref{equation:Bonus_ConditionalMu} to be expressed in terms of the original $\Sigma$ block terms:
\begin{align}
	\mu^* =& \ - \Bmat \Dmat^{-1} \yTrain \\
	\Sigma^* =& \ \Amat - \Cmat^{T} \Dmat^{-1} \Cmat
\end{align}
\begin{align}
	\therefore P(y^* | \yTrain) \sim \mathcal{N} 
    \left( - \Bmat \Dmat^{-1} \yTrain, \Amat - \Cmat^{T} \Dmat^{-1} \Cmat \right)
\end{align}
\clearpage
%------------------------------------------------------------------
\section{Linear and Logistic Regression}
%-----------------------------------------------------
\subsection{Geometry of Linear Regression}
%--------------------------------
\subsubsection{Convex Function}
Need to show if equation \ref{equation:LTwoPenalizedMSE} is a convex function of W using Jensen Inequality given by equation 
\ref{equation:JensenInequality}. \\

$M$ is the total number of training data. 
$N$ is the number of dimension for each training data. 
\begin{equation}
\label{equation:LTwoPenalizedMSE}
\begin{split}
\mathcal{L} &= \mathcal{L_D} + \mathcal{L_W} \\
	&= \sum_{m=1}^{M} \frac{1}{2M} ||W^{T}x^{(m)} + b - y^{(m)}||_{2}^{2} + \frac{\lambda}{2} ||W||_{2}^{2} \\
	&= \sum_{m=1}^{M} \frac{1}{2M} \left[\sum_{n=1}^{N}(W_{n}x^{(m)}_{n}) + b - y^{(m)}\right]^{2} + \frac{\lambda}{2} W^{T}W \\
    &= \sum_{m=1}^{M} \frac{1}{2M} \left[(W^{T}x^{(m)} + b - y^{(m)})^{2}\right] + \frac{\lambda}{2} W^{T}W 
\end{split}
\end{equation}

\begin{equation}
\label{equation:JensenInequality}
f(\alpha W_{1} + (1-\alpha)W_{2}) \leq \alpha f(W_{1}) + (1-\alpha)f(W_{2})
\end{equation}

Since the sum of two convex function is convex, we can prove each sum term, $\mathcal{L_D}$ and $\mathcal{L_W}$ on equation \ref{equation:LTwoPenalizedMSE} separately. 

You can easily prove that the sum of two convex is still convex by summing both sides of Jensen's Inequality for both convex functions and showing that Jensen's Inequality would still hold, indicating that the sum of the convex functions is still convex. This enables us to simplify the analysis by proving that each term is convex separately 

As a convex function divided by a positive value is still a convex function, we can ignore the division by $2M$ since $M > 0$ means that $\frac{1}{2M} > 0$. Similarly, $\frac{\lambda}{2} \geq 0$ and it only scales $W^{T}W$. You can divide both sides of Jensen's inequality by a positive constant and the Jensen's inequality would still hold, implying that the function is still convex when divided by a positive constant. This enables us to simplify the analysis by ignoring the positive constants. 

This means we just have to prove that equation \ref{equation:minimalLeftConvex} is convex and equation \ref{equation:minimalRightConvex} is convex. By doing so, we would have proven that equation \ref{equation:LTwoPenalizedMSE} is convex. 
\begin{align}
\label{equation:minimalLeftConvex} \mathcal{L_D} &= \left[W^{T}_{n}x^{(m)}_{n} + b_{n} - y^{(m)}_{n}\right]^{2} \\
\label{equation:minimalRightConvex} \mathcal{L_W} &= W^{T} W
\end{align}

\textbf{2.1.1.1 Proof of Convexity of $\mathcal{L_D}$ with respect to $W$}

Rearranging equation \ref{equation:JensenInequality} for $\mathcal{L_D}$,
\begin{align}
	\label{equation:Convexivity_ModifiedJensen} \mathcal{L_D}(\alpha W_1 + (1 - \alpha) W_2) - \alpha \mathcal{L_D}(W_1) 
    	- (1 - \alpha) \mathcal{L_D} \leq 0
\end{align}
\begin{multline}
	LHS \equiv \mathcal{L_D}[\alpha W_1 + (1 - \alpha) W_2 ] - 
    	\alpha \mathcal{L_D}(W_1) - (1 - \alpha) \mathcal{L_D}(W_2) \\
        = \{[\alpha W_1 + (1 - \alpha) W_2]^{T} x + (b - y)\}^{T} 
    	\{[\alpha W_1 + (1 - \alpha) W_2]^{T} x + (b - y)\} \\ 
        - \alpha [W_1^{T} x + (b - y)]^{T} [W_1^{T} x + (b - y)] \\
        - (1 -\alpha) [W_2^{T} x + (b - y)]^{T} [W_2^{T} x + (b - y)]
\end{multline}
\begin{multline}
    = [ x^{T} (\alpha W_1 + (1 - \alpha) W_2) + (b -y )^{T}]
    	\{[\alpha W_1 + (1 - \alpha) W_2]^{T} x + (b - y)\} \\
        - \alpha [x^{T} W_1 + (b - y)^{T}] [W_1^{T} x + (b - y)] \\
        - (1 -\alpha) [x^{T} W_2 + (b - y)^{T}] [W_2^{T} x + (b - y)]
\end{multline}
\begin{multline}
    = \{ x^{T} [\alpha W_1 + (1 - \alpha) W_2] [\alpha W_1^{T} + (1 - \alpha) W_2^{T}] x \\
    	+ 2 x^{T} [\alpha W_1 + (1 - \alpha) W_2] (b - y)  + (b - y)^{T} (b - y) \} \\
        - \alpha [ x^{T} W_1 W_1^{T} x + 2 x^{T} W_1 (b - y) + (b - y)^{T} (b - y)] \\
        - (1 - \alpha) [ x^{T} W_2 W_2^{T} x + 2 x^{T} W_2 (b - y) + (b - y)^{T} (b - y)] 
\end{multline}
Rearranging similar terms together,
\begin{multline}
	= \{ x^{T} \left[ \alpha^2 W_1 W_1^{T} + 2 \alpha (1 - \alpha) W_1^{T} W_2
    	+ (1 - \alpha)^2 W_2^{T} W_2 \right] x \\ 
        - \alpha (x^{T} W_1 W_1^{T} x) - (1 - \alpha)(x^{T} W_2 W_2{T} x) \} \\
        + \{ 2 \alpha x^{T} W_1 (b - y) - 2 \alpha x^{T} W_1 (b - y) \} \\
        + \{ 2 (1 - \alpha) x^{T} W_2 (b - y) - 2 (1 - \alpha) x^{T} W_2 (b - y) \} \\
        + \{ 1 - \alpha - (1 - \alpha) \} (b - y)^{T} (b - y)
\end{multline}
\begin{align}
\begin{split}
	&= (\alpha^2 - \alpha) x^{T} W_1 W_1^{T} x + 2 \alpha (1 - \alpha) x^{T} W_1^{T} W_2 x
    	+ [(1 - \alpha)^2 - (1 - \alpha)] x^{T} W_2 W_2^{T} x \\
    &= - \alpha (1 - \alpha) x^{T} W_1 W_1^{T} x + 2 \alpha (1 - \alpha) x^{T} W_1^{T} W_2 x
    	- \alpha (1 - \alpha) x^{T} W_2 W_2^{T} x \\
    &= - \alpha (1 - \alpha) [ x^{T} W_1 W_1^{T} x - 2 x^{T} W_1^{T} W_2 x + 
        x^{T} W_2 W_2^{T} x ] \\
    \label{equation:Convexivity_LD} &= - \alpha (1 - \alpha) [W_1^{T} x - W_2^{T} x]^{T} [W_1^{T} x - W_2^{T} x] \\
    &\leq 0 \equiv RHS
\end{split}
\end{align}
Equation \ref{equation:Convexivity_LD} is less than or equal to zero as $-\alpha (1 - \alpha) \leq 0 ;\ \forall \alpha \in [0, 1]$. Furthermore, the remaining quadratic term, $[W_1^{T} x - W_2^{T} x]^{T} [W_1^{T} x - W_2^{T} x] \ge 0$ since it is a square of the term $\forall \ \left( W_1^{T} x - W_2^{T} x \right) \in \mathbb{R}^N$. 

Hence, it has been shown that $\mathcal{L_D}$ is convex.
\vspace{1em}

\clearpage
\textbf{2.1.1.2 Proof of Convexity of $\mathcal{L_W}$ to $W$}

From the Left Hand Side of Equation \ref{equation:Convexivity_ModifiedJensen},
\begin{align}
\begin{split}
	LHS &\equiv \mathcal{L_W}[\alpha W_1 + (1 - \alpha) W_2 ] - 
    	\alpha \mathcal{L_W}(W_1) - (1 - \alpha) \mathcal{L_W}(W_2) \\
    &= [\alpha W_1 + (1 - \alpha) W_2]^{T} [\alpha W_1 + (1 - \alpha) W_2] 
    	- \alpha W_1^{T} W_1 - (1 - \alpha) W_2^{T} W_2 \\
    &= [\alpha W_1^{T} + (1 - \alpha) W_2^{T}] [\alpha W_1 + (1 - \alpha) W_2] 
    	- \alpha W_1^{T} W_1 - (1 - \alpha) W_2^{T} W_2 \\
    &= [\alpha^2 W_1^{T} W_1 + 2 \alpha (1 - \alpha) W_1^{T} W_2 + (1 - \alpha)^2 W_2^{T} W_2]
    	- \alpha W_1^{T} W_1 - (1 - \alpha) W_2^{T} W_2 \\
    &= (\alpha^2 - \alpha) x^{T} W_1 W_1^{T} x + 2 \alpha (1 - \alpha) x^{T} W_1^{T} W_2 x
    	+ [(1 - \alpha)^2 - (1 - \alpha)] x^{T} W_2 W_2^{T} x \\
    &= - \alpha (1 - \alpha) W_1^{T} W_1 + 2 \alpha (1 - \alpha) W_1^{T} W_2
    	- \alpha (1 - \alpha) W_2 W_2^{T}\\
    &= - \alpha (1 - \alpha) [ W_1^{T} W_1 - 2 W_1^{T} W_2 x + W_2^{T} W_2] \\
    \label{equation:Convexivity_LW} &= - \alpha (1 - \alpha) [W_1 - W_2]^{T} [W_1 - W_2] \\
    &\leq 0 \equiv RHS
\end{split}
\end{align}

Using a similar argument as that for Section 2.1.1.1, the loss function $\mathcal{L_W}$ is shown to be convex with respect to $W$. 

Therefore, the loss function $\mathcal{L}$ is a convex function with respect to W. 

\textbf{2.1.1.3 Proof of Convexity of $\mathcal{L}$ to $b$}

Instead of performing a similar proof for $b$, the bias can be thought of as the $(N+1)$th dimension of the weight. Thus, $b = W_{N+1}$ can be grouped together with matrix $W$, while the vector $x$ will be expanded to have $x_{N+1}^{m} = 1$  $\forall m$, as shown in equation \ref{equation:minimalLeftConvexSimplified}. 
Using the proof for the convexity of $\mathcal{L}$ with respect to $W$, by extension $\mathcal{L}$ is a convex function of the bias, $b$.

\begin{equation}
\label{equation:minimalLeftConvexSimplified}
L = \sum_{m=1}^{M} \frac{1}{2M} \sum_{n=1}^{N+1} \left( W^{T}_{n}x^{(m)}_{n} - y^{(m)}\right)^{2} + \frac{\lambda}{2} W^{T}W 
\end{equation}

\clearpage
 %--------------------------------
\subsubsection{DeNormalization}

Assuming $\lambda = 0$ from equation \ref{equation:LTwoPenalizedMSE} we get equation \ref{equation:denormalizationOri}. 

\begin{equation}
\label{equation:denormalizationOri}
\sum_{m=1}^{M} \frac{1}{2M} \left[(W^{T}x^{(m)} + b - y^{(m)})^{2}\right] 
\end{equation}


The original optimal weights and optimal bias are optimal to the non-transformed dataset. This means that the 
$\frac{\partial L}{\partial W} = 0$ and $\frac{\partial L}{\partial b} = 0$. More specifically, the partial gradient of the loss in equation \ref{equation:denormalizationOri} with respect to a specific weight $W_{n}$, 

\begin{equation}
\label{equation:partialLossOriW}
\frac{\partial L}{\partial W_{n}} = \sum_{m=1}^{M} \frac{1}{M} (\sum_{i = 1, i  \neq n}^{N}W_{i}x_{i}^{m} + W_{n}x_{n}^{m} + b - y^{m})x_{n}^{m} = 0
\end{equation}

and for the bias, $b$

\begin{equation}
\label{equation:partialLossOriB}
\frac{\partial L}{\partial b} = \sum_{m=1}^{M} \frac{1}{M} (\sum_{i = 1, i  \neq n}^{N}W_{i}x_{i}^{m} + W_{n}x_{n}^{m} + b - y^{m}) = 0
\end{equation}

A single dimension of $x$ scales by $\alpha > 1$ and shifts by $\beta > 1$. This is the same as de-normalizing the data point instead of normalizing it which normally deducts each data point by the mean and scaled by its variance. The reason for normalizing is to prevent any component from dominating the sum and to prevent the weights from learning the high bias that is not needed for prediction. As this is the opposite of normalization, it ends up training slower. However, this will not change the global minimum value as will be shown below. 

Note: This note is added after submission. Realize after 1 hour after submitting that the below 3 equations is flawed as sum of products is not product of sums. Should have not brought in the sum. With a similar proof, each individual gradient would be 0. So the sum of all the gradients would be 0. Hence, proving this theorem. So shouldn't have brought in the training case. Or wait, the original equation's gradient of the sum is 0, but doesn't show that each individual gradient itself is 0. So although the individual original gradient sums to 0, each individual gradient may not necessarily be 0. Therefore, this proof, doesn't work. Sigh.
Update: This equation should be true since all the terms inside is 0, which means we can factor out the 0's and group the x term on the right :)
The reason why all term inside is 0 is because it is optimal with respect to the weights. What this means is that it passes by every point. What this means is that it doesn't matter if I calculate the gradient with respect to {1, 2, ..., M} training points, the gradient must be 0 since it passes by all of them. Take batch size = 1, this means that each gradient must be = 0, therefore, we are able to group the X up. Proven! :) 
End of note after submission =D

Expanding equation \ref{equation:partialLossOriW} to account for the sum of $M$ training cases, we find equation \ref{equation:partialLossOriWExpand}. 

\begin{equation}
\label{equation:partialLossOriWExpand}
\frac{\partial L}{\partial W_{n}} = \frac{1}{M} (\sum_{i = 1, i  \neq n}^{N}W_{i}(x_{i}^{1} + ... + x_{i}^{M}) + W_{n}(x_{n}^{1} + ... + x_{n}^{M}) + Mb - (y^{1} + ... + y^{M}))(x_{n}^{1} + ... + x_{n}^{M}) = 0
\end{equation}

which we can rewrite in simpler terms as equation \ref{equation:partialLossOriWExpandSimplified}

\begin{equation}
\label{equation:partialLossOriWExpandSimplified}
\frac{\partial L}{\partial W_{n}} = \frac{1}{M} (\sum_{i = 1, i  \neq n}^{N}W_{i}\mathbf{x_{i}} + W_{n}(\mathbf{x_{n}}) + Mb - \mathbf{y})(\mathbf{x_{n}}) = 0
\end{equation}

Since equation \ref{equation:partialLossOriWExpandSimplified} is equal to $0$ for any value of $\mathbf{x_{n}}$, we must have that the inner term, 
$(\sum_{i = 1, i  \neq n}^{N}W_{i}\mathbf{x_{i}} + W_{n}(\mathbf{x_{n}}) + Mb - \mathbf{y})$ is equal 0. 

This is shown clearly in equation \ref{equation:InnerLossW}. 

\begin{equation}
\label{equation:InnerLossW}
(\sum_{i = 1, i  \neq n}^{N}W_{i}\mathbf{x_{i}} + W_{n}(\mathbf{x_{n}}) + Mb - \mathbf{y}) = 0
\end{equation}

As the model changes by scaling by a positive constant of $\alpha$ and shifted by a positive constant of $\beta$ within the square term, it does not change the minimum value as it is these transformations happen within the square term. As a result, the minimum value remains at 0 which is the lowest value for any square term. Therefore, the new global minimum value of the transformed convex loss function will remain the same compare to the original loss function global minimum. 

To illustrate, lets re-write the original loss function from equation \ref{equation:denormalizationOri} to account for the transformed Loss Function in equation \ref{equation:denormalizationTrans}. 

\begin{equation}
\label{equation:denormalizationTrans}
L^{'} = \sum_{m=1}^{M} \frac{1}{2M} \left[(\sum_{i = 1, i  \neq n}^{N}W_{i}^{i}x_{i}^{m'} + W_{n}^{'}(x_{n}^{m}) + Mb^{'} - y^{m})^{2}\right] 
\end{equation}

where the new variables are appended with a $'$ to show that their values could or has changed. This can be re-written as equation \ref{equation:denormalizationTransSimplified}. 

\begin{equation}
\label{equation:denormalizationTransSimplified}
L^{'} = \frac{1}{2M} \left[(\sum_{i = 1, i  \neq n}^{N}W_{i}^{'}\mathbf{x_{i}} + W_{n}^{'}(\mathbf{x_{n}^{'}}) + Mb^{'} - \mathbf{y})^{2}\right] 
\end{equation}

Taking the gradient with respect to $W$ results in  equation

\ref{equation:partialLossTransWExpandSimplified}. 
\begin{equation}
\label{equation:partialLossTransWExpandSimplified}
\begin{split}
\frac{\partial L^{'}}{\partial W_{n}} = 
\frac{1}{M} \left[(\sum_{i = 1, i  \neq n}^{N}W_{i}^{'}\mathbf{x_{i}} + W_{n}^{'}(\mathbf{x_{n}^{'}}) + Mb^{'} - \mathbf{y})\mathbf{x_{n}^{'}}\right] \\
=
\frac{1}{M} \left[(\sum_{i = 1, i  \neq n}^{N}W_{i}^{'}\mathbf{x_{i}} + W_{n}^{'}(\mathbf{\alpha x_{n} + \beta}) + Mb^{'} - \mathbf{y})(\mathbf{\alpha x_{n} + \beta}) \right] \\
\end{split}
\end{equation}

To achieve the global minimum, a possible solution assignment would be to re-substitute in the original values for $W_{i}^{'} = W_{i} \forall i \neq n$ and  $W_{n}^{'} = \frac{W_{n}}{\alpha}$ into equation \ref{equation:partialLossTransWExpandSimplified} as shown in equation \ref{equation:partialLossTransSub}. 

\begin{equation}
\label{equation:partialLossTransSub}
\begin{split}
\frac{\partial L^{'}}{\partial W_{n}} = \frac{1}{M} \left[(\sum_{i = 1, i  \neq n}^{N}W_{i}\mathbf{x_{i}} + \frac{W_{n}}{\alpha}(\mathbf{\alpha x_{n}} + M\beta) + Mb^{'} - \mathbf{y})(\mathbf{\alpha x_{n}} + M \beta) \right] \\
= 
\frac{1}{M} \left[(\sum_{i = 1, i  \neq n}^{N}W_{i}\mathbf{x_{i}} + W_{n}\mathbf{x_{n}} + \frac{W_{n} M \beta}{\alpha} + Mb^{'} - \mathbf{y})(\mathbf{\alpha x_{n}} + M \beta) \right] \\
=
\frac{1}{M} \left[(\sum_{i = 1, i \neq n}^{N}W_{i}\mathbf{x_{i}} + W_{n}\mathbf{x_{n}} + M (\frac{W_{n} \beta}{\alpha} + b^{'}) - \mathbf{y})(\mathbf{\alpha x_{n}} + M \beta) \right] 
\end{split}
\end{equation}

Further setting $b^{'} = b - \frac{W_{n} \beta}{\alpha}$ results  and using the result from equation \ref{equation:InnerLossW}, we get equation \ref{equation:partialLossTransSubB}.

\begin{equation}
\label{equation:partialLossTransSubB}
\begin{split}
\frac{\partial L^{'}}{\partial W_{n}} = \frac{1}{M} \left[(\sum_{i = 1, i \neq n}^{N}W_{i}\mathbf{x_{i}} + W_{n}\mathbf{x_{n}} + M (\frac{W_{n} \beta}{\alpha} + b - \frac{W_{n} \beta}{\alpha} ) - \mathbf{y})(\mathbf{\alpha x_{n}} + M \beta) \right] \\
= 
\frac{1}{M} \left[(\sum_{i = 1, i \neq n}^{N}W_{i}\mathbf{x_{i}} + W_{n}\mathbf{x_{n}} + Mb - \mathbf{y})(\mathbf{\alpha x_{n}} + M \beta) \right] \\
=
\frac{1}{M} \left[(0)(\mathbf{\alpha x_{n}} + M \beta) \right] \\ 
= 0
\end{split}
\end{equation}

Hence, from the final equation in \ref{equation:partialLossTransSubB}, the partial gradient is 0, suggesting that this assignment results in an optimal assignment of $W^{'}$ and $b^{'}$. 

As this assignment results in a similar inner term of equation \ref{equation:InnerLossW}, this suggest that the new loss function 
will end up being equal to the old loss function. Hence, the global minimum remains the same. 

The proof that this assignment works on bias is very similar and is omitted.

From this assignment of $W^{'}$ and $b^{'}$, it shows that the weight vector $W$ will move downwards to reach $W'$ and similarly for $b$ to $b^{'}$. The optimal weights  after learning will be lower compared to the optimal weights. The biased will be lower or higher or same depending on the sign of $W_{n}$ to converge into the biased learned from the non-transformed original data. 
%--------------------------------
\subsubsection{Regularization}

The new minimum value will increase as the weight and bias will fit to the regularize model more and less to the training set. 
This means the loss with respect to the training set only will increase as the regularization penalizes large values of $W$. However, this regularization helps the prediction to be more robust on the validation and the test set by preventing the model from over-fitting to the training set. 

The large weights will reduce as they are regularized whereas the small weights will increase. The bias should remain the same as it is not affected by the regularized term. 

The new minimum loss will increase as it is being compared to the training set labels, but is being optimize for both the training set and the regularized term. 
%--------------------------------
\subsubsection{Binary Classifiers for Multi-class Classification with D classes}

Given $D > 2$ and only able to use binary classifiers. A method to solve a multi-class classification task using a number of binary classifiers would be to assign a binary classifier to each class in the $D$ classes (see Figure \ref{figure:multiclass}). 

Each binary classifier would predict if a given test input belong to a specific class. This assumes that each test input can belong to more than 1 class as it is not constrained to belong to more than 1 class or no classes at all from this design. 

\begin{figure}[!htb]
	\centering
	\includegraphics[width=\textwidth]{Graphics/A1Q2_multiclass}
    \caption{\label{figure:multiclass}D binary classifiers for multi-class 
    	classification}
\end{figure}

\clearpage
%-----------------------------------------------------
\subsection{Stochastic Gradient Descent}
\label{section:convergence}
%-----------------------------------------------------
Before proceeding with our explanations for the subsequent sections, we would like to clarify on the convergence logic we selected for training our '3/5' digit classifier.

The model weights are randomly initialised. In our model, training convergence is implemented through early stopping. In our case, it is defined to be when the average validation MSE for the latest epoch is more than 0.99 of the average validation MSE for the previous epoch.

The logic is that when the average relative learnings between epochs is less than 1\%, the training is terminated prematurely for shorter computation time. This makes sense since the model is not learning much from the features. 

Training convergence also happens when the average MSE of one epoch is larger than its predecessor. This is done since an increase in the average validation MSE is a sign of over-fitting.

\subsubsection{Tuning Learning Rate, $\eta$}
Our programmed definition of convergence can be found in Section \ref{section:convergence}. 

The values $\eta$ was generated based on increasing orders of 1 (see Table \ref{table:etaAndNumUpdates}). Higher values of $\eta$ lead to fewer number of updates to reach convergence. However, from trial and error, it was discovered that there is an upper bound for $\eta$ to ensure training convergence. When the value of $\eta$ is more than 0.1 (i.e. 1, 10, etc.), the training does not converge but diverges instead as it overshoots past the minimum value. 

From our experiments, The best value for $\eta$ is selected to be 0.1 as it takes the lowest number of iterations to reach the lowest convergence value. The results also summarised in Figure \ref{figure:BestEta}.

\begin{table}[ht]
	\centering % used for centering table
    \caption{Number of updates until convergence for various values of 
    	$\eta$} % title of Table
	\label{table:etaAndNumUpdates} % is used to refer this table in the text
    \vspace{1em}
	\begin{tabular}{r r}
		\hline % single horizontal line
		$\eta$ & \textit{Number of Updates} \\ 
        [0.5ex] 
		\hline
		0.001 & 1863 \\
		0.01 & 337 \\
		0.1 & 57 \\
        $>$ 0.3 & \textit{N/A} \\
        [1ex] % [1ex] adds vertical space
		\hline % single line
	\end{tabular}
\end{table}
\begin{figure}[!htb]
	\centering
	\includegraphics[width=\textwidth]{Graphics/A1Q2_1}
    \caption{\label{figure:BestEta}Graph of training mean squared error
    	(MSE) against number of updates for the best learning rate found, 
    	$\eta = 0.1$. ($B = 50, \lambda = 1$)}
\end{figure}
\clearpage
%--------------------------------
\subsubsection{Mini-batch Size}
Our programmed definition of convergence can be found in Section \ref{section:convergence}. 

From Table \ref{table:etaBatchSizeAndNumUpdates}, the same pattern for $\eta$ is observed where higher values of $\eta$ lead to fewer number of updates. Meanwhile, there is an optimum value for batch size, $B$. The best mini-batch size is $B = 100$, which leads to the fewest number of updates, regardless of $\eta$. However, when the $B = 700$, early-stopping fails to occur in a reasonable time. 

Please refer to Figures \ref{figure:BatchSubplots_1} and \ref{figure:BatchSubplots_2} for more information.

\begin{table}[ht]
	\centering % used for centering table
    \caption{Number of updates until training convergence for various values of 
    	$\eta$ and $B$} % title of Table
	\label{table:etaBatchSizeAndNumUpdates} % is used to refer this table in the text
    \vspace{1em}
	\begin{tabular}{r|r r r}
		\toprule
		\multirow{2}{*}{$B$} & \multicolumn{3}{c}{$\eta$} \\
        & 0.001 & 0.01 & 0.1 \\
        \hline
        10 & 2871 & 491 & 211 \\
        50 & 1963 & 337 & 57 \\
        100 & 1114 & 274 & 43 \\
        700 & 6001 & 6001 & 6001 \\
        [1ex] % [1ex] adds vertical space
		\hline
	\end{tabular}
\end{table}
\clearpage
\begin{figure}[!htb]
	\centering
	\includegraphics[width=\textwidth]{Graphics/A1Q2_2_1}
    \caption{\label{figure:BatchSubplots_1}Subplots of training MSE 
    against number of updates for batch sizes, $B = 10, 50$ and learning 
    rates, $\eta$ = \textcolor{blue}{\textbf{0.001}}, \textcolor{orange}
    {\textbf{0.01}}, \textcolor{green}{\textbf{0.1}}. ($\lambda = 1$ for
    all cases)}
\end{figure}
\begin{figure}[!htb]
	\centering
	\includegraphics[width=\textwidth]{Graphics/A1Q2_2_2}
    \caption{\label{figure:BatchSubplots_2}Subplots of training MSE 
    against number of updates for batch sizes, $B = 100, 700$ and learning
    rates, $\eta$ = \textcolor{blue}{\textbf{0.001}}, \textcolor{orange}
    {\textbf{0.01}}, \textcolor{green}{\textbf{0.1}}. ($\lambda = 1$ for 
    all cases)}
\end{figure}
\clearpage
%--------------------------------
\subsubsection{Generalization}

Based on \textcolor{blue}{validation set} plot in Figure \ref{figure:modelAccuracies}, the best value for $\lambda$ is picked based on the highest validation accuracy obtained. As observed from Table \ref{table:modelAccuracies}, $\lambda$ = 0.001, 0.01 and 0.1 all give the best validation accuracy of 93.0\%. In this case, the best value of $\lambda$ chosen is 0.01, the middle ground between the three possible values.

As seen from the Figure \ref{figure:modelAccuracies}, higher values of $\lambda$ initially increases the test set accuracy. This is because $\lambda$ prevents over-fitting of the model to the training set by penalizing large values of the weights. From a bias-variance trade-off perspective, incorporating $\lambda$ effectively reduces the model variance at the cost of a slight increase in bias. This effectively leads to an overall increase in the test set accuracy. This is true up to $\lambda = 0.1$.

When $\lambda = 1$, the validation and test set accuracy drops significantly. For this high value of $\lambda$, the weights are being penalized too much and it prevents the model form effectively learning and capturing key features in the input data. From a bias-variance trade-off perspective, the high $\lambda$ value has increased the model bias significantly to the point there is a net decrease in the model's performance. 

$\lambda$ has to be tuned to the validation set instead of the training set. If $\lambda$ was tuned based on the training set, the model would further over-fitting the model to the training set. This is because $\lambda$ would have been tuned to optimise the value of the training MSE to data that was not used for training. In other words, it needs to perform well on data it has not seen before that is modeled by the validation set. 

\begin{table}[ht]
	\centering % used for centering table
    \caption{Validation and test set accuracies for various values of 
    	weight-decay regularizer, $\lambda$}
	\label{table:modelAccuracies} % is used to refer this table in the text
    \vspace{1em}
	\begin{tabular}{r|r r}
		\toprule
		$\lambda$ & Validation Accuracy & Test Accuracy \\
        \hline
        0 & 0.930 & 0.900 \\
        0.0001 & 0.910 & 0.900 \\
        0.001 & 0.930 & 0.910 \\
        0.01 & 0.930 & 0.910 \\
        0.1 & 0.930 & 0.910 \\
        1 & 0.830 & 0.795 \\
        [1ex] % [1ex] adds vertical space
		\hline
	\end{tabular}
\end{table}
\begin{figure}[!htb]
	\centering
	\includegraphics[width=\textwidth]{Graphics/A1Q2_3}
    \caption{\label{figure:modelAccuracies} Graph of 
    \textcolor{blue}{validation} and \textcolor{orange}{test} set accuracies
    against $\log_{10}(\lambda)$. ($\eta = 0.1, B = 50$)}
\end{figure} 
\clearpage
%------------------------------------------------------------------
\section{Appendices}
Dear Teaching Assistants, 
we implemented our code separately as we wanted to maximize our learning. The plots and code snippets pasted in this report come from two separate solutions which are both added as Appendices. Despite the different implementation, our results were very similar, indicating that our implementation should be correct. 

For Stochastic Gradient Descent implementation, we started from Jimmy's posted code as a starter code and work from there for this assignment as suggested from Jimmy. 

\subsection{Entire Code 1: Chee Loong Soon's version}
\begin{minted}[linenos=true,bgcolor=bg,numberblanklines=true,showspaces=false,breaklines=true]{python}
{python}

# Assignment 1 
# Optimization
# Early Stopping
# Learning rate decay
# Momentum

import tensorflow as tf
import numpy as np
import sys

# 1.2 Euclidean Distance Function 
# 1.2.2 Pairwise Distances
# Write a vectorized Tensorflow Python function that implements
# the pairwise squared Euclidean distance function for two input matrices.
# No Loops and makes use of Tensorflow broadcasting.
def PairwiseDistances(X, Z):
    """
    input:
        X is a matrix of size (B x N)
        Z is a matrix of size (C x N)
    output:
        D = matrix of size (B x C) containing the pairwise Euclidean distances
    """
    B = X.get_shape().as_list()[0]
    N = X.get_shape().as_list()[1]
    C = Z.get_shape().as_list()[0]
    # Ensure the N dimensions are consistent
    assert  N == Z.get_shape().as_list()[1]
    # Reshape to make use of broadcasting in Python
    X = tf.reshape(X, [B, 1, N])
    Z = tf.reshape(Z, [1, C, N])
    # The code below automatically does broadcasting
    D = tf.reduce_sum(tf.square(tf.sub(X, Z)), 2)
    return D

# 1.3 Making Predictions
# 1.3.1 Choosing nearest neighbours
# Write a vectorized Tensorflow Python function that takes a pairwise distance matrix
# and returns the responsibilities of the training examples to a new test data point. 
# It should not contain loops.
# Use tf.nn.top_k
def ChooseNearestNeighbours(D, K):
    """
    input:
        D is a matrix of size (B x C)
        K is the top K responsibilities for each test input
    output:
        topK are the value of the squared distances for the topK
        indices are the index of the location of these squared distances
    """
    # Take topK of negative distances since it is the closest data.
    topK, indices = tf.nn.top_k(tf.neg(D), K)
    return topK, indices

# 1.3.2 Prediction
# Compute the k-NN prediction with K = {1, 3, 5, 50}
# For each value of K, compute and report:
    # training MSE loss
    # validation MSE loss
    # test MSE loss
# Choose best k using validation error = 50
def PredictKnn(trainData , testData, trainTarget,  testTarget, K):
    """
    input:
        trainData
        testData
        trainTarget
        testTarget
    output:
        loss
    """
    D = PairwiseDistances(testData, trainData)
    topK, indices = ChooseNearestNeighbours(D, K)
    # Select the proper outputs to be averaged from the target values and average them
    trainTargetSelectedAveraged = tf.reduce_mean(tf.gather(trainTarget, indices), 1)
    # Calculate the loss from the actual values
    # Divide by 2.0 since it's average over 2M instead of M where M = number of training data.
    loss = tf.reduce_mean(tf.square(tf.sub(trainTargetSelectedAveraged, testTarget)))/2.0
    return loss

# Plot the prediction function for x = [0, 11] on training data.
def PredictedValues(x, trainData, trainTarget, K):
    """
    Plot the predicted values
    input:
        x = test target to plot and predict
    """
    D = PairwiseDistances(x, trainData)
    topK, indices = ChooseNearestNeighbours(D, K)
    predictedValues = tf.reduce_mean(tf.gather(trainTarget, indices), 1)
    return predictedValues

# 1.4 Soft-Knn & Gaussian Processes
# 1.4.1.1 Soft Decisions
# Write a Tensorflow python program based on the soft k-NN model to compute 
# predictions on the data1D.npy dataset. 
# Set lambda = 100 NOT 10 as given in assignment handout
# and plot the test-set prediction of the model. 

# Predict values using soft decision
def PredictedValuesSoftDecision(x, trainData, trainTarget):
    hyperParam = 100
    D1 = PairwiseDistances(x, trainData)
    K1 =  tf.exp(-hyperParam*D1)
    sum1 = tf.reduce_sum(tf.transpose(K1), axis=0)
    N = sum1.get_shape().as_list()[0]
    sum1 = tf.reshape(sum1, [N,1])
    rStar = tf.div(K1, sum1)
    predictedValues = tf.matmul(rStar,trainTarget)
    return predictedValues

# Predict values using Gaussian
# 1.4.1.1 Gaussian Processes
def PredictedValuesGaussianProcesses(x, trainData, trainTarget):
    hyperParam = 100
    D1 = PairwiseDistances(x, trainData)
    K1 =  tf.exp(-hyperParam*D1)
    D2 = PairwiseDistances(trainData, trainData)
    K2 =  tf.matrix_inverse(tf.exp(-hyperParam*D2))
    rStar = tf.matmul(K1, K2)
    predictedValues = tf.matmul(rStar,trainTarget)
    return predictedValues

# Comment on the difference you observe between two programs
# Gaussian has higher loss. 

# 2 Linear and Logistic Regression
# 2.2 Stochastic Gradient Descent
# Implement linear regression and stochastic gradient descent algorithm 
# with mini-batch size B = 50.
def buildGraph(learningRate, weightDecayCoeff):
    # Variable creation
    W = tf.Variable(tf.truncated_normal(shape=[64, 1], stddev=0.5), name='weights')
    b = tf.Variable(0.0, name='biases')
    X = tf.placeholder(tf.float32, [None, 64], name='input_x')
    y_target = tf.placeholder(tf.float32, [None,1], name='target_y')
    weightDecay = tf.div(tf.constant(weightDecayCoeff),tf.constant(2.0))
    # Graph definition
    y_predicted = tf.matmul(X,W) + b 
    # Error definition
    meanSquaredError = tf.reduce_mean(tf.reduce_mean(tf.square(y_predicted - y_target), 
                                                reduction_indices=1, 
                                                name='squared_error'), 
                                  name='mean_squared_error')
    weightDecayMeanSquareError = tf.reduce_mean(tf.reduce_mean(tf.square(weightDecay)))
    weightDecayTerm = tf.multiply(weightDecay, weightDecayMeanSquareError)
    meanSquaredError = tf.add(meanSquaredError,weightDecayTerm)

    # Training mechanism
    optimizer = tf.train.GradientDescentOptimizer(learning_rate = learningRate)
    train = optimizer.minimize(loss=meanSquaredError)
    return W, b, X, y_target, y_predicted, meanSquaredError, train


def ShuffleBatches(trainData, trainTarget):
    rngState = np.random.get_state()
    np.random.shuffle(trainData)
    np.random.set_state(rngState)
    np.random.shuffle(trainTarget)
    return trainData, trainTarget

def LinearRegression(trainData, trainTarget, validData, validTarget, testData, testTarget):
    figureCount = 30
    # 2.2.3 Generalization (done by partner) 
    # Run SGD with B = 50 and use validation performance to choose best weight decay coefficient
    # from weightDecay = {0., 0.0001, 0.001, 0.01, 0.1, 1.}
    # Plot weightDecay vs test set accuracy. (Done by partner) 
    weightDecayTrials= [0.0, 0.0001, 0.0001, 0.01, 0.1, 1.0]
    # Plot total loss function vs number of updates for the best learning rate found
    learningRateTrials = [0.1, 0.01, 0.001]
    # 2.2.2 Effect of the mini-batch size
    # Run with Batch Size, B = {10, 5, 100, 700} and tune the learning rate separately for each mini-batch size.
    # Plot  the total loss function vs the number of updates for each mini-batch size.
    miniBatchSizeTrials = [10, 50, 100, 700]
    learningRate = 0.01
    miniBatchSize = 10
    weightDecayCoeff = 1.0
    # for weightDecayCoeff in weightDecayTrials:
    for miniBatchSize in miniBatchSizeTrials:
        for learningRate in learningRateTrials:
            # Build computation graph
            W, b, X, y_target, y_predicted, meanSquaredError, train = buildGraph(learningRate, weightDecayCoeff)
            # Initialize session
            init = tf.global_variables_initializer()
            sess = tf.InteractiveSession()
            sess.run(init)
            initialW = sess.run(W)  
            initialb = sess.run(b)

            # print "Initial weights: %s, initial bias: %.2f", initialW, initialb
            # Training model
            numEpoch = 200
            currEpoch = 0
            wList = []

            xAxis = []
            yTrainErr = []
            yValidErr = []
            yTestErr = []
            numUpdate = 0
            step = 0
            errTrain = -1 
            errValid = -1 
            errTest = -1 
            while currEpoch <= numEpoch:
                # Shuffle the batches and return
                trainData, trainTarget = ShuffleBatches(trainData, trainTarget)
                step = 0 
                # Full batch
                while step*miniBatchSize < 700:
                    _, errTrain, currentW, currentb, yhat = sess.run([train, meanSquaredError, W, b, y_predicted], feed_dict={X: trainData[step*miniBatchSize:(step+1)*miniBatchSize], y_target: trainTarget[step*miniBatchSize:(step+1)*miniBatchSize]})
                    wList.append(currentW)
                    #if not (step*miniBatchSize % 50):
                    #    print "Iter: %3d, MSE-train: %4.2f, weights: %s, bias: %.2f", step, err, currentW.T, currentb
                    step = step + 1
                    xAxis.append(numUpdate)
                    numUpdate += 1
                    yTrainErr.append(errTrain)
                    errValid = sess.run(meanSquaredError, feed_dict={X: validData, y_target: validTarget})
                    errTest = sess.run(meanSquaredError, feed_dict={X: testData, y_target: testTarget})
                    yValidErr.append(errValid)
                    yTestErr.append(errTest)
                # Testing model
                # TO know what is being run
                currEpoch += 1
            print "LearningRate: " , learningRate, " Mini batch Size: ", miniBatchSize
            print "Iter: ", numUpdate
            print "Final Train MSE: ", errTrain
            print "Final Valid MSE: ", errValid
            print "Final Test MSE: ", errTest
            import matplotlib.pyplot as plt
            plt.figure(figureCount)
            figureCount = figureCount + 1
            plt.plot(np.array(xAxis), np.array(yTrainErr))
            plt.savefig("TrainLossLearnRate" + str(learningRate) + "Batch" + str(miniBatchSize) + '.png')

            plt.figure(figureCount)
            figureCount = figureCount + 1
            plt.plot(np.array(xAxis), np.array(yValidErr))
            plt.savefig("ValidLossLearnRate" + str(learningRate) + "Batch" + str(miniBatchSize) + '.png')
            plt.figure(figureCount)
            figureCount = figureCount + 1
            plt.plot(np.array(xAxis), np.array(yTestErr))
            plt.savefig("TestLossLearnRate" + str(learningRate) + "Batch" + str(miniBatchSize) + '.png')
    return

def SortData(inputVal, outputVal):
    """
    This sorts a given test set by the dataValue before plotting it.
    """
    p = np.argsort(inputVal, axis=0)
    inputVal = np.array(inputVal)[p]
    outputVal = np.array(outputVal)[p]
    inputVal = inputVal[:, :,0]
    outputVal = outputVal[:, :,0]
    return inputVal, outputVal

if __name__ == "__main__":
    print 'helloworld'
    N = 2 # number of dimensions
    B = 3 # number of test inputs (To get the predictions for all these inputs
    C = 2 # number of training inputs (Pick closest k from this C)
    X = tf.constant([1, 2, 3, 4, 5, 6], shape=[3, 2])
    Z = tf.constant([21, 22, 31, 32], shape=[2, 2])
    # Need to put seed so random_uniform doesn't generate new random values
    # each time you evaluate when you print, so then the values would be 
    # inconsistent as to what you would have used or checked
    #X = tf.random_uniform([B, N], seed=111)*30
    #Z = tf.random_uniform([C, N], seed=112)*30
    D = PairwiseDistances(X, Z)
    K = 1 # number of nearest neighbours
    # You calculate all the pairwise distances between each test input
    # and existing training input
    topK, indices = ChooseNearestNeighbours(D, K)
    # Prediction
    #for K in [1, 3, 5, 50]:
    for K in [1]:
        np.random.seed(521)
        Data = np.linspace(1.0 , 10.0 , num =100) [:, np.newaxis]
        Target = np.sin( Data ) + 0.1 * np.power( Data , 2) + 0.5 * np.random.randn(100 , 1)
        randIdx = np.arange(100)
        np.random.shuffle(randIdx)
        # data1D.npy
        trainData, trainTarget  = Data[randIdx[:5]], Target[randIdx[:5]]
        trainData, trainTarget  = Data[randIdx[:80]], Target[randIdx[:80]]
        validData, validTarget = Data[randIdx[80:90]], Target[randIdx[80:90]]
        testData, testTarget = Data[randIdx[90:93]], Target[randIdx[90:93]]
        testData, testTarget = Data[randIdx[90:100]], Target[randIdx[90:100]]

        #trainData, trainTarget = SortData(trainData, trainTarget)
        #validData, validTarget = SortData(validData, validTarget)
        testData, testTarget = SortData(testData, testTarget)


        # Convert to tensors from numpy
        trainData = tf.pack(trainData)
        validData = tf.pack(validData)
        testData = tf.pack(testData)
        trainTarget = tf.pack(trainTarget)
        validtarget = tf.pack(validTarget)
        testTarget = tf.pack(testTarget)
        trainMseLoss = PredictKnn(trainData, trainData, trainTarget, trainTarget, K)
        validationMseLoss = PredictKnn(trainData, validData, trainTarget, validTarget, K)
        testMseLoss = PredictKnn(trainData, testData, trainTarget, testTarget, K)
        init = tf.global_variables_initializer()
        '''
        with tf.Session() as sess:
            sess.run(init)
            print 'K ' + str(K)
            print 'trainMseLoss'
            print sess.run(trainMseLoss)
            print 'validationMseLoss'
            print sess.run(validationMseLoss)
            print 'testMseLoss'
            print sess.run(testMseLoss)
        '''
        # Plot the prediction for the x below
        x = np.linspace(0.0, 11.0, num=1000)[:, np.newaxis]
        xTensor = tf.pack(x)
        predictedValuesKnn = PredictedValues(xTensor, trainData, trainTarget, K)
        predictedValuesSoft = PredictedValuesSoftDecision(testData, trainData, trainTarget)
        predictedValuesGaussian = PredictedValuesGaussianProcesses(testData, trainData, trainTarget)
        lossSoft = tf.reduce_mean(tf.square(tf.sub(predictedValuesSoft, testTarget)))/2.0
        lossGaussian = tf.reduce_mean(tf.square(tf.sub(predictedValuesGaussian, testTarget)))/2.0
        import matplotlib.pyplot as plt
        plt.figure(0)
        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            plt.figure(K+100)
            plt.scatter(sess.run(trainData), sess.run(trainTarget))
            plt.plot(sess.run(xTensor), sess.run(predictedValuesKnn))
            fileName = str("KNN") + str(K) + str("trainingGraph.png")
            plt.savefig(fileName)

            # Plot for SoftDecision
            plt.figure(K+101)
            plt.title("Soft Decision kNN on Test Set, MSE = " + str(sess.run(lossSoft)))
            plt.xlabel("Data Value")
            plt.ylabel("Target Value")
            plt.scatter(sess.run(testData), sess.run(testTarget), label= "testValue")
            plt.plot(sess.run(testData), sess.run(predictedValuesSoft), label = "predicted")
            plt.legend()
            fileName = str("SoftDecision.png")
            plt.savefig(fileName)
            print 'SoftDecisionLoss'
            print sess.run(lossSoft)

            # Plot for Gaussian
            plt.figure(K+102)
            plt.title("Gaussian Process Regression on Test Set, MSE = " + str(sess.run(lossGaussian)))
            plt.xlabel("Data Value")
            plt.ylabel("Target Value")
            plt.scatter(sess.run(testData), sess.run(testTarget), label = "testValue")
            plt.plot(sess.run(testData), sess.run(predictedValuesGaussian), label = "predicted")
            plt.legend()
            fileName = str("ConditionalGaussian.png")
            plt.savefig(fileName)
            print 'ConditionalGaussianLoss'
            print sess.run(lossGaussian)
    # Part 2
    with np.load ("tinymnist.npz") as data :
        trainData, trainTarget = data ["x"], data["y"]
        validData, validTarget = data ["x_valid"], data ["y_valid"]
        testData, testTarget = data ["x_test"], data ["y_test"]
        LinearRegression(trainData, trainTarget,validData, validTarget, testData, testTarget)
\end{minted}

\subsection{Entire Code 2: FuYuan Tee's version}
\subsubsection{Question 1: k-Nearest Neighbour}
\begin{minted}[linenos=true,bgcolor=bg,numberblanklines=true,showspaces=false,breaklines=true]{python}
import tensorflow as tf
import numpy as np

import matplotlib.pyplot as plt

import plotly.plotly as py
import plotly.graph_objs as go
from plotly import tools

import plotly.offline as pyo
pyo.init_notebook_mode(connected=True)

# Generate data
np.set_printoptions(precision=3)
np.random.seed(521)

# Generating data
Data = np.linspace(1.0, 10.0, num=100) [:, np.newaxis]
Target = np.sin(Data) + 0.1 * np.power(Data, 2) \
     + 0.5 * np.random.randn(100, 1)
    
# Generating a random index
randIdx = np.arange(100)
np.random.shuffle(randIdx)

# Partitioning 100 datapoints into training, validation and 
# test sets consisting of 80, 10 and 10 points respectively.
trainData, trainTarget = Data[randIdx[:80]], Target[randIdx[:80]]
validData, validTarget = Data[randIdx[80:90]], Target[randIdx[80:90]]
testData, testTarget = Data[randIdx[90:]], Target[randIdx[90:]]

# Defining TensorFlow Variables

# Calculates pairwise squared Euclidean distance between matrices X and Z
# X is the input matrix which houses data points to be calculated, 
# which are calculated against reference datapoints in Z
def euclideanDistance(X, Z):
    if tf.TensorShape.num_elements(X.get_shape()) == 3:
        return tf.reduce_sum(tf.square(X - tf.transpose(Z)), axis=2) # Sums feature deviations for each variable
    else:
        return tf.square(X - tf.transpose(Z), name='Euclidean_Distance_Matrix')

# Produces a BxC responsibility vector
def responsibilityVector(D, k):
    def flatten(tensor):
        return tf.reshape(tensor, [-1])
    
    # Obtains indices of top-K points
    _, idx = tf.nn.top_k(tf.transpose(-D), k)
    
    # Creates a step sequence for M values repeated k times
    M = tf.shape(D)[1]
    step_seq = flatten(tf.transpose(tf.reshape(tf.tile(tf.range(0, M), [k]), [k, M])))
    
    # Form new index key compatible for subsequent sparse_to_dense op
    sparse_idx = tf.pack([step_seq, flatten(idx)], axis=1, name='Sparse_Indices')
    
    # Forms dense tensor
    return tf.sparse_to_dense(tf.cast(sparse_idx, tf.int32), \
                              tf.cast(tf.shape(tf.transpose(D)), tf.int32), \
                              tf.fill([tf.shape(sparse_idx)[0]], tf.divide(1.0, tf.cast(k, tf.float32))), \
                              validate_indices=False, \
                              name='Responsibility_Vector')

# Function that creates a TensorFlow model
def buildGraph(k_):    
    k = tf.constant(k_, name='k') # Hyperparameter
    X = tf.placeholder(tf.float32, shape=[None, None], name='Training_Data')
    Y = tf.placeholder(tf.float32, shape=[None, None], name='Training_Target')
    Z = tf.placeholder(tf.float32, shape=[None, None], name='Input_Data')
    T = tf.placeholder(tf.float32, shape=[None, None], name='Input_Target')
    
    D = euclideanDistance(X, Z)
    R = responsibilityVector(D, k)
    
    Y_hat = tf.matmul(R, Y, name='Y_hat')
    MSE = tf.divide(tf.reduce_sum(tf.square(T - Y_hat)), \
                    tf.scalar_mul(2, tf.cast(tf.shape(Z)[0], tf.float32)), \
                    name='Mean_Squared_Error') # Half of theorectical MSE
    
    return X, Y, Z, T, Y_hat, MSE

# Function used to adapt 'kNN' function based on type of dataset used for calculation
# Mode consists of either ['train', 'validation', 'test', 'full']
def selectDataPartition(mode):
    
    inputData = trainData
    inputTarget = trainTarget
    
    if mode == 'train':
        dataSource = trainData
        targetSource = trainTarget
    elif mode == 'validation':
        dataSource = validData
        targetSource = validTarget
    elif mode == 'test':
        dataSource = testData
        targetSource = testTarget
    elif mode == 'full':
        dataSource = np.linspace(0.0, 11.0, num=1000) [:, np.newaxis]
        targetSource = np.sin(dataSource) + 0.1 * np.power(dataSource, 2) \
                       + 0.5 * np.random.randn(1000, 1)
        
        inputData = trainData
        inputTarget = trainTarget
        
    return inputData, inputTarget, dataSource, targetSource
    
def kNN(k, mode):
    X, Y, Z, T, Y_hat, MSE = buildGraph(k)
    
    with tf.Session() as sess:
#         print "k: %d, mode: %s" % (k, mode)
        inputData, inputTarget, dataSource, targetSource = selectDataPartition(mode)
            
        error, y_pred = sess.run([MSE, Y_hat], \
                                 feed_dict={X: inputData, Y: inputTarget, \
                                            Z: dataSource, T: targetSource
                                           })
        
    return error, np.transpose(np.append(inputData, inputTarget, axis=1)), np.transpose(np.append(dataSource, y_pred, axis=1))

# Main Function
k_list = [1, 3, 5, 50]
kNN_mode = ['train', 'validation', 'test', 'full']

MSE_list = []
targetSeries = []
predictionSeries = []

# Performs kNN based on various calculation modes
for i in range(4):
    for j in range(4):
        MSE, target, prediction = kNN(k_list[i], kNN_mode[j])
        MSE_list.append(MSE)
        targetSeries.append(target)
        predictionSeries.append(prediction)
MSE_list = np.reshape(MSE_list, (4, 4))

# Generate interactive Plotly graph
def generateVisualisation(targetSeries, predictionSeries, k_list):
    subplotTitleString = []
    for i in range(len(k_list)):
        subplotTitleString.append('k = %s' % str(k_list[i]))
    
    fig = tools.make_subplots(rows=2, cols=2, subplot_titles=(subplotTitleString))

    for i in range(len(k_list)):
        traceData = go.Scatter(
            x = targetSeries[i * 4][0],
            y = targetSeries[i * 4][1],
            marker = {'color': 'blue',
                      'symbol': 200},
            mode = 'markers',
            name = 'data_k=' + str(k_list[i])
        )
        tracePred = go.Scatter(
            x = predictionSeries[4 * (i + 1) - 1][0], 
            y = predictionSeries[4 * (i + 1) - 1][1],
            marker = {'color': 'green'},
            mode = 'lines',
            name = 'prediction_k=' + str(k_list[i])
        )

        fig.append_trace(traceData, i / 2 + 1, i % 2 + 1)
        fig.append_trace(tracePred, i / 2 + 1, i % 2 + 1)
          
        fig['layout']['xaxis'+str(i+1)].update(title='x')
        fig['layout']['yaxis'+str(i+1)].update(title='y')
        
    fig['layout'].update(height=900, width=950, title='k-NN Regression on data1D', showlegend=False)
    return py.iplot(fig, filename='A1Q1_kNN_subplot2x2')
    
# Output summary table
print 'Mean Squared Error Summary:'
for i in range(5):
    if i == 0:
        print "%3s %10s %10s %6s" % ('k', 'Training', 'Validation', 'Test')
    else:
        print "%3d %10.3f %10.3f %6.3f" % (k_list[i - 1], MSE_list[i - 1][0], MSE_list[i - 1][1], MSE_list[i - 1][2])

print "\n\n\n"
kNN_visuals = generateVisualisation(targetSeries, predictionSeries, k_list)
kNN_visuals
\end{minted}

\subsubsection{Question 2.2: Stochastic Gradient Descent}
\begin{minted}[linenos=true,bgcolor=bg,numberblanklines=true,showspaces=false,breaklines=true]{python}
{python}
# Import relevant packages
import tensorflow as tf
import numpy as np
import math

import time

# Non-interactive plotting
import matplotlib.pyplot as plt
from IPython import display

# Interactive plotting
from plotly import tools
import plotly.plotly as py
import plotly.graph_objs as go
import plotly.offline as pyo
from plotly.offline import download_plotlyjs

# Configure environment
np.set_printoptions(precision=3)
np.random.seed(521)

# Activate Plotly Offline for Jupyter
pyo.init_notebook_mode(connected=True)

# Load Tiny MNIST dataset
with np.load ("tinymnist.npz") as data:
    trainData, trainTarget = data ["x"], data["y"]
    validData, validTarget = data ["x_valid"], data ["y_valid"]
    testData, testTarget = data ["x_test"], data ["y_test"]
    
# Create Tensorflow Graph
def buildGraph(eta, lambda_):
    # Model inputs
    X = tf.placeholder(tf.float32, shape=[None, None], name='Input')
    Y = tf.placeholder(tf.float32, shape=[None, None], name='Target')
    
    # Model variables
    W = tf.Variable(tf.truncated_normal(shape=[64, 1], stddev=0.5), name='Weights')
    b = tf.Variable(0.0, name='Biases')
    
    # Model parameters
    eta = tf.constant(eta, name='Learning_Rate')
    lambda_ = tf.constant(lambda_, name='L2_Regularizer')
    
    # Predicted target
    Y_hat = tf.matmul(X, W)
    
    # Mean squared error
    MSE = tf.scalar_mul(tf.divide(1.0, tf.cast(tf.shape(X)[0], tf.float32)), \
                          tf.reduce_sum(tf.square(Y_hat - Y))) \
            + tf.scalar_mul(tf.divide(tf.cast(lambda_, tf.float32), 2.0), tf.matmul(tf.transpose(W), W))
    
    # Basic accuracy definition (n_correct / n_total)
    Y_hat_thresholded = tf.cast(tf.greater_equal(Y_hat, 0.5), tf.float32)
    accuracy = tf.divide(tf.reduce_sum(tf.cast(tf.equal(Y_hat_thresholded, Y), tf.float64)), \
                         tf.cast(tf.shape(X)[0], tf.float64))
    
    # Basic gradient descent optimizer
    optimizer = tf.train.GradientDescentOptimizer(eta).minimize(MSE)
    
    return W, b, X, Y, Y_hat, MSE, accuracy, optimizer
\end{minted}

\subsubsection{Question 2.2.1: Tuning the learning rate}
\begin{minted}[linenos=true,bgcolor=bg,numberblanklines=true,showspaces=false,breaklines=true]{python}
{python}
# Tune learning rate
MAX_ITER = 2000
def tuneLearningRate(etaList, batchSize=50, lambda_=1):    
    # Returns the i-th batch of training data and targets
    # Generates a new, reshuffled batch once all previous batches are fed
    def getNextTrainingBatch(currentIter):
        currentBatchNum = currentIter % (trainData.shape[0] / batchSize)
        if currentBatchNum == 0:
            np.random.shuffle(randIdx)
        # print 'Iteration: %4d, BatchCap: %2d, BatchNum: %2d' % (currentIter, trainData.shape[0] / batchSize, currentBatchNum)
        lowerBoundIdx = currentBatchNum * batchSize
        upperBoundIdx = (currentBatchNum + 1) * batchSize 
        return trainData[lowerBoundIdx:upperBoundIdx], trainTarget[lowerBoundIdx:upperBoundIdx]
    
    # Generate updated plots for training and validation MSE
    def plotMSEGraph(MSEList, param):
        label = '$\eta$ = ' + str(param)
        label_classification = ['train.', 'valid.']

        display.clear_output(wait=True)
        plt.figure(figsize=(8,5), dpi=200)
        
        for i, MSE in enumerate(MSEList):
            plt.plot(range(len(MSE)), MSE, '.', markersize=3, label=label+' '+label_classification[i])
        
        plt.axis([0, MAX_ITER, 0, np.amax(MSEList)])
        plt.legend()
        plt.show()
    
    # Calculates the ratio between the n-th average epoch MSE and the (n-1)-th average epoch MSE
    def ratioAverageEpochMSE(currentValidMSE):
        averageN = np.average(currentValidMSE[-(np.arange(epochSize - 1) + 1)])
        averageNlessOne = np.average(currentValidMSE[-(np.arange(epochSize - 1) + epochSize)])
        return averageN / averageNlessOne
    
    # Returns True if the average epoch validation MSE is at least 99% of the previous epoch average.
    # i.e. Returns True if the average learnings between epoch is less than +1%
    # Otherwise, returns False
    def shouldStopEarly(currentValidMSE):
        if currentValidMSE.shape[0] < 2 * epochSize:
            return False
        return True if (ratioAverageEpochMSE(currentValidMSE) >= 0.99) else False
    
    summaryList = []
    randIdx = np.arange(trainData.shape[0])
    epochSize = trainData.shape[0] / batchSize
    
    for eta in etaList:
        W, b, X, Y, Y_hat, MSE, accuracy, optimizer = buildGraph(eta, lambda_)

        with tf.Session() as sess:
            tf.global_variables_initializer().run()
            
            # Creates blank training and validation MSE arrays for the Session
            currentTrainMSE = np.array([])[:, np.newaxis]
            currentValidMSE = np.array([])[:, np.newaxis]
            
            # Runs update
            currentIter = 0
            while currentIter <= MAX_ITER:
                inputData, inputTarget = getNextTrainingBatch(currentIter)
                
                _, trainError = sess.run([optimizer, MSE], feed_dict={X: inputData, Y: inputTarget})
                validError = sess.run([MSE], feed_dict={X: validData, Y: validTarget})

                currentTrainMSE = np.append(currentTrainMSE, trainError)
                currentValidMSE = np.append(currentValidMSE, validError)
                
                # Update graph of training and validation MSE arrays
                if (currentIter < 3) or (currentIter % 500 == 0):
                    plotMSEGraph([currentTrainMSE, currentValidMSE], eta)
                
                # At every epoch, check for early stopping possibilty. If so, breaks from while loop
                if currentIter % epochSize == 0:
                    if shouldStopEarly(currentValidMSE):
                        break
                
                currentIter += 1
            
        # Save session results as dictionary and appends to MSEsummaryList
        summaryList.append(
            {
                'eta': eta,
                'B': batchSize,
                'lambda': lambda_,
                'numIter': currentIter + 1,
                'epoch': float(currentIter + 1) / (trainData.shape[0] / batchSize),
                'trainMSE': currentTrainMSE,
                'validMSE': currentValidMSE,
            }
        )
            
    return summaryList


# Main Function
etaList = [0.001, 0.01, 0.1]
tunedEtaSummary = tuneLearningRate(etaList)

# Output summary table
for summary in tunedEtaSummary:
    print 'eta: %.3f, numIter: %d, validMSE: %.3f' % (summary['eta'], summary['numIter'], summary['validMSE'][-1])
    
# Produce interactive graph for best learning rate
def etaIGraph(tunedEtaSummary):
    # Create plot for each summary
    traceList = []
    for summary in tunedEtaSummary:
        traceList.append(
            go.Scatter(
                x = range(summary['numIter'] + 1),
                y = summary['trainMSE'],
                name = '$\\eta = ' + str(summary['eta']) + '$'
            )
        )
    data = go.Data(traceList)
    
    # Create figure layout
    layout = go.Layout(
        title = '$\\textit{Training performance for various learning rates, } \\eta$',
        xaxis = {'title': 'Number of Updates'},
        yaxis = {'title': 'Training MSE'},
    )

    figure = go.Figure(data=data, layout=layout)
    return py.iplot(figure, filename='A1Q2.1_bestEtaGraph')
fig2_1 = etaIGraph(tunedEtaSummary)
fig2_1
\end{minted}

\subsubsection{Question 2.2.2: Effect of the mini-batch size}
\begin{minted}[linenos=true,bgcolor=bg,numberblanklines=true,showspaces=false,breaklines=true]{python}
{python}
# Tune the mini-batch size
MAX_ITER = 6000
def tuneBatchSize(etaList, batchSizeList, lambda_=1):
    # Returns the i-th batch of training data and targets
    # Generates a new, reshuffled batch once all previous batches are fed
    def getNextTrainingBatch(currentIter):
        currentBatchNum = currentIter % (trainData.shape[0] / batchSize)
        if currentBatchNum == 0:
            np.random.shuffle(randIdx)
        # print currentBatchNum + 1
        lowerBoundIdx = currentBatchNum * batchSize
        upperBoundIdx = (currentBatchNum + 1) * batchSize 
        return trainData[lowerBoundIdx:upperBoundIdx], trainTarget[lowerBoundIdx:upperBoundIdx]
    
    # Generate updated plots for training and validation MSE
    def plotMSEGraph(MSEList, param):
        label = '$B$ = ' + str(param[0]) + ', $\eta$: ' + str(param[1])
        label_classification = ['train.', 'valid.']

        display.clear_output(wait=True)
        plt.figure(figsize=(8,5), dpi=200)
        
        for i, MSE in enumerate(MSEList):
            plt.plot(range(len(MSE)), MSE, '.', markersize=3, label=label+'\n'+label_classification[i])
        
        plt.axis([0, MAX_ITER, 0, np.amax(MSEList)])
        plt.legend()
        plt.show()
    
    # Calculates the ratio between the n-th average epoch MSE and the (n-1)-th average epoch MSE
    def ratioAverageEpochMSE(currentValidMSE):
        averageN = np.average(currentValidMSE[-(np.arange(epochSize - 1) + 1)])
        averageNlessOne = np.average(currentValidMSE[-(np.arange(epochSize - 1) + epochSize)])
        return averageN / averageNlessOne
    
    # Returns True if the average epoch validation MSE is at least 99% of the previous epoch average.
    # i.e. Returns True if the average learnings between epoch is less than +1%
    # Otherwise, returns False
    def shouldStopEarly(currentValidMSE):
        if currentValidMSE.shape[0] < 2 * epochSize:
            return False
        return True if (ratioAverageEpochMSE(currentValidMSE) >= 0.99) else False
    
    summaryList = []
    randIdx = np.arange(trainData.shape[0])
    
    for batchSize in batchSizeList:
        epochSize = trainData.shape[0] / batchSize
        batchSummary = []
        for eta in etaList:
            W, b, X, Y, Y_hat, MSE, accuracy, optimizer = buildGraph(eta, lambda_)

            with tf.Session() as sess:
                tf.global_variables_initializer().run()

                # Creates blank training and validation MSE arrays for the Session
                currentTrainMSE = np.array([])[:, np.newaxis]
                currentValidMSE = np.array([])[:, np.newaxis]

                # Runs update
                currentIter = 0
                while currentIter <= MAX_ITER:
                    inputData, inputTarget = getNextTrainingBatch(currentIter)

                    _, trainError = sess.run([optimizer, MSE], feed_dict={X: inputData, Y: inputTarget})
                    validError = sess.run([MSE], feed_dict={X: validData, Y: validTarget})

                    currentTrainMSE = np.append(currentTrainMSE, trainError)
                    currentValidMSE = np.append(currentValidMSE, validError)

                    # Update graph of training and validation MSE arrays
                    if (currentIter < 3) or (currentIter % 500 == 0):
                        plotMSEGraph([currentTrainMSE, currentValidMSE], [batchSize, eta])

                    # At every epoch, check for early stopping possibilty. If so, breaks from while loop
                    if currentIter % epochSize == 0:
                        if shouldStopEarly(currentValidMSE):
                            break

                    currentIter += 1

            # Save session results as dictionary and appends to MSEsummaryList
            batchSummary.append(
                {
                    'eta': eta,
                    'B': batchSize,
                    'lambda': lambda_,
                    'numIter': currentIter + 1,
                    'epoch': float(currentIter + 1) / (trainData.shape[0] / batchSize),
                    'trainMSE': currentTrainMSE,
                    'validMSE': currentValidMSE,
                }
            )
        summaryList.append(batchSummary)
            
    return summaryList
    
# Main function
etaList = [0.001, 0.01, 0.1]
batchSizeList = [10, 50, 100, 700]
tunedBatchSizeSummary = tuneBatchSize(etaList, batchSizeList)

# Output summary table:
for batchSummary in tunedBatchSizeSummary:
    for summary in batchSummary:
        print 'B: %5d, eta: %5.3f, numIter: %5d, validMSE: %3.3f' % \
            (summary['B'], summary['eta'], summary['numIter'], summary['validMSE'][-1])

# Generate interactive Plotly plot
def batchSizeIGraphSubplot(tunedBatchSizeSummary):
    
    # Define subplot title
    subplotTitle = []
    for batchSummary in tunedBatchSizeSummary:
        subplotTitle.append('Batch Size, B  = ' + str(batchSummary[0]['B']))
    
    # Define subplot figure
    figure = tools.make_subplots(rows=4, cols=1, subplot_titles=(subplotTitle))
    
    # Define color list
    colorList = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']
    
    # Create plot for each summary
    for i, batchSummary in enumerate(tunedBatchSizeSummary):
        traceList = []
        for j, summary in enumerate(batchSummary):
            trace = go.Scatter(
                x = range(summary['numIter'] + 1),
                y = summary['trainMSE'],
                marker = {'color': colorList[j]},
                name = '$B=' + str(summary['B']) + ', \\eta=' + str(summary['eta']) + '$'
            )
            figure.append_trace(trace, i + 1, 1)
        figure['layout']['xaxis'+str(i+1)].update(title='Number of Updates')
        figure['layout']['yaxis'+str(i+1)].update(title='Training MSE')

    # Create figure layout
    figure['layout'].update(
        height = 1800,
        title = '$\\textit{Model training performance for various batch size, } B' + \
                '\\textit{, and learning rate, } \\eta$',
        showlegend = False
    )

    return py.iplot(figure, filename='A1Q2.2_batch_size_subplot2x2')
fig2_2_visual = batchSizeIGraphSubplot(tunedBatchSizeSummary)
fig2_2_visual
\end{minted}

\subsubsection{Question 2.2.3: Generalization}
\begin{minted}[linenos=true,bgcolor=bg,numberblanklines=true,showspaces=false,breaklines=true]{python}
{python}
# Tune weight decay regularizer
MAX_ITER = 2000
def tuneLambda(lambdaList, eta=0.1, batchSize=50):
    # Returns the i-th batch of training data and targets
    # Generates a new, reshuffled batch once all previous batches are fed
    def getNextTrainingBatch(currentIter):
        currentBatchNum = currentIter % (trainData.shape[0] / batchSize)
        if currentBatchNum == 0:
            np.random.shuffle(randIdx)
        lowerBoundIdx = currentBatchNum * batchSize
        upperBoundIdx = (currentBatchNum + 1) * batchSize 
        return trainData[lowerBoundIdx:upperBoundIdx], trainTarget[lowerBoundIdx:upperBoundIdx]
    
    # Generate updated plots for training and validation MSE
    def plotMSEGraph(MSEList, param):
        label = '$\lambda$ = ' + str(param)
        label_classification = ['train.', 'valid.']

        display.clear_output(wait=True)
        plt.figure(figsize=(8,5), dpi=200)
        
        for i, MSE in enumerate(MSEList):
            plt.plot(range(len(MSE)), MSE, '-', label=label+' '+label_classification[i])
        
        plt.axis([0, MAX_ITER, 0, np.amax(MSEList)])
        plt.legend()
        plt.show()
    
    # Calculates the ratio between the n-th average epoch MSE and the (n-1)-th average epoch MSE
    def ratioAverageEpochMSE(currentValidMSE):
        averageN = np.average(currentValidMSE[-(np.arange(epochSize - 1) + 1)])
        averageNlessOne = np.average(currentValidMSE[-(np.arange(epochSize - 1) + epochSize)])
        return averageN / averageNlessOne
    
    # Returns True if the average epoch validation MSE is at least 99% of the previous epoch average.
    # i.e. Returns True if the average learnings between epoch is less than +1%
    # Otherwise, returns False
    def shouldStopEarly(currentValidMSE):
        if currentValidMSE.shape[0] < 2 * epochSize:
            return False
        return True if (ratioAverageEpochMSE(currentValidMSE) >= 0.99) else False
    
    summaryList = []
    randIdx = np.arange(trainData.shape[0])
    epochSize = trainData.shape[0] / batchSize
    
    for lambda_ in lambdaList:
        W, b, X, Y, Y_hat, MSE, accuracy, optimizer = buildGraph(eta, lambda_)

        with tf.Session() as sess:
            tf.global_variables_initializer().run()
            
            # Creates blank training and validation MSE arrays for the Session
            currentTrainMSE = np.array([])[:, np.newaxis]
            currentValidMSE = np.array([])[:, np.newaxis]
            
            # Runs update
            currentIter = 0
            while currentIter <= MAX_ITER:
                inputData, inputTarget = getNextTrainingBatch(currentIter)
                
                _, trainError = sess.run([optimizer, MSE], feed_dict={X: inputData, Y: inputTarget})
                validError = sess.run([MSE], feed_dict={X: validData, Y: validTarget})

                currentTrainMSE = np.append(currentTrainMSE, trainError)
                currentValidMSE = np.append(currentValidMSE, validError)
                
                # Update graph of training and validation MSE arrays
                if (currentIter < 3) or (currentIter % 500 == 0):
                    plotMSEGraph([currentTrainMSE, currentValidMSE], lambda_)
                
                # At every epoch, check for early stopping possibilty. If so, breaks from while loop
                if currentIter % epochSize == 0:
                    if shouldStopEarly(currentValidMSE):
                        break
                
                currentIter += 1
            
            # Compute validation and test accuracy
            validAccuracy = sess.run(accuracy, feed_dict={X: validData, Y: validTarget})
            testAccuracy = sess.run(accuracy, feed_dict={X: testData, Y: testTarget})
            
        # Save session results as dictionary and appends to MSEsummaryList
        summaryList.append(
            {
                'eta': eta,
                'B': batchSize,
                'lambda': lambda_,
                'numIter': currentIter + 1,
                'epoch': float(currentIter + 1) / (trainData.shape[0] / batchSize),
                'trainMSE': currentTrainMSE,
                'validMSE': currentValidMSE,
                'validAccuracy': validAccuracy,
                'testAccuracy': testAccuracy
            }
        )
            
    return summaryList
    
# Main Function
lambdaList = [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]
tunedLambdaSummary = tuneLambda(lambdaList)

# Output summary table
for summary in tunedLambdaSummary:
    print 'lambda: %5.4f, numIter: %5d, validMSE: %5.3f, validAcc: %3.3f, testAcc: %3.3f' % \
        (summary['lambda'], summary['numIter'], summary['validMSE'][-1], summary['validAccuracy'], summary['testAccuracy'])
        
# Produce interactive Plotly graph
def lambdaIGraph(tunedLambdaSummary):
    # Create plot for each summary
    trace1 = go.Scatter(
        x = [np.log10(summary['lambda'] + 1e-5) for summary in tunedLambdaSummary],
        y = [summary['validAccuracy'] for summary in tunedLambdaSummary],
        name = 'Validation set accuracy'
    )
    
    trace2 = go.Scatter(
        x = [np.log10(summary['lambda'] + 1e-5) for summary in tunedLambdaSummary],
        y = [summary['testAccuracy'] for summary in tunedLambdaSummary],
        name = 'Test set accuracy'
    )
    
    data = go.Data([trace1, trace2])
    
    # Create figure layout
    layout = go.Layout(
        title = '$\\textit{Validation and Test set accuracy vs. } \\lambda$',
        xaxis = {'title': '$\\log_{10}(\\lambda)$'},
        yaxis = {'title': 'Model Accuracy'},
        annotations = [
            dict(
                text = '$\\textit{Used to represent } \\log_{10}(\\lambda=0)$',
                x = -5,
                y = 0.90,
            )
        ]
    )

    figure = go.Figure(data=data, layout=layout)
    return py.iplot(figure, filename='A1Q2.3_accuracyVsLambda')
fig2_3 = lambdaIGraph(tunedLambdaSummary)
fig2_3
\end{minted}

\end{document}
