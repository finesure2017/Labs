{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIE1621 Fall 2017\n",
    "# Project \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def pprint(obj, shapeOnly=True):\n",
    "    '''\n",
    "    For debugging, print statements with numpy variable names and shape\n",
    "    '''\n",
    "    def namestr(obj):\n",
    "        namespace = globals()\n",
    "        return [name for name in namespace if namespace[name] is obj]\n",
    "    # Assumes obj is a numpy array, matrix\n",
    "    try:\n",
    "        print(namestr(obj), obj.shape)\n",
    "    except:\n",
    "        try:\n",
    "            print(namestr(obj), \",\", len(obj))\n",
    "        except:\n",
    "            print(namestr(obj))\n",
    "    if not shapeOnly:\n",
    "        print(obj)\n",
    "\n",
    "\n",
    "covarianceMatrix = np.array([[0.02778, 0.00387, 0.00021],\n",
    "                             [0.00387, 0.01112, -0.00020],\n",
    "                             [0.00021, -0.00020, 0.00115]])\n",
    "\n",
    "expectedReturn = np.array([0.1073, 0.0737, 0.0627])\n",
    "\n",
    "stepSize = 1.0\n",
    "\n",
    "delta = 4.0\n",
    "\n",
    "maxIteration = 100\n",
    "\n",
    "# Randomly initialize to anything\n",
    "initialX = np.array([0.25, 0.25, 0.5])\n",
    "initialPi = 1.2\n",
    "\n",
    "def checkShape(x, u, sigma, delta):\n",
    "    if delta <= 0.0:\n",
    "        raise ValueError(\"Risk aversion, delta needs to be > 0\")\n",
    "    if delta <= 3.5 or delta >= 4.5:\n",
    "        raise ValueError(\"Risk aversion, delta needs to be between (3.5, 4.5)\")\n",
    "    if sigma.shape[0] != sigma.shape[1]:\n",
    "        raise ValueError(\"Covariance matrix must be square\")\n",
    "    if sigma.shape[0] != x.shape[0]:\n",
    "        raise ValueError(\"Dimensions dont match between sigma and x\")\n",
    "    if u.shape[0] != x.shape[0]:\n",
    "        raise ValueError(\"Dimensions dont match between u and x\")\n",
    "    '''\n",
    "    Not always met, only met after training is over\n",
    "    if np.sum(x) != 1.0:\n",
    "        raise ValueError(\"X must sum to 1.0\")\n",
    "    '''\n",
    "\n",
    "def computeF(x, u, sigma, delta):\n",
    "    '''\n",
    "    x is the proportion, the parameters we are changing to maximize f\n",
    "    u is the expected return\n",
    "    sigma is the covariance matrix\n",
    "    delta is the parameter that controls risk aversion, delta > 0\n",
    "    '''\n",
    "    checkShape(x, u, sigma, delta)\n",
    "    \n",
    "    firstTerm = 0.0\n",
    "    for (ui, xi) in zip(u, x):\n",
    "        firstTerm += ui * xi\n",
    "    secondTerm = 0.0\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[0]):\n",
    "            secondTerm += sigma[i][j] * x[i] * x[j]\n",
    "    secondTerm *= (float(delta)/float(2.0))\n",
    "            \n",
    "    f = firstTerm - secondTerm\n",
    "    return f\n",
    "\n",
    "def computeGrad(x, pi, u, sigma, delta):\n",
    "    checkShape(x, u, sigma, delta)\n",
    "    grad = np.zeros((x.shape[0] + 1, 1))\n",
    "    dLdx = np.zeros(x.shape)\n",
    "    for i in range(x.shape[0]): \n",
    "        secondIndex = (i + 1) % x.shape[0]\n",
    "        thirdIndex = (i + 2) % x.shape[0]\n",
    "        \n",
    "        dLdx[i] = u[i] + pi - ((delta)*((float(sigma[i][i]*x[i])/2.0) \\\n",
    "                                        + sigma[i][secondIndex]*x[secondIndex]) \\\n",
    "                               + sigma[i][thirdIndex] * x[thirdIndex])\n",
    "        grad[i] = dLdx[i]\n",
    "        \n",
    "    dLdPi = np.sum(x) - 1.0\n",
    "    grad[x.shape[0]] = dLdPi\n",
    "    \n",
    "    return grad; \n",
    "\n",
    "def computeHessian(x, pi, u, sigma, delta):\n",
    "    checkShape(x, u, sigma, delta)\n",
    "    dim = x.shape[0] + 1\n",
    "    hessian = np.zeros((dim, dim))\n",
    "    for i in range(x.shape[0]): \n",
    "        for j in range(x.shape[0]): \n",
    "            hessian[i][j] = (-1.0) * delta * sigma[i][j]\n",
    "            if i == j:\n",
    "                hessian[i][j] /= 2.0\n",
    "    for i in range(x.shape[0]): \n",
    "        hessian[x.shape[0]][i] = 1.0\n",
    "        hessian[i][x.shape[0]] = 1.0\n",
    "    return hessian\n",
    "\n",
    "x = initialX.copy()\n",
    "pi = initialPi \n",
    "f = computeF(x, expectedReturn, covarianceMatrix, delta)\n",
    "\n",
    "pprint(covarianceMatrix, False)\n",
    "pprint(expectedReturn, False)\n",
    "pprint(x)\n",
    "# pprint(f)\n",
    "\n",
    "grad = computeGrad(x, pi, expectedReturn, covarianceMatrix, delta)\n",
    "pprint(grad)\n",
    "\n",
    "hessian = computeHessian(x, pi, expectedReturn, covarianceMatrix, delta)\n",
    "pprint(hessian)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton Method\n",
    "x = initialX.copy()\n",
    "pi = initialPi \n",
    "currX = x.copy()\n",
    "currPi = pi\n",
    "\n",
    "for iteration in range(maxIteration):\n",
    "    x = currX.copy()\n",
    "    pi = currPi\n",
    "    f = computeF(x, expectedReturn, covarianceMatrix, delta)\n",
    "    constraint = np.sum(x)\n",
    "    pprint(iteration, False)\n",
    "    pprint(f, False)\n",
    "    pprint(constraint, False)\n",
    "    pprint(x, False)\n",
    "    pprint(pi, False)\n",
    "    \n",
    "    grad = computeGrad(x, pi, expectedReturn, covarianceMatrix, delta)\n",
    "    hessian = computeHessian(x, pi, expectedReturn, covarianceMatrix, delta)\n",
    "    \n",
    "    direction = np.dot(np.linalg.inv(hessian), grad)\n",
    "    pprint(direction, False)\n",
    "    \n",
    "    # Stopping condition\n",
    "    if np.linalg.norm(direction) < np.power(0.1, 10):\n",
    "        break\n",
    "    xAdd = np.reshape(direction[:-1].copy(), currX.shape)\n",
    "    \n",
    "    # You always deduct in Newton Method!\n",
    "    # As newton method accounts for correct direction regardless of max or min.\n",
    "    # Newton's method always searches for critical points. \n",
    "    currX -= stepSize * xAdd\n",
    "    currPi -= stepSize * direction[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steepest Descent Method\n",
    "# Note: NOT really steepest as the question asks to use stepsize of 1.0, so don't calculate stepsize. \n",
    "# Just do the normal gradient descent in the Machine Learning literature\n",
    "x = initialX.copy()\n",
    "pi = initialPi \n",
    "currX = x.copy()\n",
    "currPi = pi\n",
    "for iteration in range(maxIteration):\n",
    "    \n",
    "    x = currX.copy()\n",
    "    pi = currPi\n",
    "    \n",
    "    f = computeF(x, expectedReturn, covarianceMatrix, delta)\n",
    "    constraint = np.sum(x)\n",
    "    pprint(iteration, False)\n",
    "    pprint(f, False)\n",
    "    pprint(constraint, False)\n",
    "    pprint(x, False)\n",
    "    pprint(pi, False)\n",
    "    \n",
    "    grad = computeGrad(x, pi, expectedReturn, covarianceMatrix, delta)\n",
    "    # No hessian computation needed for gradient descent\n",
    "    \n",
    "    direction = grad\n",
    "    pprint(direction, False)\n",
    "    \n",
    "    # Stopping condition\n",
    "    if np.linalg.norm(direction) < np.power(0.1, 10):\n",
    "        break\n",
    "    xAdd = np.reshape(direction[:-1].copy(), currX.shape)\n",
    "    \n",
    "    # Gradient Descent diverges with constant step size\n",
    "    # stepSize = 0.00001 , can only converge with small step size but not OPTIMAL\n",
    "    # Need to add cause maximizing the function, NOT minimizing\n",
    "    currX += stepSize * xAdd\n",
    "    currPi += stepSize * direction[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quasi Newton Methods \n",
    "# BFGS\n",
    "x = initialX.copy()\n",
    "pi = initialPi \n",
    "\n",
    "def BFGSUpdate(H, y, s): \n",
    "    Hnext = H + (1.0/(np.dot(np.transpose(y), s))) * np.dot(y, np.transpose(y)) - (1.0/(np.dot(np.dot(np.transpose(s), H), s))) * np.dot(np.dot(np.dot(H, s), np.transpose(s)), H)\n",
    "    return Hnext\n",
    "\n",
    "H = np.eye(x.shape[0] + 1)\n",
    "xnew = np.zeros((x.shape[0]+1,1))\n",
    "for i in range(x.shape[0]):\n",
    "    xnew[i] = x[i]\n",
    "xnew[x.shape[0]] = pi\n",
    "\n",
    "pprint(xnew, False)\n",
    "for iteration in range(maxIteration):\n",
    "    xold = xnew\n",
    "    gradFk = computeGrad(xold[:-1], xold[-1], expectedReturn, covarianceMatrix, delta)\n",
    "    direction = np.dot(np.linalg.inv(H), gradFk)\n",
    "    # Stopping condition\n",
    "    if np.linalg.norm(direction) < np.power(0.1, 10):\n",
    "        break\n",
    "    #----------------------------------------------------------\n",
    "    xnew = xold - stepSize * direction\n",
    "    gradFk1 = computeGrad(xnew[:-1], xnew[-1], expectedReturn, covarianceMatrix, delta)\n",
    "    s = xnew - xold\n",
    "    y = gradFk1 - gradFk\n",
    "    # BFGS Update\n",
    "    Hnext = BFGSUpdate(H,y,s)\n",
    "    H = Hnext\n",
    "    \n",
    "    f = computeF(xnew[:-1], expectedReturn, covarianceMatrix, delta)\n",
    "    pprint(iteration, False)\n",
    "    pprint(f, False)\n",
    "    pprint(direction, False)\n",
    "    pprint(xnew, False)\n",
    "    '''\n",
    "    pprint(xold)\n",
    "    pprint(H)\n",
    "    pprint(gradFk)\n",
    "    pprint(gradFk1)\n",
    "    pprint(y)\n",
    "    pprint(s)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtracking \n",
    "\n",
    "# From wolfe's theorem to calculate backtracking\n",
    "# 0 < gamma < beta < 1\n",
    "gamma = np.power(10.0, -4) # Common for gamma to be within [0, 0.5] # Close to 0\n",
    "#gamma = 0.5\n",
    "beta = 0.9 # Close to 1\n",
    "\n",
    "def checkGammaBeta(gamma, beta):\n",
    "    if gamma <= 0.0:\n",
    "        raise ValueError(\"Gamma must be > 0.0\")\n",
    "    if beta >= 1.0:\n",
    "        raise ValueError(\"Beta must be < 1.0\")\n",
    "    if gamma >= beta:\n",
    "        raise ValueError(\"Gamma must be < Beta\")\n",
    "    return\n",
    "\n",
    "def backtrack(stepSize, gamma, beta, direction, gradOld, gradNew, fOld, fNew):\n",
    "    '''\n",
    "    Returns true if stepsize works\n",
    "    '''\n",
    "    checkGammaBeta(gamma, beta)\n",
    "    firstConditionLhs = np.dot(np.transpose(direction), gradNew)\n",
    "    firstConditionRhs = beta * np.dot(np.transpose(direction), gradOld)\n",
    "    # Condition iii) in notes\n",
    "    if firstConditionLhs > firstConditionRhs:\n",
    "        secondConditionLhs = fNew\n",
    "        secondConditionRhs = fOld + gamma * stepSize * np.dot(np.transpose(direction), gradOld)\n",
    "        secondConditionRhs = secondConditionRhs[0][0]\n",
    "        \n",
    "        #print(\"SECOND CONDITIONLHS\", secondConditionLhs)\n",
    "        #print(\"SECOND CONDITIONRHS\", secondConditionRhs)\n",
    "        # Condition iv) in notes\n",
    "        if secondConditionLhs > secondConditionRhs:\n",
    "            # If satisfy condition, return\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton Method with Backtracking\n",
    "currX = x.copy()\n",
    "currPi = pi\n",
    "for iteration in range(maxIteration):\n",
    "    x = currX.copy()\n",
    "    pi = currPi\n",
    "    f = computeF(x, expectedReturn, covarianceMatrix, delta)\n",
    "    constraint = np.sum(x)\n",
    "    pprint(iteration, False)\n",
    "    pprint(f, False)\n",
    "    pprint(constraint, False)\n",
    "    pprint(x, False)\n",
    "    pprint(pi, False)\n",
    "    \n",
    "    grad = computeGrad(x, pi, expectedReturn, covarianceMatrix, delta)\n",
    "    hessian = computeHessian(x, pi, expectedReturn, covarianceMatrix, delta)\n",
    "    \n",
    "    direction = np.dot(np.linalg.inv(hessian), grad)\n",
    "    pprint(direction, False)\n",
    "    \n",
    "    # \n",
    "    direction *= -1.0\n",
    "    tempDirection = direction\n",
    "    value = np.dot(np.transpose(tempDirection), grad)\n",
    "    value = value[0][0]\n",
    "    # Use less than due to maximization\n",
    "    if value < 0:\n",
    "        print(\"Value: \", value)\n",
    "        direction *= -1.0\n",
    "        tempDirection = direction\n",
    "        value = np.dot(np.transpose(tempDirection), grad)\n",
    "        value = value[0][0]\n",
    "        print(\"Value: \", value)\n",
    "        if value < 0:\n",
    "            raise ValueError(\"value does not satisfy less than 0 condition\")\n",
    "\n",
    "    # Stopping condition\n",
    "    if np.linalg.norm(direction) < np.power(0.1, 10):\n",
    "        print(\"Met stopping point, breaking!\")\n",
    "        break\n",
    "    xAdd = np.reshape(direction[:-1].copy(), currX.shape)\n",
    "    stepSize = 1.0\n",
    "    while stepSize > 0.0:\n",
    "        tempCurrX = currX + stepSize * xAdd\n",
    "        tempCurrPi = currPi + stepSize * direction[-1]\n",
    "        \n",
    "        newGrad = computeGrad(tempCurrX, tempCurrPi, expectedReturn, covarianceMatrix, delta)\n",
    "        newF = computeF(tempCurrX, expectedReturn, covarianceMatrix, delta)\n",
    "        if backtrack(stepSize, gamma, beta, tempDirection, grad, newGrad, f, newF):\n",
    "            break\n",
    "        # Maximum number of steps\n",
    "        stepSize -= 0.001\n",
    "    pprint(stepSize, False)\n",
    "    #'''\n",
    "    if stepSize <= 0.0:\n",
    "        raise ValueError(\"Stepsize doesnt work\")\n",
    "    #'''\n",
    "    currX += stepSize * xAdd\n",
    "    currPi += stepSize * direction[-1]\n",
    "    \n",
    "    \n",
    "    # You always deduct in Newton Method!\n",
    "    # As newton method accounts for correct direction regardless of max or min.\n",
    "    # Newton's method always searches for critical points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steepest Descent Method with Backtracking\n",
    "# Steepest Descent Method\n",
    "# Note: NOT really steepest as the question asks to use stepsize of 1.0, so don't calculate stepsize. \n",
    "# Just do the normal gradient descent in the Machine Learning literature\n",
    "x = initialX.copy()\n",
    "pi = initialPi \n",
    "currX = x.copy()\n",
    "currPi = pi\n",
    "for iteration in range(maxIteration):\n",
    "    \n",
    "    x = currX.copy()\n",
    "    pi = currPi\n",
    "    \n",
    "    f = computeF(x, expectedReturn, covarianceMatrix, delta)\n",
    "    constraint = np.sum(x)\n",
    "    pprint(iteration, False)\n",
    "    pprint(f, False)\n",
    "    pprint(constraint, False)\n",
    "    pprint(x, False)\n",
    "    pprint(pi, False)\n",
    "    \n",
    "    grad = computeGrad(x, pi, expectedReturn, covarianceMatrix, delta)\n",
    "    # No hessian computation needed for gradient descent\n",
    "    \n",
    "    direction = grad.copy()\n",
    "    direction *= -1.0\n",
    "    pprint(direction, False)\n",
    "    tempDirection = direction \n",
    "    value = np.dot(np.transpose(tempDirection), grad)\n",
    "    value = value[0][0]\n",
    "    if value <= 0:\n",
    "        print(\"Value: \", value)\n",
    "        direction *= -1.0\n",
    "        tempDirection = direction\n",
    "        pprint(direction, False)\n",
    "        value = np.dot(np.transpose(tempDirection), grad)\n",
    "        value = value[0][0]\n",
    "        print(\"Value: \", value)\n",
    "        if value <= 0:\n",
    "            raise ValueError(\"value does not satisfy less than 0 condition\")\n",
    "        \n",
    "    \n",
    "    # Stopping condition\n",
    "    if np.linalg.norm(direction) < np.power(0.1, 10):\n",
    "        break\n",
    "    xAdd = np.reshape(direction[:-1].copy(), currX.shape)\n",
    "    \n",
    "    stepSize = 1.0\n",
    "    while stepSize > 0.0:\n",
    "        tempCurrX = currX + stepSize * xAdd\n",
    "        tempCurrPi = currPi + stepSize * direction[-1]\n",
    "        \n",
    "        newGrad = computeGrad(tempCurrX, tempCurrPi, expectedReturn, covarianceMatrix, delta)\n",
    "        newF = computeF(tempCurrX, expectedReturn, covarianceMatrix, delta)\n",
    "        if backtrack(stepSize, gamma, beta, tempDirection, grad, newGrad, f, newF):\n",
    "            break\n",
    "        # Maximum number of steps\n",
    "        stepSize -= 0.0001\n",
    "    pprint(stepSize, False)\n",
    "    #'''\n",
    "    if stepSize <= 0.0:\n",
    "        raise ValueError(\"Stepsize doesnt work\")\n",
    "    #'''\n",
    "    \n",
    "    # Gradient Descent diverges with constant step size\n",
    "    # stepSize = 0.00001 , can only converge with small step size but not OPTIMAL\n",
    "    # Need to add cause maximizing the function, NOT minimizing\n",
    "    currX += stepSize * xAdd\n",
    "    currPi += stepSize * direction[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quasi Newton Methods with Backtracking\n",
    "# BFGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "# Reading in file \n",
    "\n",
    "projFolder = \"./projectFiles/\"\n",
    "extension = \".csv\"\n",
    "\n",
    "SamsungFile = projFolder + \"Samsung\" + extension\n",
    "FacebookFile = projFolder + \"Facebook\" + extension\n",
    "AMDFile = projFolder + \"AMD\" + extension\n",
    "SalesforceFile = projFolder + \"Salesforce\" + extension\n",
    "adjustedCloseIndex = 5\n",
    "\n",
    "maxIteration = 1000\n",
    "stepSize = 0.01\n",
    "\n",
    "def getClosingPrice(fileName):\n",
    "    firstLine = True\n",
    "    output = open(fileName, \"r\")\n",
    "    closingPrices = []\n",
    "    for line in output:\n",
    "        if firstLine:\n",
    "            firstLine = False\n",
    "            continue\n",
    "        values = line.split(\",\")\n",
    "        closingPrice = float(values[adjustedCloseIndex])\n",
    "        closingPrices.append(closingPrice)\n",
    "    return np.array(closingPrices)\n",
    "\n",
    "def getDailyReturns(closingPrice):\n",
    "    # Deduct 1 from shape as always need 2 days to get daily return\n",
    "    dailyReturns = np.zeros(closingPrice.shape[0] - 1)\n",
    "    for i in range(dailyReturns.size):\n",
    "        dailyReturns[i] = closingPrice[i+1] - closingPrice[i]\n",
    "    return dailyReturns\n",
    "\n",
    "def calculateExpectedReturn(arrOfClosingPrices):\n",
    "    expectedReturnPerStock = np.mean(arrOfClosingPrices, axis = 1)\n",
    "    return expectedReturnPerStock\n",
    "\n",
    "def calculateCovarianceMatrix(arrOfClosingPrices):\n",
    "    numberOfClosingPrices = arrOfClosingPrices.shape[1]\n",
    "    numberOfStocks = arrOfClosingPrices.shape[0] \n",
    "    expectedReturnPerStock = np.mean(arrOfClosingPrices, axis = 1)\n",
    "    covarianceMatrix = np.zeros((expectedReturnPerStock.shape[0], expectedReturnPerStock.shape[0]))\n",
    "    for i in range(numberOfStocks):\n",
    "        for j in range(numberOfStocks):\n",
    "            covarianceMatrix[i][j] =  0\n",
    "            sum = 0.0\n",
    "            for k in range(numberOfClosingPrices):\n",
    "                firstStock = arrOfClosingPrices[i][k] - expectedReturnPerStock[i]\n",
    "                secondStock = arrOfClosingPrices[j][k] - expectedReturnPerStock[j] \n",
    "                sum += firstStock * secondStock\n",
    "            covarianceMatrix[i][j] = float(sum)/float(numberOfClosingPrices - 1)\n",
    "    return covarianceMatrix\n",
    "\n",
    "samsungClosingPrices = getClosingPrice(SamsungFile)\n",
    "facebookClosingPrices = getClosingPrice(FacebookFile)\n",
    "amdClosingPrices = getClosingPrice(AMDFile)\n",
    "salesforceClosingPrices = getClosingPrice(SalesforceFile)\n",
    "\n",
    "# Truncate to the minimum size\n",
    "minSize = samsungClosingPrices.size\n",
    "minSize = min(minSize, samsungClosingPrices.size)\n",
    "minSize = min(minSize, facebookClosingPrices.size)\n",
    "minSize = min(minSize, amdClosingPrices.size)\n",
    "minSize = min(minSize, salesforceClosingPrices.size)\n",
    "samsungClosingPrices = samsungClosingPrices[:minSize]\n",
    "facebookClosingPrices = facebookClosingPrices[:minSize]\n",
    "amdClosingPrices = amdClosingPrices[:minSize]\n",
    "salesforceClosingPrices = salesforceClosingPrices[:minSize]\n",
    "pprint(samsungClosingPrices)\n",
    "pprint(facebookClosingPrices)\n",
    "pprint(amdClosingPrices)\n",
    "pprint(salesforceClosingPrices)\n",
    "\n",
    "# Get daily returns\n",
    "\n",
    "samsungDailyReturns = getDailyReturns(samsungClosingPrices)\n",
    "facebookDailyReturns = getDailyReturns(facebookClosingPrices)\n",
    "amdDailyReturns = getDailyReturns(amdClosingPrices)\n",
    "salesforceDailyReturns = getDailyReturns(salesforceClosingPrices)\n",
    "pprint(samsungDailyReturns)\n",
    "pprint(facebookDailyReturns)\n",
    "pprint(amdDailyReturns)\n",
    "pprint(salesforceDailyReturns)\n",
    "\n",
    "allReturns = []\n",
    "allReturns.append(samsungDailyReturns)\n",
    "allReturns.append(facebookDailyReturns)\n",
    "allReturns.append(amdDailyReturns)\n",
    "allReturns.append(salesforceDailyReturns)\n",
    "allReturns = np.array(allReturns)\n",
    "pprint(allReturns)\n",
    "\n",
    "expectedReturn = calculateExpectedReturn(allReturns)\n",
    "pprint(expectedReturn, False)\n",
    "covarianceMatrix = calculateCovarianceMatrix(allReturns)\n",
    "# Calculate each mean\n",
    "pprint(covarianceMatrix, False)\n",
    "\n",
    "initialX = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "initialPi = 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton Method\n",
    "x = initialX.copy()\n",
    "pi = initialPi \n",
    "currX = x.copy()\n",
    "currPi = pi\n",
    "\n",
    "for iteration in range(maxIteration):\n",
    "    x = currX.copy()\n",
    "    pi = currPi\n",
    "    \n",
    "    f = computeF(x, expectedReturn, covarianceMatrix, delta)\n",
    "    constraint = np.sum(x)\n",
    "    pprint(iteration, False)\n",
    "    pprint(f, False)\n",
    "    pprint(constraint, False)\n",
    "    pprint(x, False)\n",
    "    pprint(pi, False)\n",
    "    \n",
    "    grad = computeGrad(x, pi, expectedReturn, covarianceMatrix, delta)\n",
    "    hessian = computeHessian(x, pi, expectedReturn, covarianceMatrix, delta)\n",
    "    \n",
    "    direction = np.dot(np.linalg.inv(hessian), grad)\n",
    "    pprint(direction, False)\n",
    "    \n",
    "    # Stopping condition\n",
    "    if np.linalg.norm(direction) < np.power(0.1, 10):\n",
    "        break\n",
    "    xAdd = np.reshape(direction[:-1].copy(), currX.shape)\n",
    "    \n",
    "    # You always deduct in Newton Method!\n",
    "    # As newton method accounts for correct direction regardless of max or min.\n",
    "    # Newton's method always searches for critical points. \n",
    "    currX -= stepSize * xAdd\n",
    "    currPi -= stepSize * direction[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steepest Descent Method\n",
    "# Note: NOT really steepest as the question asks to use stepsize of 1.0, so don't calculate stepsize. \n",
    "# Just do the normal gradient descent in the Machine Learning literature\n",
    "x = initialX.copy()\n",
    "pi = initialPi \n",
    "currX = x.copy()\n",
    "currPi = pi\n",
    "for iteration in range(maxIteration):\n",
    "    \n",
    "    x = currX.copy()\n",
    "    pi = currPi\n",
    "    \n",
    "    f = computeF(x, expectedReturn, covarianceMatrix, delta)\n",
    "    constraint = np.sum(x)\n",
    "    pprint(iteration, False)\n",
    "    pprint(f, False)\n",
    "    pprint(constraint, False)\n",
    "    pprint(x, False)\n",
    "    pprint(pi, False)\n",
    "    \n",
    "    grad = computeGrad(x, pi, expectedReturn, covarianceMatrix, delta)\n",
    "    # No hessian computation needed for gradient descent\n",
    "    \n",
    "    direction = grad\n",
    "    pprint(direction, False)\n",
    "    \n",
    "    # Stopping condition\n",
    "    if np.linalg.norm(direction) < np.power(0.1, 10):\n",
    "        break\n",
    "    xAdd = np.reshape(direction[:-1].copy(), currX.shape)\n",
    "    \n",
    "    # Gradient Descent diverges with constant step size\n",
    "    # stepSize = 0.00001 , can only converge with small step size but not OPTIMAL\n",
    "    # Need to add cause maximizing the function, NOT minimizing\n",
    "    currX += stepSize * xAdd\n",
    "    currPi += stepSize * direction[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
