\documentclass[a4paper,12pt]{article} 
%Packages included by Soon Chee Loong
\usepackage{amssymb}  % For \therefore
\usepackage{fullpage} % Package to use full page
\usepackage{parskip} % Package to tweak paragraph skipping
\usepackage{tikz} % Package for drawing
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{palatino}
\usepackage{minted} % For code highlighting 
\usepackage{amsmath} % To split long equations
% More custom packages (default or by Christopher) 
\usepackage{amssymb}
\usepackage{bm}
\usepackage{multirow} % Used to merge multiple tables cells along a row
\usepackage{mathtools} % Double vertical bars for norms
\usepackage{physics} % Partial derivatives
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{relsize}
\usepackage{wrapfig}
\usepackage{cite}
\usepackage{neuralnetwork}
%------------------------------------------------------------------
% Adjust line spacing
\renewcommand{\baselinestretch}{1.5}
%------------------------------------------------------------------
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
%------------------------------------------------------------------
% Colour Definitions
\definecolor{blue}{HTML}{1f77b4}
\definecolor{orange}{HTML}{ff7f0e}
\definecolor{green}{HTML}{2ca02c}
\definecolor{bg}{rgb}{0.95, 0.95, 0.95}
%------------------------------------------------------------------
% Collaboration Notes 
% 0. Ensure tex always compiles, fix any errors immediately
%		This doesn't have version control. So don't mess up.  
% 1. Work on 1 main.tex file only 
% 	Easier to back-up to local
% 	Easier to work on local if needed. 
% 2. TODO: Mark anything important to do as TODO
% 	so we can easily search for all TODO: comments before submitting. 
%------------------------------------------------------------------
\title{ECE521 Winter 2017: Assignment 2}
\author{FuYuan Tee, (999295837)
  \thanks{Equal Contribution (50\%), fuyuan.tee@mail.utoronto.ca}
\and Chee Loong Soon, (999295793) \thanks{Equal Contribution (50\%),  cheeloong.soon@mail.utoronto.ca}} 
\date{February 27th, 2017}
\begin{document}
\maketitle
\tableofcontents
\clearpage
%------------------------------------------------------------------
\section{Logistic Regression}
\subsection{Binary cross-entropy loss}
\subsubsection{Learning}

Using a two-class notMNIST dataset, a binary C-J character classifier was trained using logistic regression and cross-entropy loss. A TensorFlow Gradient descent optimizer was used to train the model. From using various learning rates, the best learning rate was selected to be $\eta = 0.01$ (refer to Table \ref{table:TuneLearningRateSGD} below).

\begin{table}[!htb]
\centering
\caption{Performance of binary classifier with gradient descent optimizer with respect to $\eta$}
\label{table:TuneLearningRateSGD}
\vspace{0.5em}
\begin{tabular}{|r|r r r|} \hline
$\eta$ & \textit{Number of Iterations} & \textit{Validation Loss} & \textit{Best Test Accuracy, \%} \\ \hline
0.001 & 1407 & 0.236 & 97.2 \\
0.01 & 1407 & 0.138 & 100 \\
0.1 & 1407 & 0.142 & 100 \\
1 & 1407 & 0.793 & 98.6 \\
\hline
\end{tabular}
\end{table}

% \begin{table}[!htb]
% \centering
% \caption{Performance of binary classifier with gradient descent optimizer with respect to $\eta$}
% \label{table:TuneLearningRateSGD}
% \vspace{0.5em}
% \begin{tabular}{|r|r r r|} \hline
% $\eta$ & \textit{Number of Iterations} & \textit{Validation Loss} & \textit{Best Test Accuracy, \%} \\ \hline
% 0.001 & (1254, 1407) & (1.176, 0.235528) & (86.2, 97.2)\\
% 0.01 & (673, 1407) & (0.805, 0.138068) & (93.1, 100) \\
% 0.1 & (260, 1407) & (0.523, 0.141785) & (98.6, 100) \\
% 1 & (78, 1407) & (0.261,  0.793168) & (99.3, 98.6) \\
% 10 & (22, None) & (3.298, None) & (98.6, None) \\
% \hline
% \end{tabular}
% \end{table}

The plots of cross-entropy loss and classification accuracy for the best learning rate can be found below:

\begin{figure}[!htb]
\centering
\includegraphics[width=0.8\textwidth]{Graphics/A2Q1_1_1_LogisticSGD}
\includegraphics[width=0.8\textwidth]{Graphics/A2Q1_1_1_LogisticSGD_1}
\caption{\label{figure:LogisticSGD} \textcolor{blue}{Training} and \textcolor{green}{test} sets cross-entropy loss and classification accuracy for logistic classifier using stochastic gradient descent (SGD) and best learning rate, $\eta = 0.01$.}
\end{figure}

\clearpage
%------------------------------------------------------------------
\subsubsection{Beyond plain SGD}

Similarly, a logistic binary classifier was trained on the same dataset using an Adam optimizer and early-stopping in TensorFlow. The results are summarised in Table \ref{table:TuneLearningRateAdam} below.

\begin{table}[!htb]
\centering
\caption{Performance of binary classifier with Adam optimizer with respect to $\eta$}
\label{table:TuneLearningRateAdam}
\vspace{0.5em}
\begin{tabular}{|r|r r r|} \hline
$\eta$ & \textit{Number of Iterations} & \textit{Validation Loss} & \textit{Best Test Accuracy, \%} \\ \hline
0.0001 & 1716 & 1.173 & 88.3 \\
0.001 & 1471 & 0.327 & 96.6 \\
0.01 & 253 & 0.298 & 97.9 \\
0.1 & 15 & 1.397 & 97.9 \\
1 & 43 & 2.115 & 97.9 \\
\hline
\end{tabular}
\end{table}

% \begin{table}[!htb]
% \centering
% \caption{Performance of binary classifier with Adam optimizer with respect to $\eta$}
% \label{table:TuneLearningRateAdam}
% \vspace{0.5em}
% \begin{tabular}{|r|r r r|} \hline
% $\eta$ & \textit{Number of Iterations} & \textit{Validation Loss} & \textit{Best Test Accuracy, \%} \\ \hline
% 0.0001 & (1716, 1400) & (1.173, 0.277022) & (88.3,  64.1379)\\
% 0.001 & (1471, 1400) & (0.327, 0.277022) & (96.6, 93.1035) \\
% 0.01 & (253, 1400) & (0.298,  0.118955) & (97.9, 97.2414) \\
% 0.1 & (15, 1400) & (1.397, 0.0270384) & (97.9, 97.931) \\
% 1 & (43, 1400) & (2.115,  0.0547311) & (97.9,   98.6207) \\
% \hline
% \end{tabular}
% \end{table}

The best learning rate was chosen to be $\eta = 0.01$ as it has the highest test accuracy and lowest validation loss. Similar plots of cross-entropy loss and classification accuracy for the best learning rate can be found in Figure \ref{figure:LogisticAdam}:

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{Graphics/A2Q1_1_2_LogisticAdam}
\caption{\label{figure:LogisticAdam} \textcolor{blue}{Training} and \textcolor{orange}{test} sets cross-entropy loss and classification accuracy for logistic classifier using Adam optimizer and best learning rate, $\eta = 0.01$.}
\end{figure}

By comparing the performance of the binary classifier using both types of optimizers, it is noted that the test classification accuracies obtained are very similar. The only difference is that the Adam optimizer is able to allow the model to converge to the optimum weights at a much higher rate. It also incorporates momentum to prevent the weights to get stuck in relatively poor local minimums. 

From the observation above, the Adam optimizer will prove to be useful in allowing for quicker convergence to more ideal solutions when learning from the \textit{notMNIST} data.

\clearpage
%------------------------------------------------------------------
\subsubsection{Comparison with linear regression}

TODO:Explanation: What d oyou observe on effect of cross-entropy loss on classification performance?
TODO: SCL adds picture with accuracy from Normal Equation
TODO: Compare the train, validation
and test classification of the least squares solution to the optimal logistic regression learnt without weight decay.

Linear Regression that is calculated using normal equation performs better on the training set since it is indeed the optimal weights for the training set. However, it overfits and performs worse on both the validation set and test set compared to the trained Logistic Regression model. 

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{Graphics/A2Q1_1_3_LossComparison}
\caption{\label{figure:ComparingDifferentLosses} Comparison between squared-error and cross-entropy loss for $\hat{y}$ against a dummy target $y = 0$.}
\end{figure}

As seen from Figure \ref{figure:ComparingDifferentLosses} above, the cross-entropy loss heavily penalises misclassifications (i.e. cross-entropy loss approaches $+ \infty$ as $\hat{y}$ approaches 1) as compared to a squared-error loss. This arises from the $\log$ term in the cross-entropy function, where $\lim_{\hat{y} \rightarrow 1} \log(1 - \hat{y}) = + \infty$. The fact that the cross-entropy loss penalises misclassifications signficantly more is be vital in minimising errors in classification applications.

\clearpage
%------------------------------------------------------------------
\subsubsection{Maximum likelihood estimation}
Binary Classification means there are 2 outputs, 0 or w. In other words, $y \in \{0,1\}$. 
Logistic function  makes prediction of the outputs using equation \ref{equation:SigmoidOutput}. This equation makes the predicted value lie between $[0,1]$. 
This equation assumes the bias is incorporated within the weights $W$ where an additional dimension in the input $x$ is filled with ones. 

\begin{equation}
\label{equation:SigmoidOutput}
\hat{y}(\bf{x}) = \sigma(W^{T}\mathbf{x}) = \frac{1}{1+e^{-(W^{T}\mathbf{x})}}
\end{equation}


The combination of Cross Entropy Loss with Sigmoid Function is known as Logistic Regression shown in \ref{equation:LogisticRegression}. 

\begin{equation}
\label{equation:LogisticRegression}
\sum_{m=1}^{M}\frac{1}{M}[-y^{(m)}log(\hat{y}(\mathbf{x}^{m})) - (1-y^{m})log(1-\hat{y}(\bf{x}^{m}))]
\end{equation}

We can derive the cross entropy loss from Maximum Likelihood Estimation Principle by assuming a Bernoulli distribution for the likelihood of the training labels as shown in equation \ref{equation:BernoulliDistribution}. 

\begin{equation}
\label{equation:BernoulliDistribution}
\Pr(y=1|x, W) = 1 - \Pr(y=0|x, W) = \hat{y}(x)
\end{equation}

The Bernoulli Distribution for the likelihood can also be re-written more concisely as equation \ref{equation:BernoulliDistributionParameterized}. 

\begin{equation}
\label{equation:BernoulliDistributionParameterized}
\Pr(y|x, W) = \hat{y}(x)^{y}(1-\hat{y}(x))^{(1-y)}
\end{equation}

Since maximizing the log-likelihood is same as maximizing the likelihood in equation \ref{equation:BernoulliDistributionParameterized}, we take the log instead as it makes the computation easier as shown in equation \ref{equation:LogLikelihoodBernoulli}. 

\begin{equation}
\label{equation:LogLikelihoodBernoulli}
\Pr(y|x, W) = ylog(\hat{y}(x)) + (1-y)log(1-\hat{y}(x))
\end{equation}

Instead of maximizing the log-likelihood, we can instead minimize the negative log-likelihood by multiplying equation \ref{equation:LogLikelihoodBernoulli} with $-1$ as shown in equation
\ref{equation:NegativeLogLikelihoodBernoulli}. 

\begin{equation}
\label{equation:NegativeLogLikelihoodBernoulli}
\Pr(y|x, W) = -ylog(\hat{y}(x)) - (1-y)log(1-\hat{y}(x))
\end{equation}

Doing this for every $M$ training case, where $m$ reprents a single training case, we get back to our original Logistic Regression equation \ref{equation:LogisticRegression}. 

Therefore, it is proven that maximizing the likelihood of training labels assuming a Bernoulli Distribution derives the Logistic Regression equation. 

\clearpage
%------------------------------------------------------------------
%------------------------------------------------------------------
\subsection{Multi-class classification}
%------------------------------------------------------------------
\subsubsection{Minimum-loss system for equal misclassification penalty}
If a classifier assigns each input, $x$ to a class with largest aposterior probability,
in other words, assign x to class $k$ if $\Pr{C_{k}|x} \ge \Pr{C_{j}|x}$ $\forall j \neq k$. 
We can show that this represents a minimum-loss system if the penalty for misclassification is equal for each class. 

The proof for minimum-loss for equal misclassification penalty is just a more specific case for a more general case for the proof for unequal misclassification penalty in section \ref{section:MinimumLossGeneral}. 

Equal misclassification penalty simply means a Loss Matrix where the diagonals are 0 or the trace of the loss matrix is 0. Also it means every non-diagonal elements would be an equal positive constant. For simplicity, we can assume that positive constant is $1.0$. 

This results in the Loss Matrix shown in matrix \ref{equation:LossMatrixEqual}. The loss matrix is square matrix of size $(K,K)$ where $K$ represents the number of classes. 
The $(j,k)$ entries of the loss matrix, $L_{jk}$ represents the penalty for misclassification to a class $j$ when the pattern in fact belongs to class $k$. 

\begin{equation}
\label{equation:LossMatrixEqual}
\mathbf{L} =
  \begin{bmatrix}
    0 & 1 & ... & ... & 1 \\
    1 & 0 & 1 & ... & 1 \\
    ... & ... & ... & ... & ... \\
    1 & ... & ... & 1 & 0
  \end{bmatrix}
\end{equation}

If you refer to the proof for section \ref{section:MinimumLossGeneral}, you will understand that picking the  class with largest aposterior probability is the same as getting rid of the term with the largest posterior probability from the loss equation in \ref{equation:MisclassificationPenalty}.

This therefore represents the minimum loss for a specific region that the input belongs to. Repeat this for every input and you get a minimal-loss system for misclassification. This proofs this specific case where the penalty for misclassification is equal. However, I will present a more specific proof below for that only works if the misclassification penalty is equal, resulting in the loss matrix shown in \ref{equation:LossMatrixEqual}. 

I will now explain why equation \ref{equation:minimumSum} represents picking the class with largest posterior probability for this specific case of equal misclassification penalty. 

The a posterior probability of a class $j$ is defined using Bayes Rule as shown in equation \ref{equation:BayesRule}.
\begin{equation}
\label{equation:BayesRule}
\Pr{C_{j} | x} = \frac{\Pr{x | C_{j}} \Pr{C_{j}}}{\sum_{k=1}^{K}  \Pr{x | C_{k}} \Pr{C_{k}}} = 
\frac{\Pr{x \in C_{j}}}{\sum_{k=1}^{K}  \Pr{x | C_{k}} \Pr{C_{k}}}
\end{equation}

The denominator for the Bayes Rule in equation \ref{equation:BayesRule}, 
$\sum_{k=1}^{K}  \Pr{x | C_{k}} \Pr{C_{k}}$ is simply a normalization constant. 

Therefore, picking the class with largest posterior probability $\Pr{C_{j} | x}$ is equivalent to picking the class with largest joint probability $\Pr{x \in C_{j}}$. 

The error for misclassification of picking a class $j$ is shown in equation \ref{equation:Misclassification}. 
The first term is all the input in that region that does not belong to the assigned class $j$ and the second term is all the input that does belong to class $j$ but was not assigned to this class outside this region $R_{j}$. 

\begin{equation}
\label{equation:Misclassification}
\Pr{mistake} = (\sum_{k=1, k \neq j}^{K} \int_{R_{j}} \Pr{x \in (R_{j}, C_{k})})  + \int_{R - R_{j}} \Pr{x \in (R -R_{j}, C_{j})}
\end{equation}

By definition of $j$ being the largest posterior probability class, which is the largest joint probability class as shown in equation \ref{equation:largestPosterior}. 

\begin{equation}
\begin{split}
\label{equation:largestPosterior}
\Pr{x \in (R_{x}, C_{j})} \ge  \Pr{x \in (R_{x}, C_{i})} \\
\forall i \neq j
\end{split}
\end{equation}

Thus, for an input that belongs to its region, $R_{x}$ to be assigned to a class, picking the class with largest posterior probability is equivalent to picking the class with largest joint probability, which gets rid of that integral term from the sum of integrals in equation \ref{equation:Misclassification} in its region, therefore minimizes the loss and represents a minimum loss system. This further explained in the proof for section \ref{section:MinimumLossGeneral} below. 

where equation \ref{equation:conditionZeroSpecific} is a specific case of equation \ref{equation:conditionZero}. 

\begin{equation}
\label{equation:conditionZeroSpecific}
\argmax_j \Pr{C_{k} | \mathbf{x} \in R_{j}} 
\end{equation}

\clearpage
%------------------------------------------------------------------
\subsubsection{Minimum-loss system for unequal misclassification penalty}
\label{section:MinimumLossGeneral}

Here we use a more general Loss Matrix shown in \ref{equation:LossMatrix}, where the entries can be any real number.  

The loss matrix is square matrix of size $(K,K)$ where $K$ represents the number of classes. 
The $(j,k)$ entries of the loss matrix, $L_{jk}$ represents the penalty for misclassification to a class $j$ when the pattern in fact belongs to class $k$. 

\begin{equation}
\label{equation:LossMatrix}
\mathbf{L} =
  \begin{bmatrix}
    L_{11} & L_{12} & ... & L_{1K} \\
    L_{21} & L_{22} & ... & L_{2K} \\
    ... & ... & ... & ... \\
    L_{K1} & ... & ... & L_{KK}
  \end{bmatrix}
\end{equation}

We can define the entire domain of the entire input training data to be the entire region $R$. 
Given a specific region, $R_{specific}$, we need to classify any input in this specific region $R_{specific}$ to a certain class. Lets just assume we assign every input in $R_{specific}$  to a class $\alpha$. Then, we can re-define this specific region to be the assigned region $R_{\alpha}$. 

To be clear, we define $R_{\alpha}$ as the region where all inputs will be classified as class $\alpha$, where $\alpha$ is a class in the $K$ different classes.
Obviously, we need to assign every domain of the region $R$ to a class in the $K$ classes. Otherwise, the loss penalty in \ref{equation:MisclassificationPenalty} will be maximized and this is bad. However, we do not necessary need to assign to every single class that is available, although we must be using at least one of the K classes as explained. 

We define $C_{k}$ to be the class that a given input $x$ actually belongs to. In this case, the input $x$ belongs to the target class $k$ and this cannot be changed by the algorithm as it is given as an input. 

To be clear, we are only able to assign the regions $R_{specific}$ to a class $j$. In other words $R_{specific} = R_{j}$. Only the inputs in this region $R_{specific}$ will be classified as a result of this assignment. This input can be written as $x \in (R_{specific}, C_{k})$. However, the input in this region $R_{specific}$ already belongs to its actual class $C_{k}$.

Therefore, the penalty for the classification of a region $R_{specific} = R_{j}$ will be $L_{jk}$. This results in $x$ being assigned to class $j$ but actually belongs to class $k$. $j=k$ implies correct classification assignments whereas $j \neq k$ implies misclassification. This is shown as $x \in (R_{j}, C_{k})$. 

The penalty for misclassification for entire region $R$ for all $K$ classes is defined in equation \ref{equation:MisclassificationPenalty}. 

\begin{equation}
\label{equation:MisclassificationPenalty}
\mathbf{E}[Error] = \sum_{k=1}^{K} \sum_{j=1}^{K} L_{jk} \Pr{x \in (R_{j}, C_{k})}
\end{equation}

The total loss equation \ref{equation:MisclassificationPenalty} sums over region for every assigned class $R_{j}$. We can simplify this to represent the loss equation for a single region assignment, $R_{specific}$ as shown in \ref{equation:MisclassificationPenaltySpecific}.

\begin{equation}
\label{equation:MisclassificationPenaltySpecific}
\mathbf{E}[Error | R_{specific}]  = \sum_{k=1}^{K} L_{specific,k} \Pr{x \in (R_{specific}, C_{k})}
\end{equation}

For a specific region, $R_{specific}$, the inputs in this region can belong to any of the $K$ classes. We represent these inputs as a vector shown in \ref{equation:InputVector}. 

\begin{equation}
\label{equation:InputVector}
\mathbf{X}_{R_{specific}} =
  \begin{bmatrix}
    x \in (R_{specific}, C_{1}) \\
    x \in (R_{specific}, C_{2})  \\
    ... \\
    x \in (R_{specific}, C_{K})  
  \end{bmatrix}
\end{equation}

Therefore, selecting a region for $R_{specific}$ is same as selecting a row $j$ from the Loss Matrix in \ref{equation:LossMatrix} to be dot product with the input vector \ref{equation:InputVector}.

The selected row $j$ from the Loss Matrix is shown in \ref{equation:LossRow}. 

\begin{equation}
\label{equation:LossRow}
\mathbf{L}_{j} =
  \begin{bmatrix}
    L_{j1} & L_{j2} & ... & L_{jK}
  \end{bmatrix}
\end{equation}

Thus, equation \ref{equation:MisclassificationPenaltySpecific} can be represented as this dot product as shown in \ref{equation:dotProduct}
\begin{equation}
\label{equation:dotProduct}
\Pr{Error | R_{j}} = \mathbf{L}_{j} \cdot \mathbf{X}_{R_{specific}} = \mathbf{L}_{j} \cdot \mathbf{X}_{R_{j}}
\end{equation}

Therefore, the condition to minimize the penalty for misclassification is to select the row $j$ such that we minimize the dot product in \ref{equation:dotProduct}. 
This is shown in equation \ref{equation:minimumRow}

\begin{equation}
\label{equation:minimumRow}
 \argmin_j \Pr{Error | R_{j}} = \argmin_j \mathbf{L}_{j} \cdot \mathbf{X}_{R_{specific}} =  \argmin_j \mathbf{L}_{j} \cdot \mathbf{X}_{R_{j}}
\end{equation}

This equation can be re-written as equation \ref{equation:minimumSum}. I replaced the joint probabilities with posterior probabilities since it does not change the $j$ that is picked as explained on the paragraph below equation \ref{equation:BayesRule}. 

\begin{equation}
\label{equation:minimumSum}
 \argmin_j \Pr{Error | R_{j}} =  \argmin_j \sum_{k=1}^{K} \mathbf{L}_{jk} \Pr{C_{k} | \mathbf{x} \in R_{specific}} =  \argmin_j \sum_{k=1}^{K} \mathbf{L}_{jk} \Pr{C_{k} | \mathbf{x} \in R_{j}}
\end{equation}

Therefore, the condition for minimum classification for an input in the region $R_{specific}$ to a class $C_{j}$ to minimize total loss is to pick the class $j$ that results in the lowest $\sum_{k=1}^{K} \mathbf{L}_{jk} \Pr{C_{k} | \mathbf{x} \in R_{specific}}$ for the region that the input belongs to. 

For the special case where the diagonals of \ref{equation:LossMatrix} is 0. i.e. $L_{ii} = 0$ $\forall i$  is 0 and the other values positive (this means the loss matrix must penalize and there are no rewards (i.e. no negative values) for any classification). This means the dot product in \ref{equation:dotProduct} gets rid of one of the terms in the input vector \ref{equation:InputVector} and it simply gets rid of that term in the sum in equation \ref{equation:MisclassificationPenaltySpecific}. We therefore pick the class $j$ with the largest value for equation \ref{equation:conditionZero}. 

\begin{equation}
\label{equation:conditionZero}
\argmax_j \mathbf{L}_{jk} \Pr{C_{k} | \mathbf{x} \in R_{j}} 
\end{equation}

to get rid of that largest term, $\mathbf{L}_{jk} \Pr{C_{k} | \mathbf{x} \in R_{j}}$ from 
the loss equation in \ref{equation:MisclassificationPenaltySpecific}. This will minimize the loss. 

\clearpage
%------------------------------------------------------------------
\subsubsection{Multi-class classification on \textit{notMNIST} dataset}

For the full \textit{notMNIST} dataset, a multi-class softmax classifier with Adam optimizer and early-stopping is implemented. The training results over various learning rates are summarised in Table \ref{table:SoftmaxResult} below.

\begin{table}[!htb]
\centering
\caption{Performance of multi-class classifier with respect to $\eta$}
\label{table:SoftmaxResult}
\vspace{0.5em}
\begin{tabular}{|r|r r r|} \hline
$\eta$ & \textit{Number of Iterations} & \textit{Validation Loss} & \textit{Best Test Accuracy, \%} \\ \hline
0.0001 & 8371 & 1.060 & 84.1 \\
0.001 & 1921 & 0.867 & 85.3 \\
0.01 & 451 & 0.807 & 85.6 \\
0.1 & 121 & 2.091 & 87.3 \\
1 & 121 & 18.852 & 87.3 \\
\hline
\end{tabular}
\end{table}

The best learning rate was chosen to be $\eta = 0.01$ as it has the lowest validation error. The performance of that model can be seen from Figure \ref{figure:Softmax} below.

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{Graphics/A2Q1_2_3_Softmax}
\caption{\label{figure:Softmax} \textcolor{blue}{Training} and \textcolor{orange}{test} sets cross-entropy loss and classification accuracy for the softmax classifier with best learning rate, $\eta = 0.01$.}
\end{figure}

TODO: Explain intuitively the drop in test classification performance between binary and multi-class classification


\clearpage
%------------------------------------------------------------------
%------------------------------------------------------------------
%------------------------------------------------------------------
\section{Neural Networks}
\subsection{Geometry of neural networks}
\subsubsection{Logistic regression weights on linearly separable dataset}
\label{section:linearlySeparableProblem}

From the cross-entropy loss function:
\begin{equation}
\label{equation:crossEntropyLoss}
\mathcal{L} = - \frac{1}{M} \sum_m^M \left[ t^{(m)} \log y^{(m)} +
	\left(1 - t^{(m)} \right) \log \left( 1 - y^{(m)} \right) \right]
\end{equation}

Let the bias, $b$, be lumped in as the zeroth weight, $w_0$ in the weight vector, $\mathbf{W}$.

Taking the derivative with respect to the weights, $\mathbf{W}$:
\begin{align}
\begin{split}
\pdv{\mathcal{L}}{\mathbf{W}} =& \pdv{\mathcal{L}}{y} \cdot \pdv{y}{\mathbf{z}} \cdot 
	\pdv{\mathbf{z}}{\mathbf{W}} \\
=& - \frac{1}{M} \sum_m^M \left[ \frac{t^{(m)}}{y^{(m)}} -
	\frac{1 - t^{(m)}}{1 - y^{(m)}} \right] \cdot 
    \left[ y^{(m)} \left(1 - y^{(m)} \right) \right] \cdot
    \mathbf{x}^{(m)'} \\
=& - \frac{1}{M} \sum_m^M \left[ t^{(m)} \left( 1 - y^{(m)} \right) -
	\left( 1 - t^{(m)} \right) y^{(m)} \right] \cdot 
    \mathbf{x}^{(m)'} \\
=& \frac{1}{M} \sum_m^M \left[ y^{(m)} - t^{(m)} \right] \cdot 
    \mathbf{x}^{(m)'} \\
=& \frac{1}{M} \sum_m^M \left[ \frac{1}{1 + e^{- (\mathbf{W'x}^{(m)}) }} 
	- t^{(m)} \right] \cdot \mathbf{x}^{(m)'} \\
\end{split}
\end{align}

Let the sets $\mathbb{X}_0 = \{\mathbf{x}_0^{(1)}, \cdots, \mathbf{x}_0^{(M_0)} \}$ and $\mathbb{X}_1 = \{\mathbf{x}_1^{(1)}, \cdots, \mathbf{x}_1^{(M_1)} \}$ be the sets of inputs with target values of $t_0 = 0$ and $t_1 = 1$ respectively, where $M = M_0 + M_1$. The partial derivative above can  then be broken down into terms involving $\mathbb{X}_0$ and $\mathbb{X}_1$, as follows:

\begin{align}
\begin{split}
\pdv{\mathcal{L}}{\mathbf{W}} =& \frac{1}{M}  \left\{ \sum_{m_0}^{M_0} \left[ \frac{1}{1 + e^{- 
	(\mathbf{W'x}_0^{(m)}) }} - t_0^{(m)} \right] \mathbf{x}_0^{(m)'} +
    \sum_{m_1}^{M_1} \left[ \frac{1}{1 + e^{- (\mathbf{W'x}_1^{(m)}) }} - t_1^{(m)} \right] 
    \mathbf{x}_1^{(m)'} \right\} \\
=& \frac{1}{M}  \left\{ \sum_{m_0}^{M_0} \left[ \frac{1}{1 + e^{- 
	(\mathbf{W'x}_0^{(m)}) }} \right] \mathbf{x}_0^{(m)'} +
    \sum_{m_1}^{M_1} \left[ \frac{1}{1 + e^{- (\mathbf{W'x}_1^{(m)}) }} - 1 \right] 
    \mathbf{x}_1^{(m)'} \right\}
\end{split} \\
\label{equation:SimplifiedGrad}
=& \frac{1}{M}  \left\{ \sum_{m_0}^{M_0} \left[ \frac{1}{1 + e^{- 
	(\mathbf{W'x}_0^{(m)}) }} \right] \mathbf{x}_0^{(m)'} +
    \sum_{m_1}^{M_1} \left[ \frac{1}{1 + e^{\mathbf{W'x}_1^{(m)}}} \right] 
    \mathbf{x}_1^{(m)'} \right\}
\end{align}


A dataset is linearly separable when there exists $w_1, \cdots, w_n, k \in \mathbb{R}$ such that $ \sum_i^n w_i x_i < k \quad \forall x_i \in \mathbb{X}_0 $ and $ \sum_i^n w_i x_i > k \quad \forall x_i \in \mathbb{X}_1 $, using the sets definition for $\mathbb{X}_0$ and $\mathbb{X}_1$ above. The dataset is linearly separable. By the property of linearly separable, it means that it is possible to select values for the optimal weight vector $\mathbf{W}^{*}$, such that:
\begin{align}
\label{equation:DotProductLimits}
\begin{cases}
\mathbf{W^{*'}x}_0^{(m)} \ll 0\\
\mathbf{W^{*'}x}_1^{(m)} \gg 0
\end{cases} \ \text{or} \quad
\begin{cases}
\mathbf{W^{*'}x}_0^{(m)} = - \infty \\
\mathbf{W^{*'}x}_1^{(m)} = + \infty
\end{cases}
\forall m = 1, \cdots, M
\end{align}

This in turn forces the coefficients of $\mathbf{x}_0^{(m)'}$ and $\mathbf{x}_1^{(m)'}$ in Equation \ref{equation:SimplifiedGrad} to be 0, leading to gradients for each individual training example to go to 0, leading to an optimal solution. Since the dataset are indicated to be linearly separable and that the weights are optimal, it means that the gradients must be 0 for any training data, which means the coefficients must go to 0, which means that the weights have magnitudes at infinity. 

Since the inputs $\mathbf{x}^{(m)}$ is finite, for Equation \ref{equation:DotProductLimits} to occur:
\begin{align}
|w_i| = \infty \quad \forall i = 1, \cdots, ..., n
\end{align}

and by extension, $\|W\|_2 = \infty$.

%------------------------------------------------------------------
\subsubsection{Logistic regression weights on linearly inseparable dataset}
\label{section:BoundedSection}

Note that the mathematical notation below is different from section \ref{section:linearlySeparableProblem}. 
Here, 
$y$ refers the the target output
$\hat{y}$ refers to the predicted output from the sigmoid. 

Rewriting cross entropy loss in \ref{equation:crossEntropyLoss} using this new notation, we get equation \ref{equation:crossEntropyLossNew}

\begin{equation}
\label{equation:crossEntropyLossNew}
L = - \frac{1}{M} \sum_m^M [ y^{(m)} \log \hat{y}^{(m)} +
	(1 - y^{(m)} ) \log ( 1 - \hat{y}^{(m)} )]
\end{equation}

Rewriting the gradient of cross entropy loss with respect to weights results in equation \ref{equation:crossEntropyGradient} where $M$ is the number of training data. 

\begin{equation}
\label{equation:crossEntropyGradient}
\frac{\partial L}{\partial W} = \frac{1}{M} \sum_m^M [ \frac{1}{1 + e^{- (\mathbf{W'x}^{(m)})}} - y^{(m)} ] \cdot \mathbf{x}^{(m)} = \frac{1}{M} \sum_m^M [ \hat{y}^{m} - y^{(m)} ] \cdot \mathbf{x}^{(m)} 
\end{equation}

For single sigmoid neuron on a linearly inseparable binary classification problem, we need to show that the length of optimal weights vector learn on this dataset will always be bounded. 

Lets clearly define how each term above is interpreted mathematically. 

For a linearly inseparable case, Equation \ref{equation:DotProductLimits} cannot occur by definition. As a result, the coefficients of $\mathbf{x}_0^{(m)'}$ and $\mathbf{x}_1^{(m)'}$ will never be zero themselves.This fact extends to the individual gradient of each training example being non-zero. As a result, the only way for an optimal solution to be reached is for the individual gradients of various training example canceling each other out to obtain an overall gradient value of 0. Similarly, the optimal solution is reached without having $\mathbf{W}^{*'} \mathbf{x}^{(m)} = \pm \infty$. As a result, $\|W\|_2 < \infty$.

In other words, linearly inseparable means that it is impossible to get an optimal weight with  unbounded weights without the terms canceling each other out. Therefore, the only way to get an optimal weight (i.e. the gradient is 0) is for the terms to cancel each other out. 

Binary Classification problem means the output can only be 2 values. For the case of a single sigmoid where it can only output values between 0 to 1 (i.e. $\hat{y} \in [0, 1]$), it mathematically makes sense to define the 2 values for the binary classification to be $\{0, 1\}$. 

A single sigmoid neuron is defined as equation \ref{equation:SigmoidOutput}. Since it is only a single sigmoid neuron, this means that the sigmoid must be centered at 0 as there are no next layer math that shifts the sigmoid. The sigmoid is only scaled while being centered at 0 to predict outputs between 0 or 1. Since the weights and biases (which can be included in the weights) are inside the sigmoid equation \ref{equation:SigmoidOutput}, this means the sigmoid can only be centered at 0 as it is not shifted. 

However, the single sigmoid can be flipped around 0. In other words, it can either in the Case A where it output values $\hat{y} = 1 > 0.5$ for inputs $x>0$ and $\hat{y} = 0 < 0.5$ for inputs $x<0$. 
Or, it can be the Case B  where it output values $\hat{y} = 0 < 0.5$ for inputs $x>0$ and $\hat{y} = 1 >0.5$ for inputs $x < 0$. It must either predict values 0 or 1 since the weights are assumed to be unbounded. 

We will now prove that the weights must be unbounded using contradiction. 
Assume that the length of the optimal weights are unbounded. This means that the gradient of the Cross Entropy Loss  with respect to loss must be 0 shown in equation \ref{equation:zeroGrad}.

\begin{equation}
\label{equation:zeroGrad}
\frac{\partial L}{\partial W} = 0 
\end{equation}

However, since the dataset is linearly inseparable, the sigmoid must make errors and the only way for the gradient to be 0 is for the errors to cancel each other out. Instead of looking at all M training examples, we can simplify this to look at only 2 training example, where we simplify equation \ref{equation:crossEntropyGradient} to only be two training example as shown in equation 
\ref{equation:crossEntropyGradientSimplified} and ignore the normalization constant. 

\begin{equation}
\label{equation:crossEntropyGradientSimplified}
\frac{\partial L}{\partial W} = [ \hat{y}^{(0)} - y^{(0)} ] \cdot \mathbf{x}^{(0)} + [\hat{y}^{(1)} - y^{(1)} ] \cdot \mathbf{x}^{(1)} 
\end{equation}

Since the sigmoid can only be in Case A or Case B defined above, we will look at these two cases separately. Also, we only look at data points that are misclassified as correctly classified data points will only result in a zero gradient and does not help in canceling out gradients from misclassified points. 

Suppose the sigmoid is in Case A, then it for it to be linearly inseparable and optimal, it the two input to cancel must be on different sides of the domain, one in $x^(0) = c_{1} > 0$ and  $x^(1) = -c_{2} < 0$ and their respective outputs are  $y^(0) = 0$ and  $y^(1) = 1$, where $c_{1}, c_{2} > 0$. 
Substituting these into equation \ref{equation:crossEntropyGradientSimplified}, we get
equation \ref{equation:CaseA}, which shows that the gradient cannot cancel out and will not be 0 but a positive value instead regardless of how many training data we include. This means that the single sigmoid can never be in Case A as it will lead to a contradiction that the weights are optimal since the gradient is not 0. 

\begin{equation}
\label{equation:CaseA}
\frac{\partial L}{\partial W} = [1 - 0] \cdot (c_{1}) + [0 - 1] \cdot (-c_{2}) > 0 \neq 0
\end{equation}

Suppose the sigmoid is in Case B, then it for it to be linearly inseparable and optimal, it the two input to cancel must be on different sides of the domain, one in $x^(0) = c_{1} > 0$ and  $x^(1) = -c_{2} < 0$ and their respective outputs are  $y^(0) = 1$ and  $y^(1) = 0$, where $c_{1}, c_{2} > 0$, where $c_{1}$ and $c_{2}$ represents different positive constants that can be equal.  
Substituting these into equation \ref{equation:crossEntropyGradientSimplified}, we get
equation \ref{equation:CaseB}, which shows that the gradient cannot cancel out and will not be 0 but a negative value instead regardless of how many training data we include. This means that the single sigmoid can never be in Case B as it will lead to a contradicton that the weights are optimal since the gradient is not 0. 

\begin{equation}
\label{equation:CaseB}
\frac{\partial L}{\partial W} = [0 - 1] \cdot (c_{1}) + [1 - 0] \cdot (-c_{2}) < 0 \neq 0
\end{equation}

Since the single sigmoid can only be in either Case A or Case B for the unbounded weights to be optimal for a linearly inseparable binary classification problem but this leads to a contradiction that the weights cannot be optimal. This shows that theres no possible configuration for the single sigmoid to be in for an unbounded weight. By contradiction, this proves the the weights must be unbounded. 

\clearpage
%------------------------------------------------------------------
\subsubsection{Neural network weights on linearly inseparable dataset}

A Single Hidden Layer Neural Network can classify a linearly inseparable binary classification dataset
since it is a sum of non-linear activations by the universality theorem.

Unbounded case,
Before being fed into the single hidden layer neural network, each single sigmoid neuron will have bounded optimal weights due to the linearly inseparable dataset as proven in section \ref{section:BoundedSection}. However, these inputs can then be transformed by the first hidden layer to be into linearly separable transformed inputs for the final output activation. The final output activation will be able to classify these linearly separable transformed inputs and will have unbounded optimal weights as proven in section \ref{section:linearlySeparableProblem}. As the neural network $vec\{W\}$ is a concatenation of both the weights from the first hidden layer and the final activation output, it will consist of both bounded and unbounded weights and the $vec\{W\} = \inf$ (i.e. unbounded) for this globally optimal solution where it classifies each data correctly. 

Bounded case, 
we can of course show that there exist some locally optimal solution where the input remains linearly inseparable and so the final layer will remain bounded.Hence, the $vec\{W\} < \inf$ (i.e. bounded). 

Consider a simple example of the famous XOR problem on 4 data points that is linearly inseparable. 
The data points are 2-Dimensional and have a 1-Dimensional binary output. 
Let $\mathbf{x}^{i}$ be the $i^{th}$ input where $i \in \{1, 2, 3, 4\}$, where
$\mathbf{x}^{i} = [x_{1}^{i}, x_{2}^{i}]$ since the input is 2-Dimensional. 
Define the 4 exact inputs as shown in Table \ref{table:TruthTable}.

\begin{table}[ht]
\centering % used for centering table
\caption{Input and Target Output Data} % title of Table
\label{table:TruthTable} % 
\begin{tabular}{c c c} % centered columns (3 columns)
\hline % single horizontal line
$X_{2}$ & $X_{1}$ & $y$ \\ [0.5ex] 
\hline
0 & 0 & 0 \\
0 & 1 & 1 \\
1 & 0 & 1 \\
1 & 1 & 0 \\ [1ex] % [1ex] adds vertical space
\hline % single line
\end{tabular}
\end{table}

Using 2 hidden units in the first layer. Both with 2-Dimensional weights and  1-Dimensional bias. 

first hidden unit has weights and bias
$\mathbf{W}^{1} = [W^{1}_{1},W^{1}_{2}]$ and $b^{1}$

second hidden unit has weights and bias
$\mathbf{W}^{2} = [W^{2}_{1}, W^{2}_{2}]$ and $b^{2}$

Final output activation has weights 2-Dimension weights and 1-Dimensional bias 
$\mathbf{W}^{3} = [W^{3}_{1}, W^{3}_{2}]$ and $b^{3}$

Output activation of the hidden units is defined in \ref{equation:SigmoidOutput}. 

The output activation of the output layer is said to be a step function where it outputs
1 if the input is $ > 0 $ and 0 if the input is $\le 0$

Optimal weights means that the weights are at a local minimum and its gradient is 0. 
We can also say that an input data that is perfectly classified has no loss and therefore zero gradient. 

The simple neural network with 2 hidden units in a single hidden layer
with a 2 dimensional input is shown below. 

\begin{neuralnetwork}[height=2]
		\newcommand{\nodetextclear}[2]{}
		\newcommand{\nodetextx}[2]{$x_#2$}
		\newcommand{\nodetexty}[2]{$y_#2$}
		\newcommand{\nodetexth}[2]{$h_#2$}
		\inputlayer[count=2, bias=false, title=Input\\layer, text=\nodetextx]
		\hiddenlayer[count=2, bias=false, title=Hidden\\layer, text=\nodetexth] \linklayers
		\outputlayer[count=1, title=Output\\layer, text=\nodetexty] \linklayers
	\end{neuralnetwork}
    
These are all related as shown below in equation \ref{equation:NeuralNetwork}
\begin{equation}
\label{equation:NeuralNetwork}
\begin{split}
z_{1} =& \mathbf{W}^{1} \cdot \mathbf{X}^{T} + b^{1} \\
h_{1} =& \sigma (z_{1}) \\
z_{2} =& \mathbf{W}^{2} \cdot \mathbf{X}^{T} + b^{2} \\
h_{2} =& \sigma (z_{2}) \\
\mathbf{h} =& [h_{1}, h_{2}] \\
\hat{y} =& step(\mathbf{W}^{3} \cdot \mathbf{h}^{T} + b^{3}) \\
\end{split}
\end{equation}

The final prediction, $\hat{y}$ in equation \ref{equation:NeuralNetwork} is compared against the actual target 
$y$ using the cross entropy loss function. 

The first configuration in the equations \ref{equation:BoundedConfiguration} presented below results in all the points correctly classified and the weights are all bounded. The weights and biases are all $ < \inf$ 
but it correctly classifies the 4 points. Therefore, it is an optimal solution that is bounded. 

\begin{equation}
\label{equation:BoundedConfiguration}
\begin{split}
\mathbf{W}^{1} =& [W^{1}_{1},W^{1}_{2}] = [2, 2] \\
b^{1} =& -1 \\
\mathbf{W}^{2} =& [W^{2}_{1}, W^{2}_{2}] = [-2, -2] \\
b^{2} =& 3 \\
\mathbf{W}^{3} =& [W^{3}_{1}, W^{3}_{2}] = [2, 2] \\
b^{3} =& -3 \\
\end{split}
\end{equation}

The output for the equations in \ref{equation:NeuralNetwork} using configuration \ref{equation:BoundedConfiguration} is shown in Table \ref{table:BoundedOutput}.

\begin{table}[ht]
\centering % used for centering table
\caption{Bounded Output Table} % title of Table
\label{table:BoundedOutput} % 
\begin{tabular}{c c c c c c c c} % centered columns (3 columns)
\hline % single horizontal line
$X_{2}$ & $X_{1}$ & $y$ & $z_{1}$  & $z_{2}$  & $h_{1}$ & $h_{2}$ & $\hat{y}$ \\ [0.5ex] 
\hline
0 & 0 & 0 & -1 &  3  & 0 & 1 & 0 \\
0 & 1 & 1 &  1 &  1  & 1 & 1 & 1 \\
1 & 0 & 1 &  1 &  1  & 1 & 1 & 1 \\
1 & 1 & 0 &  3 & -1  & 1 & 0 & 0 \\ [1ex] % [1ex] adds vertical space
\hline % single line
\end{tabular}
\end{table}

The illustration is shown below shows the Bounded configuration on the neural network. note that the link from $x_{1}$ to $h_{2}$ is covered and is supposed to be -2. 
In yellow are the bias links. 

	\begin{neuralnetwork}[height=2.5, layertitleheight=0, nodespacing=2.8cm, layerspacing=1.7cm]
		\newcommand{\nodetextclear}[2]{}
		\newcommand{\nodetextxnb}[2]{\ifnum0=#2 \else $x_#2$ \fi}
		\newcommand{\logiclabel}[1]{\,{$\scriptstyle#1$}\,}
		\newcommand{\nodetexth}[2]{$h_#2$}
		\newcommand{\nodetexty}[2]{$y_#2$}
		\newcommand{\linklabelsE}[4]{\logiclabel{-1}}
		\newcommand{\linklabelsA}[4]{\logiclabel{+2}}
		\newcommand{\linklabelsB}[4]{\logiclabel{-2}}
		\newcommand{\linklabelsC}[4]{\logiclabel{-3}}
		\newcommand{\linklabelsD}[4]{\logiclabel{+3}}
		\setdefaultnodetext{\nodetextclear}
		% Input layer
		\inputlayer[count=2, bias=false, text=\nodetextxnb]
		% links to first hidden layer from input layer
		\hiddenlayer[count=2, bias=true, biaspos=center, text=\nodetexth]
			\link[from layer=0, to layer=1, from node=1, to node=1, label=\linklabelsA]
			\link[from layer=0, to layer=1, from node=2, to node=1, label=\linklabelsB]
			\link[from layer=0, to layer=1, from node=1, to node=2, label=\linklabelsA]
			\link[from layer=0, to layer=1, from node=2, to node=2, label=\linklabelsB]
			\link[from layer=1, to layer=1, from node=0, to node=1, label=\linklabelsE]
			\link[from layer=1, to layer=1, from node=0, to node=2, label=\linklabelsD]
		\outputlayer[count=1, bias=true, text=\nodetexty]
		% links to hidden to output
			\link[from layer=1, to layer=2, from node=1, to node=1, label=\linklabelsA]
			\link[from layer=1, to layer=2, from node=2, to node=1, label=\linklabelsA]
		% links from bias node
			\link[from layer=2, to layer=2, from node=0, to node=1, label=\linklabelsC]
            
	\end{neuralnetwork}


The second configuration in the equations \ref{equation:UnboundedConfiguration} presented below results in all the points correctly classified and there exist unbounded weights. The weights $ \mathbf{W}^{3} = \inf$ 
and it correctly classifies the 4 points. Therefore, it is an optimal solution that is unbounded. 

\begin{equation}
\label{equation:UnboundedConfiguration}
\begin{split}
\mathbf{W}^{1} =& [W^{1}_{1},W^{1}_{2}] = [2, -1] \\
b^{1} =& -1 \\
\mathbf{W}^{2} =& [W^{2}_{1}, W^{2}_{2}] = [-1, 2] \\
b^{2} =& -1 \\
\mathbf{W}^{3} =& [W^{3}_{1}, W^{3}_{2}] = [\inf, \inf] \\
b^{3} =& -1 \\
\end{split}
\end{equation}

The output for the equations in \ref{equation:NeuralNetwork} using configuration \ref{equation:UnboundedConfiguration} is shown in Table \ref{table:UnboundedOutput} .

\begin{table}[ht]
\centering % used for centering table
\caption{UnBounded Output Table} % title of Table
\label{table:UnboundedOutput} % 
\begin{tabular}{c c c c c c c c} % centered columns (3 columns)
\hline % single horizontal line
$X_{2}$ & $X_{1}$ & $y$ & $z_{1}$  & $z_{2}$  & $h_{1}$ & $h_{2}$ & $\hat{y}$ \\ [0.5ex] 
\hline
0 & 0 & 0 & -1 &  -1  & 0 & 0 & 0 \\
0 & 1 & 1 &  1 &  -2  & 1 & 0 & 1 \\
1 & 0 & 1 & -2 &   1  & 0 & 1 & 1 \\
1 & 1 & 0 &  0 &   0  & 0 & 0 & 0 \\ [1ex] % [1ex] adds vertical space
\hline % single line
\end{tabular}
\end{table}

The illustration is shown below shows the UnBounded configuration on the neural network.
In yellow are the bias links. 

	\begin{neuralnetwork}[height=2.5, layertitleheight=0, nodespacing=2.8cm, layerspacing=1.7cm]
		\newcommand{\nodetextclear}[2]{}
		\newcommand{\nodetextxnb}[2]{\ifnum0=#2 \else $x_#2$ \fi}
		\newcommand{\logiclabel}[1]{\,{$\scriptstyle#1$}\,}
		\newcommand{\nodetexth}[2]{$h_#2$}
		\newcommand{\nodetexty}[2]{$y_#2$}
		\newcommand{\linklabelsE}[4]{\logiclabel{\inf}}
		\newcommand{\linklabelsA}[4]{\logiclabel{+2}}
		\newcommand{\linklabelsB}[4]{\logiclabel{-1}}
		\newcommand{\linklabelsC}[4]{\logiclabel{+0}}
		\newcommand{\linklabelsD}[4]{\logiclabel{-1}}
		\setdefaultnodetext{\nodetextclear}
		% Input layer
		\inputlayer[count=2, bias=false, text=\nodetextxnb]
		% links to first hidden layer from input layer
		\hiddenlayer[count=2, bias=true, biaspos=center, text=\nodetexth]
			\link[from layer=0, to layer=1, from node=1, to node=1, label=\linklabelsA]
			\link[from layer=0, to layer=1, from node=2, to node=1, label=\linklabelsB]
			\link[from layer=0, to layer=1, from node=1, to node=2, label=\linklabelsB]
			\link[from layer=0, to layer=1, from node=2, to node=2, label=\linklabelsA]
			\link[from layer=1, to layer=1, from node=0, to node=1, label=\linklabelsB]
			\link[from layer=1, to layer=1, from node=0, to node=2, label=\linklabelsD]
		\outputlayer[count=1, bias=true, text=\nodetexty]
		% links to hidden to output
			\link[from layer=1, to layer=2, from node=1, to node=1, label=\linklabelsE]
			\link[from layer=1, to layer=2, from node=2, to node=1, label=\linklabelsE]
		% links from bias node
			\link[from layer=2, to layer=2, from node=0, to node=1, label=\linklabelsB]
            
	\end{neuralnetwork}


From both configurations above, this example shows that under cross entropy loss without weight decay, 
there are some locally optimal solutions with bounded weight vector $||vec\{W^{*}\}||_{2} < \inf$ and some other locally optimal solutions with unbounded weight vector $||vec\{W^{*}\}||_{2} = \inf$


\clearpage
%------------------------------------------------------------------
%------------------------------------------------------------------
\subsection{Feedforward fully connected neural networks}
\subsubsection{Layer-wise building block}
\begin{minted}[linenos=true,bgcolor=bg,numberblanklines=true,showspaces=false,breaklines=true]
{python}
    def layerWiseBuildingBlock(self, inputTensor, numberOfHiddenUnits):
        """
        Layer-wise Building Block
        Input: 
            inputTensor: Hidden activations from previous layer
            numberOfHiddenUnitsInCurrentLayer
        Output: z = Weighted sum of the inputs
        Initialize Weight Matrix and Biases in this function
        A list of hidden activations
        No Loops
        """
        numInput = inputTensor.get_shape().as_list()[1]
        numOutput = numberOfHiddenUnits
        # Xavier Initialization
        #variance = tf.div(tf.constant(3.0), tf.add(numInput, tf.constant(numOutput)))
        variance = 3.0/(numInput + numOutput)
        weight = tf.Variable(tf.truncated_normal(shape=[numInput, numOutput], stddev = math.sqrt(variance)))
        bias = tf.Variable(tf.zeros([numOutput]))
        weightedSum = tf.matmul(tf.cast(inputTensor, "float32"), weight) + bias
        tf.add_to_collection("Weights", weight)
\end{minted}
\clearpage
%------------------------------------------------------------------
\subsubsection{Learning}
A neural network with hyperparameters listed in Table \ref{table:NN_HParam_2_2_2}. ReLU functions were used for hidden layer activations while softmax was used for output layer activations. The model was trained using various learning rates, as summarised in Table \ref{table:NN_LearningRate}.

\begin{table}[!htb]
\centering
\caption{Neural network hyperparameters}
\label{table:NN_HParam_2_2_2}
\vspace{0.5em}
\begin{tabular}{|l|c|} \hline
\textit{Hyperparameter} & \textit{Value} \\ \hline
Batch size, $B$ & 500 \\
L2 regularizer, $\lambda$ & $3 \times 10^{-4}$  \\
Hidden Layers & 1 \\
Hidden Units & (1000) \\
Dropout Rate & 0\% \\
\hline
\end{tabular}
\end{table}

\begin{table}[!htb]
\centering
\caption{Performance of neural network with respect to $\eta$}
\label{table:NN_LearningRate}
\vspace{0.5em}
\begin{tabular}{|r|r r r|} \hline
\textit{Learning Rate,} $\eta$ & \textit{Epochs} & \textit{Validation Loss} & \textit{Best Test Accuracy, \%} \\ \hline
0.0001 & 21 & 0.354 & 91.5 \\
0.001 & 6 & 0.349 & 91.9 \\
0.01 & 7 & 0.588 & 91.4 \\
\hline
\end{tabular}
\end{table}

The best learning rate was found to be $\eta = 0.001$, which produces the lowest validation loss and highest test accuracy.

The performance of this model can be seen in Figure \ref{figure:A2Q2_2_1_bestEtaGraph} below.

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{Graphics/A2Q2_2_1_bestEtaGraph}
\caption{\label{figure:Softmax} \textcolor{blue}{Training}, \textcolor{orange}{validation} and \textcolor{green}{test} sets cross-entropy losses and classification errors for neural network with best learning rate, $\eta = 0.001$.}
\end{figure}

TODO: For best learning rate, plot without early stopping and highlight early stopping on graph.

\clearpage
%------------------------------------------------------------------
\subsubsection{Early stopping}

\clearpage
%------------------------------------------------------------------
%------------------------------------------------------------------
\subsection{Effect of hyperparameters}
\subsubsection{Number of hidden units}
Using the best learning rate from the previous section, the number of hidden units in the neural network was adjusted \textit{ceteris paribus} (refer to Table \ref{table:NN_HParam_2_3_1}).

\begin{table}[!htb]
\centering
\caption{Neural network hyperparameters}
\label{table:NN_HParam_2_3_1}
\vspace{0.5em}
\begin{tabular}{|l|c|} \hline
\textit{Hyperparameter} & \textit{Value} \\ \hline
Batch size, $B$ & 500 \\
L2 regularizer, $\lambda$ & $3 \times 10^{-4}$  \\
Learning rate, $\eta$ & 0.001 \\
Hidden Layers & 1 \\
Dropout Rate & 0\% \\
\hline
\end{tabular}
\end{table}

Table \ref{table:NN_NumUnits} summarises the performance of the models with varying hidden units. As seen from the table, having a neural network with more hidden units, so long as there is sufficient weights regularization, leads to marginally better performance.

\begin{table}[!htb]
\centering
\caption{Performance of neural network with respect to the number of hidden units}
\label{table:NN_NumUnits}
\vspace{0.5em}
\begin{tabular}{|r|r r r|} \hline
\textit{Hidden Units} & \textit{Epochs} & \textit{Lowest Validation Error, \%} & \textit{Lowest Test Error, \%} \\ \hline
100 & 9 & 7.4 & 9.2 \\
500 & 6 & 7.1 & 8.4 \\
1000 & 6 & 6.1 & 8.1 \\
\hline
\end{tabular}
\end{table}

\clearpage
%------------------------------------------------------------------
\subsubsection{Number of layers}

The model was adjusted to have one or two hidden layers while keeping the total number of hidden units and other hyperparameters the same (refer to Table \ref{table:NN_HParam_2_3_2}). The performance of these two models can be seen in Table \ref{table:NN_NumLayers}.

\begin{table}[!htb]
\centering
\caption{Neural network hyperparameters}
\label{table:NN_HParam_2_3_2}
\vspace{0.5em}
\begin{tabular}{|l|c|} \hline
\textit{Hyperparameter} & \textit{Value} \\ \hline
Batch size, $B$ & 500 \\
L2 regularizer, $\lambda$ & $3 \times 10^{-4}$  \\
Learning rate, $\eta$ & 0.001 \\
Total Hidden Units & 1000 \\
Dropout Rate & 0\% \\
\hline
\end{tabular}
\end{table}

\begin{table}[!htb]
\centering
\caption{Performance of neural network with respect to number of hidden layers}
\label{table:NN_NumLayers}
\vspace{0.5em}
\begin{tabular}{|c c|r r r|} \hline
\textit{Hidden Layers} & \textit{Hidden Units} & \textit{Epochs} & \textit{Validation Loss} & \textit{Best Test Accuracy, \%} \\ \hline
1 & (1000) & 6 & 0.349 & 91.8 \\
2 & (500, 500) & 5 & 0.340 & 91.9 \\
\hline
\end{tabular}
\end{table}

The final training error for the two-layer model turned out to be $8.4\%$. From the table above, the two-layer model achieves marginally higher accuracy.

The performance of the two-layer model is depicted in the figure below.

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{Graphics/A2Q2_3_1_EffectOfLayers}
\caption{\label{figure:NN_EffectOfLayers} Training and validation classification errors for \textcolor{blue}{one-layer} and \textcolor{orange}{two-layer} neural network.}
\end{figure}

\clearpage
%------------------------------------------------------------------
%------------------------------------------------------------------
\subsection{Regularization and visualization}
\subsubsection{Dropout}
Dropout was introduced to the neural network at a rate of 50\% (refer to Table \ref{table:NN_Dropout}). The performance of this model is shown in Figure \ref{figure:NN_EffectOfDropout}.

\begin{table}[!htb]
\centering
\caption{Neural network hyperparameters}
\label{table:NN_HParam_2_4_1}
\vspace{0.5em}
\begin{tabular}{|l|c|} \hline
\textit{Hyperparameter} & \textit{Value} \\ \hline
Batch size, $B$ & 500 \\
L2 regularizer, $\lambda$ & $3 \times 10^{-4}$  \\
Learning rate, $\eta$ & 0.001 \\
Hidden Layers & 1 \\
Hidden Units & (1000) \\
\hline
\end{tabular}
\end{table}

\begin{table}[!htb]
\centering
\caption{Performance of neural network with respect to dropout}
\label{table:NN_Dropout}
\vspace{0.5em}
\begin{tabular}{|c|r r r|} \hline
\textit{Dropout Rate, \%} & \textit{Epochs} & \textit{Validation Loss} & \textit{Best Test Accuracy, \%} \\ \hline
0 & 6 & 0.349 & 91.8 \\
50 & 9 & 0.365 & 91.4 \\
\hline
\end{tabular}
\end{table}

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{Graphics/A2Q2_4_1_EffectOfDropout}
\caption{\label{figure:NN_EffectOfDropout} Training and validation classification errors \textcolor{blue}{with} and \textcolor{orange}{without dropout}.}
\end{figure}

It is noted that the model with dropout takes a longer time to achieve early stopping. This makes sense as dropout forces weights to learn independently of their neighbours by turning off neighbouring activations at random. This leads to an overall result where more training is required until reach early stopping is reached. 

In terms of performance, the model with dropout bears a slightly lower performance, albeit a minute one, from that without dropout. Hence, applying dropout in this situation did not improve the performance of the network but it necessarily. However, the benefits of dropout on model performance are expected to be more pronounced when applied onto a much larger or deeper neural network.

\clearpage
%------------------------------------------------------------------
\subsubsection{Visualization}
To visualise the results, the 784 incoming weights to each hidden unit is plotted as a 28 by 28 grayscale heatmap. This process is repeated to form a 40 by 25 array of heatmaps representing the 1000 hidden units. 

This process is done at 25\%, 50\%, 75\% and 100\% of the early stopping points for the basic neural network (see Table \ref{table:NN_HParam_2_2_2}) and the neural network with dropout (see Table \ref{table:NN_HParam_2_4_1}).

From Figures \ref{figure:VizNoDropout} and \ref{figure:VizDropout} below, the neurons were able to pick up on patterns in the data. Upon close inspection, one can observe shades and patterns that resemble letters in the English alphabet, especially so for the weights of model without dropout. 

The visualised weights change very quickly from 0\% (Gaussian noise) to 25\% (distinguishable patterns). After that, the difference between each snapshot of the weights becomes more subtle, in tandem with the plateauing loss function of the model. This trend is observed in both visualisations involving weights with and without dropout.

One must note that the input data, $\mathbf{x}$, have been mean- and variance-normalised. As a result, the patterns observed in Figure \ref{figure:VizNoDropout} is less obvious (lower contrast) than it would have been, had the input data not been normalised prior to training.

Comparing between the two sets of visualisation with and without dropout, an stark observation is that the patterns are much less distinguishable for the model with dropout. This observation can be explained by the fact that dropout out neurons stochastically trains the neighbouring neurons to learn independently of one another. Graphically, this results in each weight or cell to be to be less correlated to it neighbours, leading to less obvious patterns in the visualisations in Figure \ref{figure:VizDropout}.

\begin{figure}[!htb]
\centering
\textbf{Hidden Layer Weights (without Dropout)}
\vspace{1em}
\includegraphics[width=\textwidth]{Graphics/A2Q2_4_2_VizNoDropout_1}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{Graphics/A2Q2_4_2_VizNoDropout_2}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{Graphics/A2Q2_4_2_VizNoDropout_3}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{Graphics/A2Q2_4_2_VizNoDropout_4}
\caption{\label{figure:VizNoDropout} Visualisation of hidden layer weights for model without dropout at 25\%, 50\%, 75\% and 100\% of early stopping point.}
\end{figure}

\begin{figure}[!htb]
\centering
\textbf{Hidden Layer Weights (with Dropout)}
\vspace{1em}
\includegraphics[width=\textwidth]{Graphics/A2Q2_4_2_VizDropout_1}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{Graphics/A2Q2_4_2_VizDropout_2}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{Graphics/A2Q2_4_2_VizDropout_3}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{Graphics/A2Q2_4_2_VizDropout_4}
\caption{\label{figure:VizDropout} Visualisation of hidden layer weights for model with dropout at 25\%, 50\%, 75\% and 100\% of early stopping point.}
\end{figure}

\clearpage
%------------------------------------------------------------------
\subsection{Exhaustive search for the best set of hyperparameters}
\subsubsection{Random search}
TODO: 

Neural network hyperparameters (except batch size, $B$) were generated stochastically as per assignment instructions. To ensure reproducibility, the seed number '3793' was chosen to be the concatenation of the last two digits of both student numbers. 

This random generation of the model hyperparameters was done five times and the results of are summarised in the table below.

\begin{table}[!htb]
\centering
\caption{Randomly generated neural network hyperparameters and their performances}
\label{table:NN_RandomGeneration}
\vspace{0.5em}
\begin{tabular}{|l|c c c c c|} \hline
\textit{Hyperparameter} & \textit{Run 1} & \textit{Run 2} & \textit{Run 3} & \textit{Run 4} & \textit{Run 5} \\ \hline
Batch size, $B$ & 500 & 500 & 500 & 500 & 500 \\
L2 regularizer, $\lambda$ & 1.70e-3 & 1.40e-3 & 6.38e-4 & 2.67e-4 & 5.15e-4  \\
Learning rate, $\eta$ & 7.05e-4 & 6.85e-3 & 4.97e-3 & 3.77e-3 & 1.08e-3 \\
Hidden Layers & 5 & 4 & 5 & 2 & 1 \\
Hidden Units & \footnotemark & \footnotemark & \footnotemark & \footnotemark & \footnotemark \\
Dropout Rate & 0\% & 50\% & 0\% & 50\% & 0\% \\
\hline
\small{Lowest Validation Error, \%} & 5.9 & 11.4 & 7.6 & 9.6 & 7.2  \\
Lowest Test Error, \% & 7.9 & 12.2 & 8.4 & 10.9 & 8.1 \\
\hline
\end{tabular}
\end{table}
\footnotetext[1]{(236, 438, 326, 298, 496)}
\footnotetext[2]{(409, 381, 103, 388)}
\footnotetext[3]{(480, 124, 154, 322, 361)}
\footnotetext[4]{(184, 377)}
\footnotetext[5]{(197)}

The results above were shared in the class Piazza webpage.

\clearpage
%------------------------------------------------------------------
\subsubsection{Exchange ideas among the groups}

From the postings for Question 2.5.2 scoured on the class Piazza webpage, our colleague \textit{Eric Jiang} had the best performing neural network model at the time of writing. This model was generated using hyperparameter values summarised in Table \ref{table:NN_ExchangeIdeas}. 

We obtained comparatively similar values when we simulated his model using our random seeds. We obtained validation and test errors of $6.4\%$ and $8.3\%$ respectively.

\begin{table}[!htb]
\centering
\caption{Best model hyperparameters on Piazza and its performance}
\label{table:NN_ExchangeIdeas}
\vspace{0.5em}
\begin{tabular}{|l|c|} \hline
\textit{Hyperparameter} & \textit{Values} \\ \hline
Batch size, $B$ & 500 \\
L2 regularizer, $\lambda$ & 9.10e-4  \\
Learning rate, $\eta$ & 2.92e-3 \\
Hidden Layers & 5 \\
Hidden Units & (496, 276, 486, 299, 376) \\
Dropout Rate & 0\% \\
\hline
\small{Lowest Validation Error, \%} & 7.0 \\
Lowest Test Error, \% & 6.4 \\
\hline
\end{tabular}
\end{table}
\footnotetext{at time of writing}

\clearpage
%------------------------------------------------------------------
%------------------------------------------------------------------
%------------------------------------------------------------------
\section{Appendices}
\subsection{Entire Code: Chee Loong Soon's version}

\clearpage
%------------------------------------------------------------------
%------------------------------------------------------------------
\subsection{Entire Code: FuYuan Tee's version}
\subsubsection{Base Code for Q1}
\begin{minted}[linenos=true,bgcolor=bg,numberblanklines=true,showspaces=false,breaklines=true]
{python}
import tensorflow as tf
import numpy as np
import math

import time
import datetime

# Non-interactive plotting
import matplotlib.pyplot as plt
from IPython import display

# Interactive plotting
from plotly import tools
import plotly.plotly as py
import plotly.graph_objs as go
import plotly.offline as pyo
from plotly.offline import download_plotlyjs

# Configure environment
%config InlineBackend.figure_format = 'retina'
np.set_printoptions(precision=3)

# Global Variables
CURRENT_DIR = '/Users/christophertee/Dropbox/University/MASc/Courses/Winter 2017' + \
'/ECE521 (Inference Algorithms & Machine Learning)/Assignment 2'
LOG_DIR = '/Logs'

# Activate Plotly Offline for Jupyter
pyo.init_notebook_mode(connected=True)

# Load notMNIST dataset
'''
 Training Set: 3500 images
 Validation Set: 100 images
 Test Set: 145 images
 Images are 28 x 28 (normalised) pixels
'''
with np.load("notMNIST.npz") as data:
    Data, Target = data ["images"], data["labels"]
    
    # Subsetting data for classes 'C' (2) and 'J' (9)
    posClass = 2 # 'C'
    negClass = 9 # 'J'
    dataIndx = (Target==posClass) + (Target==negClass)
    
    Data = Data[dataIndx] / 255.
    Target = Target[dataIndx].reshape(-1, 1)
    
    # Converts target labels to 'C' (0) and 'J' (1)
    Target[Target==posClass] = 1
    Target[Target==negClass] = 0
    
    # Set random seed
    np.random.seed(521)
    
    # Generate and shuffle random index
    randIndx = np.arange(len(Data))
    np.random.shuffle(randIndx)
    
    Data = Data[randIndx]
    Target = Target[randIndx]
    
    # Flatten arrays of dimension m x 28 x 28 into array of dimension m x 784
    Data = Data.reshape(Data.shape[0], -1)
    
    # Standardizing inputs of dataset
    Data -= np.mean(Data, axis=0)
    Data /= np.std(Data, axis=0)
    
    # Partition data into training, validation and test datasets
    Data, Target = Data[randIndx], Target[randIndx]
    trainData, trainTarget = Data[:3500], Target[:3500]
    validData, validTarget = Data[3500:3600], Target[3500:3600]
    testData, testTarget = Data[3600:], Target[3600:]
    
MAX_ITER = 10000
def tuneLearningRate(etaList, model, section_dir, batchSize=500, lambda_=0.01, opt='Adam'):    
    # Returns the i-th batch of training data and targets
    # Generates a new, reshuffled batch once all previous batches are fed
    def getNextTrainingBatch(currentIter, randIdx):
        currentBatchNum = currentIter % (trainData.shape[0] / batchSize)
        if currentBatchNum == 0:
            np.random.shuffle(randIdx)
        lowerBoundIdx = currentBatchNum * batchSize
        upperBoundIdx = (currentBatchNum + 1) * batchSize 
        return trainData[randIdx[lowerBoundIdx:upperBoundIdx]], trainTarget[randIdx[lowerBoundIdx:upperBoundIdx]]
    
    # Generate updated plots for training and validation MSE
    def plotErrGraph(errList, param):
        label = '$\eta$ = ' + str(param)
        label_classification = ['train.', 'valid.']

        display.clear_output(wait=True)
        plt.figure(figsize=(8,5), dpi=200)
        
        for i, err in enumerate(errList):
            plt.plot(range(len(err)), err, '-', label=label+' '+label_classification[i])
        
        plt.axis([0, MAX_ITER, 0, np.amax(errList)])
        plt.legend()
        plt.show()
    
    # Calculates the ratio between the n-th average epoch MSE and the (n-1)-th average epoch MSE
    def ratioAverageEpochMSE(currentValidErr):
        averageN = np.average(currentValidErr[-(np.arange(epochSize - 1) + 1)])
        averageNlessOne = np.average(currentValidErr[-(np.arange(epochSize - 1) + epochSize)])
        return averageN / averageNlessOne
    
    # Returns True if the average epoch validation MSE is at least 99.9% of the previous epoch average.
    # i.e. Returns True if the average learnings between epoch is less than +0.1%
    # Otherwise, returns False
    def shouldStopEarly(currentValidErr):
        if currentValidErr.shape[0] < 2 * epochSize:
            return False
        return True if (ratioAverageEpochMSE(currentValidErr) > 0.999) else False
    
    
    # Start of function
    summaryList = []
    randIdx = np.arange(trainData.shape[0])
    epochSize = trainData.shape[0] / batchSize
    randIdx = np.arange(trainData.shape[0])
    
    assert section_dir
    chapter_dir = '/Binary Loss/Section ' + section_dir + '/'
    current_time = '{:%b%d %H_%M_%S}'.format(datetime.datetime.now())
    
    for eta in etaList:
        # Reset graph to prevent duplication of ops and variables
        tf.reset_default_graph()
        
        # Build new graph
        assert model in ['linear', 'logistic', 'softmax']
        if model == 'linear':
            W, b, X, Y, YhatThres, error, accuracy, optimizer = buildLinearGraph(eta, lambda_)
        elif model == 'logistic':
            W, b, X, Y, YhatThres, error, accuracy, optimizer = buildLogisticGraph(eta, lambda_, opt)
        elif model == 'softmax':
            W, b, X, Y, YhatThres, error, accuracy, optimizer = buildSoftmaxGraph(eta, lambda_)
        
        # Begin session
        with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)) as sess:
            # Log starting time
            startTime = time.time()
            
            # Create summary writer
            writer = tf.summary.FileWriter(CURRENT_DIR + LOG_DIR + chapter_dir + current_time + \
                                           '/Eta-' + str(eta), \
                                           graph=sess.graph)
            
            # Initialise all TensorFlow variables
            tf.global_variables_initializer().run()

            # Creates blank training and validation MSE arrays for the Session
            currentTrainErr = np.array([])[:, np.newaxis]
            currentValidErr = np.array([])[:, np.newaxis]
            currentTestErr = np.array([])[:, np.newaxis]
            
            currentTrainAcc = np.array([])[:, np.newaxis]
            currentValidAcc = np.array([])[:, np.newaxis]
            currentTestAcc = np.array([])[:, np.newaxis]
    
            # Runs update
            currentIter = 0
            while currentIter < MAX_ITER:
                inputData, inputTarget = getNextTrainingBatch(currentIter, randIdx)
                
                _, trainErr, trainAcc = sess.run([optimizer, error, accuracy], feed_dict={X: inputData, Y: inputTarget})
                validErr, validAcc = sess.run([error, accuracy], feed_dict={X: validData, Y: validTarget})
                testErr, testAcc = sess.run([error, accuracy], feed_dict={X: testData, Y: testTarget})

                currentTrainErr = np.append(currentTrainErr, trainErr)
                currentValidErr = np.append(currentValidErr, validErr)
                currentTestErr = np.append(currentTestErr, testErr)
                
                currentTrainAcc = np.append(currentTrainAcc, trainAcc)
                currentValidAcc = np.append(currentValidAcc, validAcc)
                currentTestAcc = np.append(currentTestAcc, testAcc)
                
                # Update graph of training and validation MSE arrays
#                 if (currentIter < 3) or (currentIter % 1000 == 0):
#                     plotErrGraph([currentTrainErr, currentValidErr], eta)
                
                # At every epoch, check for early stopping possibilty. If so, breaks from while loop
                if currentIter % epochSize == 0:
                    if shouldStopEarly(currentValidErr):
                        writer.close()
                        break
                
                currentIter += 1
                
                if currentIter == MAX_ITER:
                    writer.close()
            
        # Save session results as dictionary and appends to MSEsummaryList
        summaryList.append(
            {
                'eta': eta,
                'B': batchSize,
                'lambda': lambda_,
                'optimizer': opt,
                'numIter': currentIter + 1,
                'epoch': float(currentIter + 1) / epochSize,
                'trainError': currentTrainErr,
                'validError': currentValidErr,
                'testError': currentTestErr,
                'trainAccuracy': currentTrainAcc,
                'validAccuracy': currentValidAcc,
                'testAccuracy': currentTestAcc
            }
        )
        
        # Print stats when one eta run is done
        print 'eta: %7.3f, numIter: %7d, validError: %.3f, testAcc: %.3f duration: %3.1fs' % \
            (summaryList[-1]['eta'], summaryList[-1]['numIter'], summaryList[-1]['validError'][-1], \
             np.mean(summaryList[-1]['testAccuracy'][-epochSize:]), time.time() - startTime)
            
    return summaryList
\end{minted}
%------------------------------------------------------------------
\subsubsection{Q1.1.1: Learning}
\begin{minted}[linenos=true,bgcolor=bg,numberblanklines=true,showspaces=false,breaklines=true]
{python}
# TensorFlow logistic model
def buildLogisticGraph(eta, lambda_, opt):
    assert opt in ['GD', 'Adam']
    with tf.device('/cpu:0'):
        with tf.variable_scope('logistic_regression'):
            # Model inputs
            with tf.name_scope('placeholders'):
                X = tf.placeholder(tf.float32, shape=[None, None], name='input')
                Y = tf.placeholder(tf.float32, shape=[None, None], name='target')

            # Model parameters
            with tf.name_scope('parameters'):
                W = tf.get_variable('weights', shape=[784, 1], initializer=tf.truncated_normal_initializer(stddev=0.5))
                b = tf.get_variable('biases', shape=[1, 1], initializer=tf.constant_initializer(0.0))
    
            with tf.device('/cpu:0'):
                # Predicted target
                with tf.name_scope('prediction'):
                    Z = tf.add(tf.matmul(X, W), b, 'logits')
                    Yhat = tf.sigmoid(Z, name='pred')

                # Metrics
                with tf.name_scope('metrics'):
                    with tf.name_scope('error'):
                        error = tf.add(tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Z, labels=Y, name='elem_x_entropy'), \
                                                      name='total_x_entropy'), \
                                       tf.multiply(lambda_ / 2, tf.matmul(tf.transpose(W), W), name='l2_loss'), \
                                       name='total_loss')
                    with tf.name_scope('threshold'):
                        YhatThres = tf.cast(tf.greater_equal(Yhat, 0.5, name='pred_thres'), tf.float32)
                    with tf.name_scope('accuracy'):
                        accuracy = tf.truediv(tf.reduce_sum(tf.cast(tf.equal(YhatThres, Y), tf.int32), name='total_matches'), \
                                              tf.shape(X)[0], \
                                              name='accuracy')

        # Optimizer
        with tf.device('/cpu:0'):
            if opt == 'GD':
                optimizer = tf.train.GradientDescentOptimizer(eta).minimize(error)
            elif opt == 'Adam':
                optimizer = tf.train.AdamOptimizer(eta).minimize(error)
    
    return W, b, X, Y, YhatThres, error, accuracy, optimizer
    
etaList = [0.001, 0.01, 0.1, 1, 10]
summary1_1 = tuneLearningRate(etaList, model='logistic', section_dir='1.1', opt='GD')
\end{minted}
%------------------------------------------------------------------
\subsubsection{Q.1.1.2: Beyond Plain SGD}
\begin{minted}[linenos=true,bgcolor=bg,numberblanklines=true,showspaces=false,breaklines=true]
{python}
etaList = [1e-4, 1e-3, 1e-2, 1e-1, 1]
summary1_2 = tuneLearningRate(etaList, model='logistic', section_dir='1.2', opt='Adam')
\end{minted}
%------------------------------------------------------------------
\subsubsection{Q1.1.3 Comparison with linear regression}
\begin{minted}[linenos=true,bgcolor=bg,numberblanklines=true,showspaces=false,breaklines=true]
{python}
# TensorFlow Linear Graph
def buildLinearGraph(eta, lambda_):
    with tf.device('/cpu:0'):
        with tf.variable_scope('linear_regression'):
            # Model inputs
            with tf.name_scope('placeholders'):
                X = tf.placeholder(tf.float32, shape=[None, None], name='Input')
                Y = tf.placeholder(tf.float32, shape=[None, None], name='Target')

            # Model parameters
            with tf.name_scope('parameters'):
                W = tf.get_variable('weights', shape=[784, 1], initializer=tf.truncated_normal_initializer(stddev=0.5))
                b = tf.get_variable('biases', shape=[1, 1], initializer=tf.constant_initializer(0.0))
    
            with tf.device('/cpu:0'):
                # Predicted target
                with tf.name_scope('prediction'):
                    Yhat = tf.add(tf.matmul(X, W), b, 'pred')

                # Metrics
                with tf.name_scope('metrics'):
                    with tf.name_scope('error'):
                        error = tf.add(tf.reduce_mean(tf.nn.l2_loss(tf.subtract(Yhat, Y), name='elem_l2_loss'), \
                                                      name='total_l2_loss'), \
                                       tf.multiply(lambda_ / 2, tf.matmul(tf.transpose(W), W), name='l2_loss'), \
                                       name='total_loss')
                    with tf.name_scope('threshold'):
                        YhatThres = tf.cast(tf.greater_equal(Yhat, 0.5, name='pred_thres'), tf.float32)
                    with tf.name_scope('accuracy'):
                        accuracy = tf.truediv(tf.reduce_sum(tf.cast(tf.equal(YhatThres, Y), tf.int32), name='total_matches'), \
                                              tf.shape(X)[0], \
                                              name='accuracy')

        # Optimizer
        with tf.device('/cpu:0'):
            optimizer = tf.train.AdamOptimizer(eta).minimize(error)
    
    return W, b, X, Y, YhatThres, error, accuracy, optimizer

etaList = [1e-4, 1e-3, 1e-2, 1e-1, 1, 10]
print 'Linear Regression:'
summary1_3_linear = tuneLearningRate(etaList, model='linear', section_dir='1.3Lin', lambda_=0.0)
print '\nLogistic Regression:'
summary1_3_logistic = tuneLearningRate(etaList, model='logistic', section_dir='1.3Log', lambda_=0.0)

def generateDummyLosses():
    # Generate dummy data
    Yhat = np.linspace(0.01,1,100)[:, np.newaxis]
    Ydummy = 0.0

    # Calculate losses
    xEntropy = - Ydummy * np.log(Yhat) - (1 - Ydummy) * np.log(1 - Yhat)
    squaredErr = (Yhat - Ydummy)**2
    
    # Generate plot traces
    xEntropyTrace = go.Scatter(
        x = Yhat,
        y = xEntropy,
        name = 'Cross-Entropy'
    )
    squareErrTrace = go.Scatter(
        x = Yhat,
        y = squaredErr,
        name = 'Squared-Error'
    )
    
    data = go.Data([xEntropyTrace, squareErrTrace])
    
    # Generate figure layout
    layout = go.Layout(
        title = '$\\text{Comparison of Cross Entropy and Squared Error Losses on Dummy Variable } y = 0$',
        xaxis = {'title': '$\\hat{y}$'},
        yaxis = {'title': 'Loss'}
    )
    
    # Creates and plots figure
    figure = go.Figure(data=data, layout=layout)
    return py.iplot(figure, filename='A2Q1.1.3_LossComparison')

fig1_3 = generateDummyLosses()
fig1_3
\end{minted}
%------------------------------------------------------------------
\subsubsection{Q1.2.3: Multi-class classification}
\begin{minted}[linenos=true,bgcolor=bg,numberblanklines=true,showspaces=false,breaklines=true]
{python}
# TensorFlow Softmax Modeldef buildSoftmaxGraph(eta, lambda_):
    with tf.device('/cpu:0'):
        with tf.variable_scope('softmax'):
            # Model inputs
            with tf.name_scope('placeholders'):
                X = tf.placeholder(tf.float32, shape=[None, None], name='input')
                Y = tf.placeholder(tf.float32, shape=[None, None], name='target')

            # Model parameters
            with tf.name_scope('parameters'):
                W = tf.get_variable('weights', shape=[784, 10], initializer=tf.truncated_normal_initializer(stddev=0.5))
                b = tf.get_variable('biases', shape=[1, 10], initializer=tf.constant_initializer(0.0))
    
    with tf.device('/cpu:0'):
        # Predicted target
        with tf.name_scope('prediction'):
            Z = tf.add(tf.matmul(X, W), b, 'logits')
            Yhat = tf.nn.softmax(Z, name='activation')

        # Metrics
        with tf.name_scope('metrics'):
            with tf.name_scope('error'):
                with tf.name_scope('x_entropy'):
                    x_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z, \
                                                                                       labels=Y,\
                                                                                       name='elem_x_entropy'), \
                                               name='total_x_entropy')
                with tf.name_scope('l2_loss'):
                    l2_loss = tf.multiply(lambda_ / 2, \
                                          tf.reduce_sum(tf.matmul(tf.transpose(W), W)), name='l2_loss')
                with tf.name_scope('total_loss'):
                    error = tf.add(x_entropy, l2_loss, name='total_loss')
            with tf.name_scope('threshold'):
                YhatThres = tf.cast(tf.argmax(Yhat, axis=1, name='pred_threshold'), tf.float32)
            with tf.name_scope('accuracy'):
                Ycollapsed = tf.cast(tf.argmax(Y, axis=1), tf.float32, name='target_reverse_one_hot')
                accuracy = tf.truediv(tf.reduce_sum(tf.cast(tf.equal(YhatThres, Ycollapsed) \
                                                            , tf.int32), \
                                                    name='total_matches'), \
                                      tf.shape(X)[0], \
                                      name='accuracy')

    # Optimizer
    with tf.device('/cpu:0'):
        optimizer = tf.train.AdamOptimizer(eta).minimize(error)
    
    return W, b, X, Y, YhatThres, error, accuracy, optimizer
    
# Load notMNIST dataset
'''
 Training Set: 15,000 images
 Validation Set: 1,000 images
 Test Set: 2,724 images
 Images are 28 x 28 (normalised) pixels
'''
with np.load("notMNIST.npz") as data:
    Data, Target = data ["images"], data["labels"]
    
    # Set random seed
    np.random.seed(521)
    
    # Generate and shuffle random index
    randIndx = np.arange(len(Data))
    np.random.shuffle(randIndx)
    
    Data = Data[randIndx]/255.
    Target = Target[randIndx]
    
    # Generates one-hot version of target
    oneHot = np.zeros((Target.shape[0], 10))
    oneHot[np.arange(Target.shape[0]), Target] = 1
    Target = oneHot
    
    # Flatten arrays of dimension m x 28 x 28 into array of dimension m x 784
    Data = Data.reshape(Data.shape[0], -1)
    
    # Standardizing inputs of dataset
    Data -= np.mean(Data, axis=0)
    Data /= np.std(Data, axis=0)
    
    # Partition data into training, validation and test datasets
    trainData, trainTarget = Data[:15000], Target[:15000]
    validData, validTarget = Data[15000:16000], Target[15000:16000]
    testData, testTarget = Data[16000:], Target[16000:]
\end{minted}
%------------------------------------------------------------------
\subsubsection{Base Code for Q2}
\begin{minted}[linenos=true,bgcolor=bg,numberblanklines=true,showspaces=false,breaklines=true]
{python}
import tensorflow as tf
import numpy as np
import math

import time
import datetime

# Non-interactive plotting
import matplotlib.pyplot as plt
from IPython import display

# Interactive plotting
from plotly import tools
import plotly.plotly as py
import plotly.graph_objs as go
import plotly.offline as pyo
from plotly.offline import download_plotlyjs

# Configure environment
%config InlineBackend.figure_format = 'retina'
np.set_printoptions(precision=3)

# Global Variables
CURRENT_DIR = '/Users/christophertee/Dropbox/University/MASc/Courses/Winter 2017' + \
'/ECE521 (Inference Algorithms & Machine Learning)/Assignment 2'
LOG_DIR = '/Logs'

# Activate Plotly Offline for Jupyter
pyo.init_notebook_mode(connected=True)

# Load notMNIST dataset
'''
 Training Set: 15,000 images
 Validation Set: 1,000 images
 Test Set: 2,724 images
 Images are 28 x 28 (normalised) pixels
'''
with np.load("notMNIST.npz") as data:
    Data, Target = data ["images"], data["labels"]
    
    # Set random seed
    np.random.seed(521)
    
    # Generate and shuffle random index
    randIndx = np.arange(len(Data))
    np.random.shuffle(randIndx)
    
    Data = Data[randIndx]/255.
    Target = Target[randIndx]
    
    # Generates one-hot version of target
#     Target = np.eye(10)[Target]
    oneHot = np.zeros((Target.shape[0], 10))
    oneHot[np.arange(Target.shape[0]), Target] = 1
    Target = oneHot
    
    # Flatten arrays of dimension m x 28 x 28 into array of dimension m x 784
    Data = np.reshape(Data, (Data.shape[0], 784))
    
    # Standardizing inputs of dataset
    Data -= np.mean(Data, axis=0)
    Data /= np.std(Data, axis=0)
    
    # Partition data into training, validation and test datasets
    trainData, trainTarget = Data[:15000], Target[:15000]
    validData, validTarget = Data[15000:16000], Target[15000:16000]
    testData, testTarget = Data[16000:], Target[16000:]
    
# TensorFlow Fully-Connected Neural Network
def build_FCNN(eta, lambda_, numHidLayers, layerUnits, dropout_rate):
    NUM_FEATURES = 784
    NUM_TARGET_CATEGORIES = 10
    '''
    - Builds a fully-connected neural network with numHidLayers hidden layers, with 
      layerUnits[i] in the i-th hidden layer
      
    Inputs:
        eta: Learning rate
        lambda_: L2 regularizer
        numHidLayers: Number of hidden layers
        layerUnits: List of number of units in each layer; len(layerUnits) == numHidLayer
    '''
    
    def connect_layers(h, numNextUnit, name='input_to_layer'):
        '''
        - Creates weights and biases to connect previous layer to next layer
        - Returns the weighted sum of inputs for to the next layer
        - Initializes weights using Xavier initialization.
        
        Inputs:
            hIn: Incoming activation tensor from previous layer
            numOut: Number of hidden units of next layer
            
        Outputs:
            z: Weighted sum of inputs into the next layer
        '''
        numPrevUnit = h.get_shape()[1]
        
        # Xavier initialization for weights
        xavier = tf.sqrt(3 / tf.cast(numPrevUnit + numNextUnit, tf.float32), name='xavier')
        
        W = tf.get_variable('weights', shape=[numPrevUnit, numNextUnit], \
                            initializer=tf.truncated_normal_initializer(stddev=xavier, seed=1))
        b = tf.get_variable('biases', shape=[1, numNextUnit], initializer=tf.constant_initializer(0.0))
        
        tf.add_to_collection(tf.GraphKeys.WEIGHTS, W)
        tf.add_to_collection(tf.GraphKeys.BIASES, b)
        
        return tf.add(tf.matmul(h, W), b, name=name)
    
    
    def build_layers(numHidLayers, layerUnits, dropout_rate):
        '''
        - Builds hidden layers and output layer
        
        Inputs:
            numHidLayers: Number of hidden layers
            layerUnits: List of number of units in each layer; len(layerUnits) == numHidLayer
            
        Output:
            zOut: Logits to output layer
        '''
        ### Builds hidden layers
        ZList = [] # Stores the inputs to the i-th layer
        hList = [] # Stores the activation of the i-th layer
        
        for i in range(numHidLayers):
            # Create the i-th hidden layer
            with tf.variable_scope('hidden_layer_%d' % i) as scope:
                # Calculates input to the i-th hidden layer
                # Uses placeholder tensor X instead of Z_(i-1) in connectLayers for the first hidden layer
                if i == 0:
                    ZList.append(connect_layers(X, layerUnits[i]))
                else:
                    ZList.append(connect_layers(hList[-1], layerUnits[i]))
                
                # Calculates ReLU activation of each layer
                hList.append(tf.nn.dropout(tf.nn.relu(ZList[i], name='hidden_activation'), keep_prob=1-dropout_rate, \
                                          name='dropped_activations'))
            
        # Adds activation variables into GraphKey.ACTIVATIONS collection
        for h in hList:
            tf.add_to_collection(tf.GraphKeys.ACTIVATIONS, h)
            
        ### Builds output layer
        with tf.variable_scope('output_layer'):
            ZList.append(connect_layers(hList[-1], NUM_TARGET_CATEGORIES, name='logits'))
        
        return ZList[numHidLayers]
    
    
    #######################################
    #           Function begins           #
    #######################################
    mode = '/gpu:0'
    tf.set_random_seed(521)
    
    assert len(layerUnits) == numHidLayers
    
    with tf.device('/cpu:0'):
        with tf.name_scope('placeholders'):
            X = tf.placeholder(tf.float32, shape=[None, NUM_FEATURES], name='input_layer')
            Y = tf.placeholder(tf.float32, shape=[None, NUM_TARGET_CATEGORIES], name='target')
        
    with tf.device(mode):
        logits = build_layers(numHidLayers, layerUnits, dropout_rate) # Logits to output layer
        
        # Calculates prediction
        with tf.name_scope('prediction'):
            Yhat = tf.nn.softmax(logits, name='pred')
            YhatThres = tf.cast(tf.argmax(Yhat, axis=1, name='pred_threshold'), tf.float32)
        
    with tf.device('/cpu:0'):
        # Calculates training metrics
        with tf.name_scope('metrics'):
            # Calculates loss function
            with tf.name_scope('loss'):
                # Calculates cross-entropy
                with tf.name_scope('xent'):
                    xent = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, \
                                                                                  labels=Y, \
                                                                                  name='elem_x_entropy'), \
                                          name='total_x_entropy')
                # Calculates l2 loss
                with tf.name_scope('l2_loss'):
                    weights = tf.get_collection_ref(tf.GraphKeys.WEIGHTS)
                    biases = tf.get_collection_ref(tf.GraphKeys.BIASES)
                    l2_loss = tf.multiply(lambda_ / 2, \
                                          tf.add_n([tf.nn.l2_loss(W) for W in weights]), name='total_l2_loss')
                # Calculates total loss
                with tf.name_scope('total_loss'):
                    error = tf.add(xent, l2_loss, name='total_loss')
            # Calculates accuracy
            with tf.name_scope('accuracy'):
                Ycollapsed = tf.cast(tf.argmax(Y, axis=1), tf.float32, name='target_reverse_one_hot')
                accuracy = tf.truediv(tf.reduce_sum(tf.cast(tf.equal(YhatThres, Ycollapsed) \
                                                            , tf.int32), \
                                                    name='total_matches'), \
                                      tf.shape(X)[0], \
                                      name='accuracy')
    with tf.device(mode):
        # Optimizer
        with tf.variable_scope('Adam'):
            optimizer = tf.train.AdamOptimizer(eta).minimize(error)
    
    return X, Y, YhatThres, weights, error, accuracy, optimizer
    
# Define training function
def train_NN(etaList, numHidLayersList, layerUnitsList, sectionDir, batchSize=500, lambda_=3e-4, dropout_rate=0):    
    MAX_ITER = 3000
    '''
    - Returns the i-th batch of training data and targets
    - Generates a new, reshuffled batch once all previous batches are fed
    '''
    def get_next_training_batch(currentIter, randIdx):
        currentBatchNum = currentIter % (trainData.shape[0] / batchSize)
        if currentBatchNum == 0:
            np.random.shuffle(randIdx)
        lowerBoundIdx = currentBatchNum * batchSize
        upperBoundIdx = (currentBatchNum + 1) * batchSize 
        return trainData[randIdx[lowerBoundIdx:upperBoundIdx]], trainTarget[randIdx[lowerBoundIdx:upperBoundIdx]]
    
    # Generate updated plots for training and validation MSE
    def plot_err_graph(errList, param):
        label = '$\eta$ = ' + str(param)
        label_classification = ['train.', 'valid.']

        display.clear_output(wait=True)
        plt.figure(figsize=(8,5), dpi=200)
        
        for i, err in enumerate(errList):
            plt.plot(range(len(err)), err, '-', label=label+' '+label_classification[i])
        
        plt.axis([0, MAX_ITER, 0, np.amax(errList)])
        plt.legend()
        plt.show()
    
    # Calculates the ratio between the n-th average epoch MSE and the (n-1)-th average epoch MSE
    def average_epoch_err_ratio(validErr):
        averageN = np.mean(validErr[-epochSize:])
        averageNlessOne = np.mean(validErr[-epochSize*2:-epochSize])
        return averageN / averageNlessOne
    
    # Returns True if the average epoch validation MSE is at least 99.9% of the previous epoch average.
    # i.e. Returns True if the average learnings between epoch is less than +0.1%
    # Otherwise, returns False
    def should_stop_early(validErr):
        if validErr.shape[0] < 2 * epochSize:
            return False
        return True if (average_epoch_err_ratio(validErr) > 1) else False
    
    # Start of function
    summaryList = []
    epochSize = trainData.shape[0] / batchSize
    
    assert sectionDir
    
    for eta in etaList:
        for i, layerUnits in enumerate(layerUnitsList):
            numHidLayers = numHidLayersList[i]
            
            # Set random seed
            np.random.seed(521)
            randIdx = np.arange(trainData.shape[0])
            
            # Prints run hyperparameters
            print 'Eta: ', eta
            print 'Hidden Layers: ', numHidLayers
            print 'Units in Layers: ', layerUnits
            print 'Dropout Rate: ', dropout_rate
            
            # Create directory paths
            chapterDir = '/Neural Network/Section ' + sectionDir + '/Eta_' + str(eta) + '/'
            currentTime = '{:%b%d %H_%M_%S}'.format(datetime.datetime.now())
            writeDir = CURRENT_DIR + LOG_DIR + chapterDir + currentTime

            # Build new graph
            tf.reset_default_graph() # Reset graph to prevent duplication of ops and variables
            X, Y, YhatThres, W, error, accuracy, optimizer = build_FCNN(eta, lambda_, numHidLayers, layerUnits, dropout_rate)
            # Create saver
        #     saver = tf.train.Saver()

            # Begin session
            with tf.Session(config=tf.ConfigProto(log_device_placement=False)) as sess:
                # Log starting time
                startTime = time.time()

                # Create summary writer
                writer = tf.summary.FileWriter(writeDir, graph=sess.graph)

                # Initialise all TensorFlow variables
                tf.global_variables_initializer().run()

                # Create blank training, validation and test error arrays for the Session
                trainErr = np.array([])[:, np.newaxis]
                validErr = np.array([])[:, np.newaxis]
                testErr = np.array([])[:, np.newaxis]

                # Create blank training, validation and test accuracies arrays for the Session
                trainAcc = np.array([])[:, np.newaxis]
                validAcc = np.array([])[:, np.newaxis]
                testAcc = np.array([])[:, np.newaxis]
                
                # Create blank arrays to store weights for the Session
                weights_l0 = np.array([])[:, np.newaxis]

                # Runs update
                currentIter = 0
                while currentIter < MAX_ITER:
                    # To be memory efficient, save weights only at the start of each epoch
                    if currentIter % epochSize == 0:
                        if currentIter == 0:
                            weights_l0 = sess.run(W)[0][:,:, np.newaxis]
                        else:
                            weights_l0 = np.append(weights_l0, sess.run(W)[0][:,:, np.newaxis], axis=2)
                    
                    # Create training mini-batch stochastically from data
                    inputData, inputTarget = get_next_training_batch(currentIter, randIdx)

                    # Calculate error and accuracy metrics before optimization routine
                    currTrainErr, currTrainAcc = sess.run([error, accuracy], feed_dict={X: inputData, Y: inputTarget})
                    currValidErr, currValidAcc = sess.run([error, accuracy], feed_dict={X: validData, Y: validTarget})
                    currTestErr, currTestAcc = sess.run([error, accuracy], feed_dict={X: testData, Y: testTarget})
                    
                    # Append run values
                    trainErr = np.append(trainErr, currTrainErr)
                    validErr = np.append(validErr, currValidErr)
                    testErr = np.append(testErr, currTestErr)

                    trainAcc = np.append(trainAcc, currTrainAcc)
                    validAcc = np.append(validAcc, currValidAcc)
                    testAcc = np.append(testAcc, currTestAcc)
                    
                    # Perform optimization routine
                    sess.run(optimizer, feed_dict={X: inputData, Y: inputTarget})

                    # Provide info on training progress at the end of every 10th epoch
                    if currentIter == 0 or currentIter % (epochSize * 10) == 0:
                        print 'epoch: %5.0f, currTrainErr: %10.2f, currValidErr: %10.2f' % \
                            ((currentIter + 1) / epochSize, currTrainErr, currValidErr)
        #                 plot_err_graph([trainErr, validErr], eta)

                    # At the end of each epoch, check for early stopping possibilty. If so, break from while loop.
                    if (currentIter + 1) % epochSize == 0:
                        if should_stop_early(validErr):
                            writer.close()
                            print 'Exited from early stopping\n'
                            break

                    # At every 10 epochs, creates checkpoint for session
#                     if currentIter % epochSize == 10:
#                         saver.save(sess, writeDir, global_step=currentIter)

                    currentIter += 1

                    if currentIter == MAX_ITER:
                        print 'MAX_ITER reached\n'
                        writer.close()

            # Save session results as dictionary and appends to summaryList
            summaryList.append(
                {
                    'eta': eta,
                    'B': batchSize,
                    'lambda': lambda_,
                    'hidden_layers': numHidLayers,
                    'hidden_units': layerUnits,
                    'numIter': currentIter + 1 - epochSize,
                    'epoch': float(currentIter + 1) / epochSize - 1,
                    'trainError': trainErr[::epochSize].copy()[::],
                    'validError': validErr[::epochSize].copy()[::],
                    'testError': testErr[::epochSize].copy()[::],
                    'trainAccuracy': trainAcc[::epochSize].copy()[::],
                    'validAccuracy': validAcc[::epochSize].copy()[::],
                    'testAccuracy': testAcc[::epochSize].copy()[::],
                    'weights_layer_0': weights_l0
                }
            )

            # Print stats when a run is done
            print 'epochs: %.1f, currentIter: %d' % (summaryList[-1]['epoch'], summaryList[-1]['numIter'])
            print 'trainError: %.3f, validError: %.3f, testError: %.3f' % \
                    (summaryList[-1]['trainError'][-1], summaryList[-1]['validError'][-1], summaryList[-1]['testError'][-1])
            print 'Max trainAccuracy: %.3f, Max validAccuracy: %.3f, Max testAccuracy: %.3f' % \
                    (np.max(trainAcc), \
                     np.max(validAcc), \
                     np.max(testAcc))
            print 'runTime: %3.1fs\n\n' % (time.time() - startTime)

    return summaryList
\end{minted}
%------------------------------------------------------------------
\subsubsection{Q2.2 :Feedforward fully connected neural network}
\begin{minted}[linenos=true,bgcolor=bg,numberblanklines=true,showspaces=false,breaklines=true]
{python}
summary2_2_1 = train_NN(etaList=[1e-4, 1e-3, 1e-2], numHidLayersList=[1], layerUnitsList=[[1000]], sectionDir='2.2.1')
\end{minted}
%------------------------------------------------------------------
\subsubsection{Q2.3.1: Number of hidden units}
\begin{minted}[linenos=true,bgcolor=bg,numberblanklines=true,showspaces=false,breaklines=true]
{python}
summary2_3_1 = train_NN(etaList=[1e-3], numHidLayersList=[1, 1, 1], layerUnitsList=[[100], [500], [1000]], sectionDir='2.3.1')
\end{minted}
%------------------------------------------------------------------
\subsubsection{Q2.3.2: Number of layers}
\begin{minted}[linenos=true,bgcolor=bg,numberblanklines=true,showspaces=false,breaklines=true]
{python}
summary2_3_2 = train_NN(etaList=[1e-3], numHidLayersList=[1, 2], layerUnitsList=[[1000], [500, 500]], sectionDir='2.3.2')
\end{minted}
%------------------------------------------------------------------
\subsubsection{Q2.4.1: Dropout}
\begin{minted}[linenos=true,bgcolor=bg,numberblanklines=true,showspaces=false,breaklines=true]
{python}
summary2_4_1 = train_NN(etaList=[1e-3], numHidLayersList=[1], layerUnitsList=[[1000]], \
                        dropout_rate=0.5, sectionDir='2.4.1')
\end{minted}
%------------------------------------------------------------------
\subsubsection{Q2.4.2: Visualization}
\begin{minted}[linenos=true,bgcolor=bg,numberblanklines=true,showspaces=false,breaklines=true]
{python}
# Create visualization method
'''
Creates four plots at 25%, 50%, 75% and 100% of training epochs. 
Each plot consists of 40x25 subplots of 28x28 heatmaps to represent each of the 1000x28x28 weights.
'''
def visualise_weights(weights, hasDropout):
    # Mark start time
    startTime = time.time()
    
    # Calculate total number of epochs ran
    numEpochs = weights.shape[2]
    
    # Loop through 25%, 50%, 75%, 100% of runs
    for i in range(4):
        fig = plt.figure(figsize=(16,20))
        # Loop through all 1000 weights in the hidden layer
        for j in range(weights.shape[1]):
            # Plots on 40x25 subplot
            plt.subplot(40, 25, j + 1)
            
            # Create 28x28 heatmap of data
            plt.imshow(np.reshape(weights[:, j:j+1, int((i + 1) * numEpochs / 4) - 1], (28, 28)), cmap='gray')
            
            # Turn off axes for each subplot
            plt.gca().axis('off')
        
        # Place subplot main title
        fig.text(0.5, 0.9, str((i + 1) * 100 / 4) + '%', fontdict=dict(fontsize=20), \
                 transform=fig.transFigure, horizontalalignment='center')
        plt.show()
        
    # Print total plot duration
    print 'Duration: %.1fs' % (time.time() - startTime)
    
# Visualise weights of model without dropout
visualise_weights(summary2_2_1[-1]['weights_layer_0'], hasDropout=False)

# Visualise weights of model with dropout
visualise_weights(summary2_4_1[-1]['weights_layer_0'], hasDropout=True)
\end{minted}
%------------------------------------------------------------------
\subsubsection{Q2.5.1: Random search}
\begin{minted}[linenos=true,bgcolor=bg,numberblanklines=true,showspaces=false,breaklines=true]
{python}
def run_exhaustive_search():
    '''
    Performs exhaustive search on hyperparameters by sampling from a random uniform distribution. 
    Seed number used is a concatenation of the last two digits of both our student numbers. ('37' + '93')
    
    Selects random values for:
        ln(eta):            [-7.5, -4.5]
        hidden layers:      [1, 5]
        hidden layer units: [100, 500]
        ln(lambda):         [-9, -6]
        dropout:            'yes' or 'no'
    '''
    # Set random seed
    np.random.seed(3793)
    
    # Create placeholders to record randomly generated hyperparameters
    etaList = []
    layersList = []
    unitsList = []
    lambdaList = []
    dropoutList = []
    summary2_5_1 = []
    
    # Assign randomly generated values for model hyperparameters
    for i in range(5):
        # Randomly sample values for hyperparameters
        etaList.append(np.exp(np.random.uniform(-7.5, -4.5, size=1)))
        layersList.append(np.random.randint(1, 6, size=1))
        unitsList.append(np.random.randint(100, 501, size=layersList[i]))
        lambdaList.append(np.exp(np.random.uniform(-9, -6, size=1)).astype(np.float32))
        dropoutList.append(np.random.randint(0, 2))
    
    # Train each model
    for i in range(5):
        # Perform training run
        summary2_5_1.append(train_NN(etaList[i], layersList[i], [unitsList[i]], \
                                     dropout_rate=0.5*dropoutList[i], lambda_=lambdaList[i], sectionDir='2.5.1')[0])
        
    return summary2_5_1
        
summary2_5_1 = run_exhaustive_search()
\end{minted}
\clearpage
%------------------------------------------------------------------
\subsubsection{Q2.5.2: Exchange ideas among the group}
\begin{minted}[linenos=true,bgcolor=bg,numberblanklines=true,showspaces=false,breaklines=true]
{python}
np.random.seed(1897356)
testSummary = train_NN([0.00292], [5], [[496, 276, 486, 299, 376]], sectionDir='Test', dropout_rate=0, lambda_=0.00091)
\end{minted}
\end{document}
